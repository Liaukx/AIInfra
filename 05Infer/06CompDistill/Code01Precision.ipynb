{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36174d92",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 01: 低精度推理性能对比\n",
    "\n",
    "> authored by: 汪袁烁,ZOMI\n",
    "\n",
    "在大模型部署和应用中，推理效率和精度之间的平衡一直是一个关键挑战。随着模型规模的不断增长（从几亿参数到千亿参数），存储需求和计算开销也随之急剧增加。低精度推理技术通过使用更少的比特数来表示模型参数和计算中间结果，为解决这一问题提供了有效途径。\n",
    "\n",
    "本文将以 Qwen3 4B 模型为研究对象，对比 FP16（作为基线）、FP8、FP6 和 INT8 四种精度在推理效率和精度上的表现，帮助读者理解不同精度量化对模型性能的影响。\n",
    "\n",
    "## 1. 技术原理：量化基础\n",
    "\n",
    "模型量化的核心思想是将神经网络中的浮点数参数和激活值从高精度（如 FP32）转换为低精度（如 FP16、INT8 等）表示。这一过程可以用以下公式表示：\n",
    "\n",
    "对于整数量化，我们有：\n",
    "\n",
    "$$ x_{int} = \\text{round}(x_{float} / s + z) $$\n",
    "\n",
    "其中，$s$ 是缩放因子（scale），$z$ 是零点（zero point），用于将浮点数映射到整数域。\n",
    "\n",
    "对于浮点数量化（如 FP8），则是通过减少指数位和尾数位的数量来实现，这会直接影响数值的表示范围和精度。\n",
    "\n",
    "量化带来的好处主要有三点：\n",
    "\n",
    "1. 减少内存占用：例如 INT8 仅需 FP32 1/4 的存储空间\n",
    "2. 提高计算效率：低精度计算通常更快，尤其在支持 SIMD 指令的硬件上\n",
    "3. 降低功耗：低精度计算需要更少的能量\n",
    "\n",
    "但量化也可能导致精度损失，这也是我们本次实验需要验证的重点。\n",
    "\n",
    "## 2. 实验环境准备\n",
    "\n",
    "首先，让我们准备实验所需的环境和库。我们将使用 Hugging Face 的 Transformers 库加载 Qwen3 4B 模型，使用 Accelerate 库进行分布式加速，并使用 Evaluate 库评估模型性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装必要的库\n",
    "!pip install transformers accelerate evaluate datasets bitsandbytes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c64c88",
   "metadata": {},
   "source": [
    "接下来，导入所需的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# 设置随机种子，保证实验可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a7546",
   "metadata": {},
   "source": [
    "## 3. 模型与数据集准备\n",
    "\n",
    "我们将使用 Qwen3 4B 模型和一个常用的评估数据集。为了简化实验，我们选择了相对较小的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型名称\n",
    "#model_name = \"Qwen/Qwen3-4B-Chat\"\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"  # 或者 \"Qwen/Qwen3-4B\" / \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# 设置 padding 和截断策略\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a4a98",
   "metadata": {},
   "source": [
    "对于评估数据集，我们选择了常用的\"lambada\"数据集，它主要用于评估模型的句子续写能力："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载评估数据集\n",
    "# dataset = load_dataset(\"lambada\")\n",
    "dataset = load_dataset(\"EleutherAI/lambada_openai\")\n",
    "\n",
    "# 取前 128 个样本作为测试集（简化实验）\n",
    "test_dataset = dataset[\"test\"].select(range(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6863fb9b",
   "metadata": {},
   "source": [
    "让我们看看数据集中的样本是什么样子的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43183a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看一个样本\n",
    "print(\"样本示例：\")\n",
    "print(test_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961971c",
   "metadata": {},
   "source": [
    "## 4. 评估函数定义\n",
    "\n",
    "在开始实验前，我们需要定义一个评估函数，用于计算模型在不同精度下的推理时间和准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, time, math\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformer_engine.pytorch as te\n",
    "from transformer_engine.common import recipe as te_recipe\n",
    "from transformer_engine.common.recipe import Format\n",
    "from accelerate.utils import convert_model, has_transformer_engine_layers\n",
    "\n",
    "def build_samples(ds):\n",
    "    samples = []\n",
    "    for item in ds:\n",
    "        ws = item[\"text\"].split()\n",
    "        if len(ws) < 2:\n",
    "            continue\n",
    "        prefix = \" \".join(ws[:-1])\n",
    "        target = re.sub(r\"[^\\w'-]+$\", \"\", ws[-1]).lower()\n",
    "        samples.append((prefix, target))\n",
    "    return samples\n",
    "\n",
    "\n",
    "def batch_iter(lst, bs):\n",
    "    for i in range(0, len(lst), bs):\n",
    "        yield lst[i:i+bs]\n",
    "\n",
    "def norm(w: str) -> str:\n",
    "    return re.sub(r\"[^\\w'-]+$\", \"\", w).lower()\n",
    "\n",
    "def pad_batch_to_multiple(batch, multiple=8):\n",
    "    \"\"\"把最后一个 batch 补齐到 multiple 的倍数（只在 FP8 路径用）。\n",
    "       通过重复最后一个样本来补齐；返回 (padded_batch, valid_len)。\"\"\"\n",
    "    n = len(batch)\n",
    "    if n % multiple == 0:\n",
    "        return batch, n\n",
    "    need = multiple - (n % multiple)\n",
    "    return batch + [batch[-1]] * need, n  # valid_len=n，后处理时只取前 n 个结果\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_model(model, tokenizer, test_dataset,max_new_tokens=1,\n",
    "                   use_te_fp8=False, fp8_recipe=None, batch_size=8):\n",
    "    samples = build_samples(test_dataset)\n",
    "    model.eval()\n",
    "    total_t, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for batch in batch_iter(samples, batch_size):\n",
    "        # —— 若走 FP8，为避免解码阶段 S=1 导致 B×S 不是 8 的倍数，这里把最后批次凑齐到 8 —— \n",
    "        valid_len = len(batch)\n",
    "        if use_te_fp8 and fp8_recipe is not None:\n",
    "            batch, valid_len = pad_batch_to_multiple(batch, multiple=8)\n",
    "\n",
    "        prefixes = [p for p, _ in batch]\n",
    "        targets  = [t for _, t in batch]\n",
    "\n",
    "        # pad 到 16 的倍数，满足线性层最后一维/前导维的对齐要求\n",
    "        inputs = tokenizer(\n",
    "            prefixes,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            pad_to_multiple_of=16,\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        if use_te_fp8 and fp8_recipe is not None:\n",
    "            with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "                out_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "        else:\n",
    "            out_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        total_t += (time.perf_counter() - t0)\n",
    "\n",
    "        # 只评前 valid_len 个（如果我们为对齐补了样本，其余不要计入指标）\n",
    "        out_texts = tokenizer.batch_decode(out_ids, skip_special_tokens=True)[:valid_len]\n",
    "        targets   = targets[:valid_len]\n",
    "\n",
    "        for pred_text, tgt in zip(out_texts, targets):\n",
    "            m = re.match(r\"\\s*([A-Za-z]+(?:['-][A-Za-z]+)?)\", pred_text)\n",
    "            pred = norm(m.group(1)) if m else \"\"\n",
    "            correct += int(pred == tgt)\n",
    "            total   += 1\n",
    "\n",
    "    avg = (total_t / total) if total else 0.0\n",
    "    acc = (correct / total) if total else 0.0\n",
    "    return total_t, avg, acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5869e",
   "metadata": {},
   "source": [
    "## 5. FP16 精度实验\n",
    "\n",
    "首先，我们以 FP16 精度作为基线进行实验。FP16（半精度浮点数）使用 16 位表示一个浮点数，相比 FP32（单精度）能节省一半的存储空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c10046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 FP16 精度的模型\n",
    "# 可以先在命令行 export HF_HUB_DISABLE_XET=1 有效预防 504 超时\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # 指定为 FP16 精度\n",
    "    device_map={\"\": \"cuda:0\"}  # 自动分配设备\n",
    ")\n",
    "\n",
    "# 评估 FP16 模型\n",
    "print(\"开始评估 FP16 模型...\")\n",
    "total_time_fp16, avg_time_fp16, acc_fp16 = evaluate_model(\n",
    "    model_fp16, \n",
    "    tokenizer, \n",
    "    test_dataset,\n",
    "    max_new_tokens=1   \n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(f\"FP16 - 总推理时间: {total_time_fp16:.2f}秒, \"\n",
    "      f\"平均每个样本: {avg_time_fp16:.4f}秒, \"\n",
    "      f\"准确率: {acc_fp16:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50e74c",
   "metadata": {},
   "source": [
    "FP16 之所以被广泛用作基线，是因为它在精度损失相对较小的情况下，能显著提升推理速度并减少内存占用。对于大多数模型，从 FP32 转为 FP16 不会导致明显的精度下降，但能带来约 2 倍的性能提升。\n",
    "\n",
    "## 6. INT8 精度实验\n",
    "### 快速开始 \n",
    "\n",
    "接下来，我们尝试 INT8 精度。INT8 使用 8 位整数表示数据，相比 FP16 能再减少一半的存储空间，即仅为 FP32 的 1/4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 INT8 精度的模型\n",
    "model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,  # 启用 INT8 量化\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 评估 INT8 模型\n",
    "print(\"开始评估 INT8 模型...\")\n",
    "total_time_int8, avg_time_int8, acc_int8 = evaluate_model(\n",
    "    model_int8, \n",
    "    tokenizer, \n",
    "    test_dataset,\n",
    "    max_new_tokens=1   \n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(f\"INT8 - 总推理时间: {total_time_int8:.2f}秒, \"\n",
    "      f\"平均每个样本: {avg_time_int8:.4f}秒, \"\n",
    "      f\"准确率: {acc_int8:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc0193",
   "metadata": {},
   "source": [
    "### 理论分析 - LLM.int8()\n",
    "\n",
    "快速实验的部分到这里就结束了，可是这些简短的代码后面采用的技术却值得我们深究。其实刚才我们使用的就是之前介绍的[LLM.int8()](https://arxiv.org/pdf/2208.07339)量化技术。它提出了**向量级（vector-wise）量化** + **异常值（outliers）混合精度路径**\n",
    "\n",
    "INT8 量化是目前应用最广泛的低精度技术之一，因为它在精度和性能之间取得了很好的平衡。其核心原理是将浮点范围映射到整数范围，通常使用最小-最大量化方法：\n",
    "\n",
    "$$ x_{int8} = \\text{clip}(\\text{round}(x_{float} / s + 127), 0, 255) $$\n",
    "\n",
    "其中 $s$ 是缩放因子，计算方式为：$s = \\frac{\\text{max}(|x_{float}|)}{127}$。\n",
    "\n",
    "\n",
    "### 源码阅读 - LLM.int8()\n",
    "\n",
    "为了更形象的了解LLM.int8()，我找出了其中[源码](https://github.com/bitsandbytes-foundation/bitsandbytes)的部分。由于需要解读的内容比较多，我单独放在了`./LLM.int8()源码阅读`下。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f8e47",
   "metadata": {},
   "source": [
    "## 7. FP8 精度实验\n",
    "\n",
    "FP8 是一种较新的低精度浮点格式，相比 FP16 进一步减少了位数，但保留了浮点数的动态范围优势。\n",
    "更具体的，在使用FP8量化前，我们应该先将模型转换成Transformer Engine（TE）的形式以来支持FP8（E4M3 / E5M2）的算子，并且由于H100（Hopper）硬件对 BF16 有很好的支持。可以有接近FP16的速度并且更加稳定，因此我们推荐先将模型转为BF16格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847607c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate.utils import convert_model, contextual_fp8_autocast, has_transformer_engine_layers\n",
    "from transformer_engine.common import recipe as te_recipe\n",
    "from transformer_engine.common.recipe import Format\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import has_transformer_engine_layers\n",
    "accelerator = Accelerator()  \n",
    "\n",
    "model_bf16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": \"cuda:0\"}\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "with torch.no_grad():  \n",
    "    model_te = convert_model(\n",
    "        model_bf16,\n",
    "        to_transformer_engine=True,\n",
    "        _convert_linear=True,\n",
    "        _convert_ln=True,\n",
    "    )\n",
    "\n",
    "\n",
    "model_te = model_bf16\n",
    "\n",
    "\n",
    "assert has_transformer_engine_layers(model_te), \"TE 层替换失败\"\n",
    "\n",
    "\n",
    "# 2) 定义 FP8 配方（HYBRID=前向E4M3，反向E5M2；纯推理时只会用到前向E4M3）\n",
    "fp8_recipe = te_recipe.DelayedScaling(\n",
    "    fp8_format=Format.HYBRID,\n",
    "    amax_history_len=16,\n",
    "    amax_compute_algo=\"max\",\n",
    ")\n",
    "\n",
    "# 3) 用 Accelerate 的 FP8 上下文包一层 forward\n",
    "#    注：默认 eval() 时会禁用 FP8，为推理生效需 use_during_eval=True\n",
    "model_te.forward = contextual_fp8_autocast(\n",
    "    model_te.forward,\n",
    "    fp8_recipe=fp8_recipe,\n",
    "    use_during_eval=True   # <<— 关键：推理/评估也启用 FP8\n",
    ")\n",
    "\n",
    "model_te.eval()\n",
    "\n",
    "def generate_one(prompt, max_new_tokens=64):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_te.device)\n",
    "    with torch.inference_mode():\n",
    "        # 这里 forward 已被 FP8 autocast 包裹，无需再写 with te.fp8_autocast(...)\n",
    "        out = model_te.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "t, a, acc = evaluate_model(model_te, tokenizer, test_dataset, max_new_tokens=1,use_te_fp8=True, fp8_recipe=fp8_recipe, batch_size=8)\n",
    "print(f\"FP8 - 总推理时间: {t:.2f}s, 平均每样本: {a:.4f}s, 准确率: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90831d8d",
   "metadata": {},
   "source": [
    "FP8 有两种主要格式：E4M3（4 位指数，3 位尾数）和 E5M2（5 位指数，2 位尾数）。E4M3 提供更高的精度但范围较小，而 E5M2 则相反。在实际应用中，会根据具体场景选择合适的格式。\n",
    "\n",
    "相比 INT8，FP8 在表示非常大和非常小的数值时更有优势，这使得它在某些场景下能保持比 INT8 更高的精度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp8_recipe = te_recipe.DelayedScaling(\n",
    "    margin=0, interval=1, fp8_format=Format.HYBRID, amax_history_len=16, amax_compute_algo=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1d251",
   "metadata": {},
   "source": [
    "比如这里，设 Format.HYBRID，那么 forward 用 E4M3，backward 用 E5M2（以适应梯度阶段对动态范围更大的需求），如果你设 fp8_format = Format.E4M3，那么forward 和 backward中的所有 FP8 张量／算子都用 E4M3 格式。\n",
    "\n",
    "## 8. FP6 精度实验\n",
    "\n",
    "FP6 是一种更激进的低精度格式，使用 6 位表示浮点数。由于位数更少，它的精度会受到更大影响，但理论上能提供更高的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f32c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载基础模型\n",
    "model_fp6 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 2}\n",
    ")\n",
    "\n",
    "# 实现 FP6 量化（简化版）\n",
    "def quantize_to_fp6(tensor):\n",
    "    \"\"\"将张量量化为 FP6 精度（简化实现）\"\"\"\n",
    "    # 在实际应用中，这会更复杂，需要考虑指数和尾数位的分配\n",
    "    # 这里使用一种简单的缩放方法作为示例\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    scale = (max_val - min_val) / 63  # 6 位可以表示 64 个值\n",
    "    return ((tensor - min_val) / scale).round().clamp(0, 63)\n",
    "\n",
    "# 对模型参数应用 FP6 量化\n",
    "for param in model_fp6.parameters():\n",
    "    param.data = quantize_to_fp6(param.data).float() / 63 * (param.data.max() - param.data.min()) + param.data.min()\n",
    "\n",
    "# 评估 FP6 模型\n",
    "print(\"开始评估 FP6 模型...\")\n",
    "total_time_fp6, avg_time_fp6, acc_fp6 = evaluate_model(\n",
    "    model_fp6, \n",
    "    tokenizer, \n",
    "    test_dataset\n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(f\"FP6 - 总推理时间: {total_time_fp6:.2f}秒, \"\n",
    "      f\"平均每个样本: {avg_time_fp6:.4f}秒, \"\n",
    "      f\"准确率: {acc_fp6:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a7809",
   "metadata": {},
   "source": [
    "注意：上面的 FP6 量化是一个简化实现。在实际应用中，FP6 的实现会更复杂，通常采用 E2M3（2 位指数，3 位尾数）的格式。由于 FP6 的表示能力有限，它通常只用于对精度要求不高的场景，或者作为研究探索。\n",
    "\n",
    "## 9. 实验结果对比\n",
    "\n",
    "现在我们已经完成了所有精度的实验，让我们将结果汇总并进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总结果\n",
    "results = {\n",
    "    \"Precision\": [\"FP16\", \"FP8\", \"FP6\", \"INT8\"],\n",
    "    \"Total Time (s)\": [total_time_fp16, total_time_fp8, total_time_fp6, total_time_int8],\n",
    "    \"Avg Time per Sample (s)\": [avg_time_fp16, avg_time_fp8, avg_time_fp6, avg_time_int8],\n",
    "    \"Accuracy\": [acc_fp16, acc_fp8, acc_fp6, acc_int8],\n",
    "    \"Speedup vs FP16\": [1.0, total_time_fp16/total_time_fp8, \n",
    "                       total_time_fp16/total_time_fp6, \n",
    "                       total_time_fp16/total_time_int8],\n",
    "    \"Accuracy Drop\": [0.0, acc_fp16 - acc_fp8, \n",
    "                     acc_fp16 - acc_fp6, \n",
    "                     acc_fp16 - acc_int8]\n",
    "}\n",
    "\n",
    "# 打印结果表格\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddd414",
   "metadata": {},
   "source": [
    "让我们可视化这些结果，以便更直观地比较不同精度的表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c95832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
    "\n",
    "# 创建对比图表\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 速度对比\n",
    "ax1.bar(results[\"Precision\"], results[\"Speedup vs FP16\"], color=['blue', 'green', 'red', 'purple'])\n",
    "ax1.set_title('不同精度相对 FP16 的速度提升倍数')\n",
    "ax1.set_ylabel('速度提升倍数')\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 精度对比\n",
    "ax2.bar(results[\"Precision\"], results[\"Accuracy\"], color=['blue', 'green', 'red', 'purple'])\n",
    "ax2.set_title('不同精度下的模型准确率')\n",
    "ax2.set_ylabel('准确率')\n",
    "ax2.set_ylim(0, 1.0)  # 准确率范围在 0-1 之间\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91936e",
   "metadata": {},
   "source": [
    "从实验结果中，我们可以观察到以下几点：\n",
    "\n",
    "1. 性能提升：一般来说，精度越低，推理速度越快。INT8 和 FP8 通常能提供 2-3 倍的速度提升，而 FP6 可能更快。\n",
    "\n",
    "2. 精度损失：随着精度降低，模型准确率通常会有所下降。FP8 的精度损失通常较小，而 FP6 可能会有较明显的精度损失。\n",
    "\n",
    "3. 权衡选择：INT8 通常在速度和精度之间提供最佳平衡，是实际应用中的首选；FP8 在需要更高精度的场景下表现更好；而 FP6 则适用于对速度要求极高但可以接受较大精度损失的场景。\n",
    "\n",
    "## 总结与思考\n",
    "\n",
    "本实验对比了不同低精度格式（FP16、FP8、FP6 和 INT8）对 Qwen3 4B 模型推理性能和精度的影响。实验结果表明，低精度推理确实能显著提升模型的推理速度，但也可能带来一定的精度损失。\n",
    "\n",
    "这些发现对于大模型的实际部署具有重要指导意义，帮助开发者在不同的硬件条件和精度要求下选择合适的量化策略。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
