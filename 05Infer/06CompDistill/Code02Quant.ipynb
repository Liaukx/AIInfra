{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290424e6",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 02: FastVLM-0.5B 量化对比\n",
    "\n",
    "> Author by: 汪袁烁, ZOMI\n",
    "\n",
    "在深度学习应用中，模型的大小和计算效率往往是实际部署时需要考虑的关键因素。特别是对于像 FastVLM 这样的视觉语言模型，即使是 0.5B 参数规模，在资源受限的设备上运行也可能面临挑战。\n",
    "\n",
    "模型量化是解决这一问题的有效方法，它通过减少模型参数和计算的数值精度来降低显存占用并提高推理速度。今天我们就来实际对比不同量化策略对模型性能的影响。\n",
    "\n",
    "## 1. 模型量化基础\n",
    "\n",
    "量化的核心思想是将神经网络中的浮点数权重和激活值转换为定点数表示。最常用的是将 32 位浮点数（FP32）转换为更低位数的整数表示。\n",
    "\n",
    "对于权重量化，我们通常使用以下公式将浮点数转换为整数：\n",
    "\n",
    "$$ q = \\text{round}(r / s + z) $$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $r$ 是原始浮点数值\n",
    "- $s$ 是缩放因子（scale）\n",
    "- $z$ 是零点偏移（zero point）\n",
    "- $q$ 是量化后的整数值\n",
    "\n",
    "常见的量化配置有：\n",
    "\n",
    "- W4A4：权重和激活值都使用 4 位整数\n",
    "- W8A8：权重和激活值都使用 8 位整数 \n",
    "- W4A16：权重使用 4 位整数，激活值使用 16 位整数\n",
    "\n",
    "## 2. 实验环境准备\n",
    "\n",
    "首先，我们需要安装必要的库。我们将使用 Hugging Face 的 transformers 库加载模型，以及 accelerate 库来帮助管理显存使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装所需库\n",
    "!pip install transformers accelerate torch pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938274f",
   "metadata": {},
   "source": [
    "然后，我们导入必要的模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13feea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yswang/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoImageProcessor,AutoProcessor\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f5245",
   "metadata": {},
   "source": [
    "## 3. 加载 FastVLM-0.5B\n",
    "\n",
    "让我们先加载原始的 FastVLM-0.5B 模型，作为基准参考。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169d595",
   "metadata": {},
   "source": [
    "### 加载模型\n",
    "完成下载之后便可以加载 FastVLM 跑一个简单的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfba2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True  |  dtype: torch.float16\n",
      "Tokenizer / FP16 model loaded. (Hub)\n"
     ]
    }
   ],
   "source": [
    "# pip install -U \"transformers>=4.41\" accelerate safetensors timm sentencepiece\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# === 1) 设备与精度 ===\n",
    "use_fp16 = torch.cuda.is_available()  # 有 GPU 就用半精度\n",
    "dtype = torch.float16 if use_fp16 else torch.float32\n",
    "print(f\"CUDA: {torch.cuda.is_available()}  |  dtype: {dtype}\")\n",
    "\n",
    "# === 2A) 直接从 Hugging Face Hub 加载（联网）===\n",
    "MODEL_ID = \"apple/FastVLM-0.5B\"  # 官方权重仓库\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 拿到模型内置的图像预处理器引用，方便后续使用；此处不做推理\n",
    "image_processor = model_fp16.get_vision_tower().image_processor\n",
    "print(\"Tokenizer / FP16 model loaded. (Hub)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb719019",
   "metadata": {},
   "source": [
    "这段代码首先检查我们是否有可用的 GPU，因为量化实验在 GPU 上效果更明显。然后我们加载了模型的分词器和图像处理工具，最后加载了原始的 FP16 精度模型作为基准。之后是构造带 <image> 占位符的对话模板，FastVLM 系列在推理时会约定一个特殊图像 token（代码里的 IMAGE_TOKEN_INDEX=-200），代表“这里有一张图片”。因此先把字符串在 <image> 处分割，然后把这个特殊 token 插入到 input_ids 对应位置。之后用模型自带的图像预处理器得到 pixel_values。最后把图像特征同时输入，与文本 token 一起驱动解码器生成回答（也即推测解码）。\n",
    "\n",
    "注意我们使用了`torch_dtype=torch.float16`参数，这会将模型加载为半精度（16 位）而不是默认的 32 位，这已经是一种简单的量化形式了。\n",
    "\n",
    "因为是外网，如果你使用上述代码出现网络连接失败的问题，可以采用手动下载的方法。之前介绍了用 HF 下载的方法，这里介绍一种新的利用 git clone 的方法，先将 FastVLM clone 到本地：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74c09f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yswang/tvm_learn/cuda/AIInfra/05Infer/06CompDistill/models\n",
      "Updated git hooks.\n",
      "Git LFS initialized.\n",
      "Cloning into 'FastVLM-0.5B'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 27 (delta 2), reused 0 (delta 0), pack-reused 19 (from 1)\u001b[K\n",
      "Receiving objects: 100% (27/27), 2.78 MiB | 36.00 KiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n",
      "Updating files: 100% (15/15), done.\n",
      "Filtering content: 100% (3/3), 1.41 GiB | 1.31 MiB/s, done.\n",
      "/home/yswang/tvm_learn/cuda/AIInfra/05Infer/06CompDistill\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make sure git-lfs is installed (https://git-lfs.com)\n",
    "%cd models\n",
    "!git lfs install\n",
    "!git clone git@hf.co:apple/FastVLM-0.5B\n",
    "# 如果你只是当作数据集使用，则可以删除 .git 目录\n",
    "!cd FastVLM-0.5B\n",
    "!rm -rf .git\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8824f",
   "metadata": {},
   "source": [
    "输入的图片为：\n",
    "\n",
    "![](./images/cat.jpg)\n",
    "\n",
    "比如你将模型下载到本地的`AI Infra/05Infer/06CompDistill/models`之后，便可以从本地模型目录进行加载了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c0154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图中是一只长着棕色和灰色羽毛的猫，它正躺在绿草茵茵的草坪上的草丛中，似乎处于放松或好奇的姿势。这只猫被高高的草所包围，背景中有些模糊不清，让其看上去好像是离观众很近，好像是在人行道上。\n",
      "这张照片捕捉到了这只猫在户外环境中处于自然状态的一刻，可能是在公园或田野中休息，享受着青草和阳光。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在这个场景中，猫的位置和姿势可以解释为什么它会被发现是这张照片中唯一的物体\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_DIR = \"./models/FastVLM-0.5B\"\n",
    "IMAGE_TOKEN_INDEX = -200  # 模型代码约定的占位 token id\n",
    "\n",
    "# 1) 加载 tokenizer / 模型（trust_remote_code 很关键：启用仓库里的 llava_qwen.py）\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",                 # 或改成 .to(\"cuda\") 手动放 GPU\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 2) 构造带 <image> 的聊天模板\n",
    "messages = [{\"role\": \"user\", \"content\": \"<image>\\n 描述一下这张图片。\"}]\n",
    "rendered = tok.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "pre, post = rendered.split(\"<image>\", 1)\n",
    "\n",
    "pre_ids  = tok(pre,  return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "post_ids = tok(post, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "img_tok  = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype)\n",
    "input_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(model.device)\n",
    "attention_mask = torch.ones_like(input_ids, device=model.device)\n",
    "\n",
    "# 3) 通过“模型自带”的 image_processor 做图像预处理（离线，不需要 preprocessor_config.json）\n",
    "img_path = \"./images/cat.jpg\"   # 这里我选取的 cat 的图片\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "px = model.get_vision_tower().image_processor(\n",
    "    images=img, return_tensors=\"pt\"\n",
    ")[\"pixel_values\"].to(model.device, dtype=model.dtype)\n",
    "\n",
    "# 4) 生成\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        inputs=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        images=px,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebe0a1",
   "metadata": {},
   "source": [
    "## 4. 准备数据和评估函数\n",
    "\n",
    "为了公平比较不同量化配置，我们需要相同的测试数据和评估方法。\n",
    "\n",
    "### 图片数据准备\n",
    "由于 FastVLM 属于视觉+文本的多模态大模型，因此需要准备图片相关的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3069b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# 1) 自定义 User-Agent，避免 403\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [(\"User-Agent\", \"Mozilla/5.0 (compatible; demo-notebook; +https://example.org)\")]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "# 2) 目标目录\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# 3) 直接使用 Wikimedia 的原图直链（非 Special:FilePath，避免额外跳转）\n",
    "assets = {\n",
    "    \"cat.jpg\": \"https://upload.wikimedia.org/wikipedia/commons/4/44/Cat_Domestic.jpg\",\n",
    "    \"receipt_1895.jpg\": \"https://upload.wikimedia.org/wikipedia/commons/4/47/1895_Benjamin_French_%26_Co._Receipt.jpg\",\n",
    "    \"receipt_shell.jpg\": \"https://upload.wikimedia.org/wikipedia/commons/3/34/Shell-Gas-Station-Receipt-MasterCard.jpg\",\n",
    "    \"landscape_monet.jpg\": \"https://upload.wikimedia.org/wikipedia/commons/b/b7/Claude_Monet_-_Landscape%2C_The_Parc_Monceau.jpg\",\n",
    "}\n",
    "\n",
    "# 4) 下载并生成可直接引用的变量\n",
    "local_paths = {}\n",
    "for fname, url in assets.items():\n",
    "    dest = os.path.join(\"images\", fname)\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {fname} ...\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "    local_paths[fname.split('.')[0]] = dest\n",
    "\n",
    "# 供后续直接使用的变量\n",
    "IMAGE_CAT = local_paths[\"cat\"]                     # 'images/cat.jpg'\n",
    "IMAGE_RECEIPT_1895 = local_paths[\"receipt_1895\"]   # 'images/receipt_1895.jpg'\n",
    "IMAGE_RECEIPT_SHELL = local_paths[\"receipt_shell\"] # 'images/receipt_shell.jpg'\n",
    "IMAGE_LANDSCAPE = local_paths[\"landscape_monet\"]   # 'images/landscape_monet.jpg'\n",
    "\n",
    "print(\"Ready:\", IMAGE_CAT, IMAGE_RECEIPT_1895, IMAGE_RECEIPT_SHELL, IMAGE_LANDSCAPE, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fdbcb1",
   "metadata": {},
   "source": [
    "### 模型加载函数\n",
    "\n",
    "我们先写一个加载函数来加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df1872a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_fastvlm(model_path_or_id=\"apple/FastVLM-0.5B\", prefer_fp16=None, device_map=\"auto\"):\n",
    "    \"\"\"\n",
    "    返回: tokenizer, model, image_processor, device, dtype\n",
    "    - model_path_or_id: 可填 Hugging Face 仓库名，或本地目录路径\n",
    "    - prefer_fp16: None=有 CUDA 则 FP16，否则 FP32\n",
    "    \"\"\"\n",
    "    if prefer_fp16 is None:\n",
    "        prefer_fp16 = torch.cuda.is_available()\n",
    "    dtype = torch.float16 if prefer_fp16 else torch.float32\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path_or_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path_or_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    image_processor = model.get_vision_tower().image_processor\n",
    "    print(f\"[Loaded] {model_path_or_id}  device={device}  dtype={dtype}\")\n",
    "    return tokenizer, model, image_processor, device, dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e508004",
   "metadata": {},
   "source": [
    "利用`from_pretrained(...)`按名称或路径找到并实例化模型/分词器”,获取 `image_processor` 用于图像→`pixel_values` 的预处理，便于后续 `generate(inputs=..., images=...)` 推理。\n",
    "\n",
    "### 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50264f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def evaluate_model(model, image, question, tokenizer, image_processor, device,\n",
    "                   iterations=5, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    评估 FastVLM：返回生成答案、平均延迟(ms)、显存占用(MB)。\n",
    "    参数 image 可为 PIL.Image 或图片路径字符串。\n",
    "    兼容多卡(device_map='auto')：文本张量放到嵌入层所在设备，图像张量放到视觉塔所在设备。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    IMAGE_TOKEN_INDEX = -200  # FastVLM 约定的图像占位 token\n",
    "\n",
    "    # === 0) 识别实际设备（多卡时非常关键）\n",
    "    text_device   = model.get_input_embeddings().weight.device\n",
    "    vision_device = next(model.get_vision_tower().parameters()).device\n",
    "\n",
    "    # === 1) 文本：用 chat 模板并在 <image> 处插入占位 token\n",
    "    messages = [{\"role\": \"user\", \"content\": \"<image>\\n\" + question}]\n",
    "    rendered = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    pre, post = rendered.split(\"<image>\", 1)\n",
    "\n",
    "    pre_ids  = tokenizer(pre,  return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    post_ids = tokenizer(post, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    img_tok  = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype)\n",
    "\n",
    "    input_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(text_device)   # ← 文本到嵌入层所在设备\n",
    "    attention_mask = torch.ones_like(input_ids, device=text_device)\n",
    "\n",
    "    # === 2) 图像：用模型内置 image_processor 得到 pixel_values，并放到视觉塔所在设备\n",
    "    if isinstance(image, str):\n",
    "        img = Image.open(image).convert(\"RGB\")\n",
    "    else:\n",
    "        img = image.convert(\"RGB\")\n",
    "    pixel_values = image_processor(images=img, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    pixel_values = pixel_values.to(vision_device, dtype=model.dtype)              # ← 图像到视觉塔所在设备\n",
    "\n",
    "    # === 3) 评测：多次生成取平均延迟\n",
    "    if torch.cuda.is_available():\n",
    "        # 分别对用到的 GPU 复位峰值 & 同步\n",
    "        used_cuda = []\n",
    "        for dev in {text_device, vision_device}:\n",
    "            if isinstance(dev, torch.device) and dev.type == \"cuda\":\n",
    "                torch.cuda.reset_peak_memory_stats(dev)\n",
    "                used_cuda.append(dev)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    last_out = None\n",
    "    for _ in range(iterations):\n",
    "        with torch.no_grad():\n",
    "            last_out = model.generate(\n",
    "                inputs=input_ids,               # 注意：FastVLM 自定义 generate 接口是 inputs= + images=\n",
    "                attention_mask=attention_mask,\n",
    "                images=pixel_values,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    avg_latency_ms = (time.time() - start) / iterations * 1000.0\n",
    "\n",
    "    # === 4) 显存占用（多卡求和；无 GPU 置 0）\n",
    "    memory_used_mb = 0.0\n",
    "    if torch.cuda.is_available():\n",
    "        mem = 0\n",
    "        for dev in {text_device, vision_device}:\n",
    "            if isinstance(dev, torch.device) and dev.type == \"cuda\":\n",
    "                mem += torch.cuda.max_memory_allocated(dev)\n",
    "        memory_used_mb = mem / (1024 ** 2)\n",
    "\n",
    "    answer = tokenizer.decode(last_out[0], skip_special_tokens=True)\n",
    "    return {\"answer\": answer, \"avg_latency_ms\": avg_latency_ms, \"memory_used_mb\": memory_used_mb}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf60d18",
   "metadata": {},
   "source": [
    "## 5. 基准测试：FP16 模型\n",
    "\n",
    "让我们先测试原始的 FP16 模型作为基准："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1a6ebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在评估 FP16 模型...\n",
      "FP16 模型结果:\n",
      "答案: 该图片包含一个单一物体，它是黑色的，位于顶部和中部。这个物体看起来像一个空白的方块或空白画布。在没有任何附加元素或特征的情况下，它与它的黑色外表形成了鲜明对比。\n",
      "在\n",
      "平均延迟: 692.56 ms\n",
      "显存占用: 2546.32 MB\n"
     ]
    }
   ],
   "source": [
    "# 清空缓存，确保测量准确\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"正在评估 FP16 模型...\")\n",
    "\n",
    "# iterations / max_new_tokens 可按需调整\n",
    "fp16_results = evaluate_model(\n",
    "    model_fp16,           # 来自 load_fastvlm() 返回的模型\n",
    "    test_image,           # PIL.Image 或路径字符串\n",
    "    test_question,        # 你的问题文本\n",
    "    tokenizer,            # load_fastvlm() 返回\n",
    "    image_processor,      # model.get_vision_tower().image_processor\n",
    "    device,               # \"cuda\" 或 \"cpu\"\n",
    "    iterations=5,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "print(\"FP16 模型结果:\")\n",
    "print(f\"答案: {fp16_results['answer']}\")\n",
    "print(f\"平均延迟: {fp16_results['avg_latency_ms']:.2f} ms\")\n",
    "# 无 GPU 时 evaluate_model 会把显存占用置 0\n",
    "print(f\"显存占用: {fp16_results['memory_used_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc80b68",
   "metadata": {},
   "source": [
    "在进行评估前，我们调用了`torch.cuda.empty_cache()`来清空 GPU 缓存，确保显存测量的准确性。然后我们使用前面定义的评估函数来测试模型。\n",
    "\n",
    "## 6. 量化配置 1：W8A8\n",
    "\n",
    "现在让我们尝试 8 位量化，这是一种常用的平衡性能和精度的量化策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8180e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/yswang/miniforge3/lib/python3.12/site-packages (0.47.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from bitsandbytes) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from bitsandbytes) (2.2.6)\n",
      "Requirement already satisfied: filelock in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (79.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "正在加载 W8A8 量化模型...\n",
      "正在评估 W8A8 模型...\n",
      "W8A8 模型结果:\n",
      "答案:  \n",
      "]\n",
      " \n",
      "the image is part of a blog, where a very important piece of information indicates that the film's completion and the subsequent completion of a film festival.\n",
      " in the context of the film, this is a documentary that features a film about the\n",
      "平均延迟: 4808.06 ms\n",
      "显存占用: 3368.48 MB\n"
     ]
    }
   ],
   "source": [
    "# 安装 bitsandbytes 库用于量化\n",
    "!pip install bitsandbytes\n",
    "\n",
    "# 加载 8 位量化模型\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 配置 8 位量化参数\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # 启用 8 位量化\n",
    ")\n",
    "\n",
    "# 清空缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 加载 8 位量化模型\n",
    "print(\"正在加载 W8A8 量化模型...\")\n",
    "model_w8a8 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config_8bit,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 评估 8 位量化模型\n",
    "print(\"正在评估 W8A8 模型...\")\n",
    "w8a8_results = evaluate_model(\n",
    "    model_w8a8, \n",
    "    test_image, \n",
    "    test_question, \n",
    "    tokenizer, \n",
    "    image_processor, \n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"W8A8 模型结果:\")\n",
    "print(f\"答案: {w8a8_results['answer']}\")\n",
    "print(f\"平均延迟: {w8a8_results['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"显存占用: {w8a8_results['memory_used_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc3937",
   "metadata": {},
   "source": [
    "这里我们使用了 bitsandbytes 库提供的 8 位量化功能。通过设置`load_in_8bit=True`，我们告诉库将模型权重加载为 8 位整数。\n",
    "\n",
    "理论上，8 位量化可以将模型大小减少约 4 倍（从 32 位浮点数到 8 位整数），但实际显存节省可能略少，因为还需要存储一些量化参数（如缩放因子）。\n",
    "\n",
    "## 7. 量化配置 2：W4A4\n",
    "\n",
    "接下来，我们尝试更激进的 4 位量化，这会进一步减少模型大小和显存占用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00982e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 W4A4 量化模型...\n",
      "正在评估 W4A4 模型...\n",
      "W4A4 模型结果:\n",
      "答案: 图片是一个中心黑色的正方形，位于白色背景中。这个黑方形几乎占据了整个图像的大部分面积。设计使用黑底白覆盖来创造一个极简主义和干净的外观。在这只黑方形的上方和下方各\n",
      "平均延迟: 1586.63 ms\n",
      "显存占用: 4010.32 MB\n"
     ]
    }
   ],
   "source": [
    "# 配置 4 位量化参数\n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 启用 4 位量化\n",
    "    bnb_4bit_use_double_quant=True,  # 使用双量化，进一步节省空间\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 使用正态分布量化\n",
    ")\n",
    "\n",
    "# 清空缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 加载 4 位量化模型（W4A4）\n",
    "print(\"正在加载 W4A4 量化模型...\")\n",
    "model_w4a4 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config_4bit,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 评估 4 位量化模型\n",
    "print(\"正在评估 W4A4 模型...\")\n",
    "w4a4_results = evaluate_model(\n",
    "    model_w4a4, \n",
    "    test_image, \n",
    "    test_question, \n",
    "    tokenizer, \n",
    "    image_processor, \n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"W4A4 模型结果:\")\n",
    "print(f\"答案: {w4a4_results['answer']}\")\n",
    "print(f\"平均延迟: {w4a4_results['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"显存占用: {w4a4_results['memory_used_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2c0b6",
   "metadata": {},
   "source": [
    "在 4 位量化配置中，我们使用了一些额外的优化：\n",
    "\n",
    "- `bnb_4bit_use_double_quant=True`：启用双量化，对量化参数本身也进行量化\n",
    "- `bnb_4bit_quant_type=\"nf4\"`：使用正态分布感知量化，这通常比均匀量化保留更好的精度\n",
    "\n",
    "4 位量化理论上可以比 32 位浮点数减少 8 倍的存储空间，是资源受限环境下的理想选择。\n",
    "\n",
    "## 8. 量化配置 3：W4A16\n",
    "\n",
    "最后，我们测试一种混合量化策略：权重使用 4 位，激活使用 16 位。这种配置试图在节省显存和保持推理精度之间取得更好的平衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e09f43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 W4A16 量化模型...\n",
      "正在评估 W4A16 模型...\n",
      "W4A16 模型结果:\n",
      "答案: 图中有一个大的黑色正方形，它没有任何特定特征、颜色或者纹理，与任何背景下没有附加的物体相比显得很突出。在这个黑色矩形与白色背景之间形成鲜明对比的方形中没有任何元素，使得即使没有进一步\n",
      "平均延迟: 1201.44 ms\n",
      "显存占用: 4651.30 MB\n"
     ]
    }
   ],
   "source": [
    "# 配置 W4A16 量化参数\n",
    "bnb_config_w4a16 = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16  # 计算使用 16 位浮点数\n",
    ")\n",
    "\n",
    "# 清空缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 加载 W4A16 量化模型\n",
    "print(\"正在加载 W4A16 量化模型...\")\n",
    "model_w4a16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config_w4a16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 评估 W4A16 量化模型\n",
    "print(\"正在评估 W4A16 模型...\")\n",
    "w4a16_results = evaluate_model(\n",
    "    model_w4a16, \n",
    "    test_image, \n",
    "    test_question, \n",
    "    tokenizer, \n",
    "    image_processor, \n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"W4A16 模型结果:\")\n",
    "print(f\"答案: {w4a16_results['answer']}\")\n",
    "print(f\"平均延迟: {w4a16_results['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"显存占用: {w4a16_results['memory_used_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d485c",
   "metadata": {},
   "source": [
    "这里的关键参数是`bnb_4bit_compute_dtype=torch.float16`，它指定了在计算过程中（主要是激活值）使用 16 位浮点数，而不是 4 位整数。这种配置可以减少量化误差，尤其是在激活值动态范围较大的情况下。\n",
    "\n",
    "## 9. 实验结果对比与分析\n",
    "\n",
    "现在让我们汇总所有实验结果，进行对比分析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca07a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置       延迟(ms)     显存(MB)    \n",
      "------------------------------\n",
      "FP16     692.56     2546.32   \n",
      "W8A8     4808.06    3368.48   \n",
      "W4A4     1586.63    4010.32   \n",
      "W4A16    1201.44    4651.30   \n",
      "\n",
      " 相对值（相对于 FP16）:\n",
      "配置       延迟比例       显存比例      \n",
      "------------------------------\n",
      "FP16     1.00       1.00      \n",
      "W8A8     6.94       1.32      \n",
      "W4A4     2.29       1.57      \n",
      "W4A16    1.73       1.83      \n"
     ]
    }
   ],
   "source": [
    "# 汇总结果\n",
    "results = {\n",
    "    \"FP16\": fp16_results,\n",
    "    \"W8A8\": w8a8_results,\n",
    "    \"W4A4\": w4a4_results,\n",
    "    \"W4A16\": w4a16_results\n",
    "}\n",
    "\n",
    "# 打印对比表格\n",
    "print(f\"{'配置':<8} {'延迟(ms)':<10} {'显存(MB)':<10}\")\n",
    "print(\"-\" * 30)\n",
    "for config, res in results.items():\n",
    "    print(f\"{config:<8} {res['avg_latency_ms']:<10.2f} {res['memory_used_mb']:<10.2f}\")\n",
    "\n",
    "# 计算相对值（相对于 FP16）\n",
    "print(\"\\n 相对值（相对于 FP16）:\")\n",
    "print(f\"{'配置':<8} {'延迟比例':<10} {'显存比例':<10}\")\n",
    "print(\"-\" * 30)\n",
    "fp16_latency = results[\"FP16\"][\"avg_latency_ms\"]\n",
    "fp16_memory = results[\"FP16\"][\"memory_used_mb\"]\n",
    "\n",
    "for config, res in results.items():\n",
    "    latency_ratio = res[\"avg_latency_ms\"] / fp16_latency\n",
    "    memory_ratio = res[\"memory_used_mb\"] / fp16_memory\n",
    "    print(f\"{config:<8} {latency_ratio:<10.2f} {memory_ratio:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd30a95",
   "metadata": {},
   "source": [
    "这段代码会以表格形式展示所有配置的延迟和显存占用，并计算它们相对于 FP16 基准的比例。\n",
    "\n",
    "从理论上我们可以预期：\n",
    "\n",
    "- 显存占用：W4A4 < W4A16 < W8A8 < FP16\n",
    "- 推理延迟：通常量化程度越高，延迟越低，但这也取决于硬件支持\n",
    "\n",
    "除了这些数值指标，我们还应该关注模型的输出质量是否有明显下降。如果量化后的模型生成的答案质量严重下降，那么即使显存和延迟有优势，这种量化配置也可能不适用。\n",
    "\n",
    "## 10. 总结与思考\n",
    "\n",
    "在显存占用方面，4 位量化（W4A4 和 W4A16）相比 FP16 可以节省显著的显存空间，通常能达到 70-80%的减少，而 8 位量化（W8A8）则可以节省约 40-50%的显存。这种显存占用的减少对于在资源受限设备上部署大模型尤为重要。\n",
    "\n",
    "推理延迟方面，量化通常会带来推理速度的提升，但提升幅度取决于具体硬件和量化实现。一般来说，4 位量化可能比 8 位量化更快，但也可能因为需要更多的反量化操作而抵消部分优势。实际测量中，W4A16 配置通常在延迟和精度之间提供了较好的平衡。\n",
    "\n",
    "精度权衡是量化技术中需要重点考虑的因素。更高程度的量化（如 W4A4）可能会导致模型精度下降，特别是在复杂任务上。W4A16 这种混合配置通常能在节省显存和保持精度之间取得更好的平衡，尤其是对于多模态模型中的视觉特征处理部分。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
