{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b423d7e4",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 03:多模态输出采样与控制\n",
    "\n",
    "> 多模态生成模型正在改变我们创造和理解内容的方式，而采样策略则是控制这一过程的隐形艺术家。\n",
    "\n",
    "在当前的人工智能浪潮中，多模态生成模型已经成为内容创作的重要工具。无论是通过文字描述生成图像，还是根据静态图像创建视频，这些模型都展现出了惊人的能力。\n",
    "\n",
    "但你是否曾经好奇，为什么同样的输入提示，有时会产生令人惊艳的结果，有时却平平无奇？这背后往往取决于采样策略的选择。\n",
    "\n",
    "今天我们将深入探讨温度（Temperature）和 Top-P（核采样）两种采样策略如何影响多模态生成模型的输出结果，以及如何通过调整这些参数来平衡生成结果的**多样性**、**创造性**和**质量**。\n",
    "\n",
    "### 1. Temperature 采样\n",
    "\n",
    "温度参数或许是控制生成随机性最直观的方式。它在数学上调整了模型输出概率分布的平滑程度。\n",
    "\n",
    "给定原始概率分布 $P(x_i|x_{<i})$，应用温度参数 $T$ 后的新概率分布为：\n",
    "\n",
    "$$\\hat{P}(x_i|x_{<i}) = \\frac{\\exp(\\frac{z_i}{T})}{\\sum_j \\exp(\\frac{z_j}{T})}$$\n",
    "\n",
    "其中 $z_i$ 是模型输出的 logits 值。\n",
    "\n",
    "**温度参数的影响**：\n",
    "\n",
    "- 当 $T > 1$ 时，概率分布变得更加平滑，生成结果更加多样但可能不够准确\n",
    "- 当 $T < 1$ 时，概率分布更加尖锐，生成结果更加确定但可能缺乏创造性\n",
    "\n",
    "为了更好地理解这一概念，让我们通过代码来实现温度采样的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def apply_temperature(logits, temperature):\n",
    "    if temperature > 0:\n",
    "        # 应用温度缩放\n",
    "        scaled_logits = logits / temperature\n",
    "        # 重新计算 softmax 概率\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        return probs\n",
    "    else:\n",
    "        raise ValueError(\"温度参数必须大于 0\")\n",
    "\n",
    "# 示例用法\n",
    "logits = torch.tensor([2.0, 1.0, 0.5, 0.2])\n",
    "print(\"原始概率:\", F.softmax(logits, dim=-1).numpy())\n",
    "\n",
    "for temp in [0.5, 1.0, 2.0]:\n",
    "    temp_probs = apply_temperature(logits, temp)\n",
    "    print(f\"温度 {temp} 后的概率: {temp_probs.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c49796",
   "metadata": {},
   "source": [
    "这段代码展示了温度参数如何影响概率分布。当温度值较低时（如 0.5），概率分布更加集中，模型更倾向于选择最高概率的选项；而当温度值较高时（如 2.0），概率分布更加平滑，模型的选择更加多样化。\n",
    "\n",
    "## 2. Top-P 采样\n",
    "\n",
    "Top-P 采样，也称为核采样，选择概率累积超过阈值 p 的最小可能词元集合，然后从这个集合中重新归一化概率并采样。\n",
    "\n",
    "形式上，给定概率分布 $P$ 和阈值 $p ∈ (0, 1]$，我们按概率降序排列，找到最小的集合 $S$ 使得：\n",
    "\n",
    "$$\\sum_{x_i \\in S} P(x_i) \\geq p$$\n",
    "\n",
    "然后从集合 $S$ 中按照重新归一化的概率进行采样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(probs, p):\n",
    "    # 对概率进行排序\n",
    "    sorted_probs, indices = torch.sort(probs, descending=True)\n",
    "    # 计算累积概率\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    # 移除累积概率超过 p 的部分\n",
    "    indices_to_remove = cumulative_probs > p\n",
    "    # 确保至少保留一个 token\n",
    "    indices_to_remove[..., 1:] = indices_to_remove[..., :-1].clone()\n",
    "    indices_to_remove[..., 0] = 0\n",
    "    \n",
    "    # 将需要移除的 token 概率设为 0\n",
    "    sorted_probs[indices_to_remove] = 0\n",
    "    # 重新归一化概率\n",
    "    sorted_probs /= sorted_probs.sum()\n",
    "    \n",
    "    # 恢复到原始顺序\n",
    "    return sorted_probs.scatter(-1, indices, sorted_probs)\n",
    "\n",
    "# 示例用法\n",
    "probs = torch.tensor([0.4, 0.3, 0.2, 0.1])\n",
    "print(\"原始概率:\", probs.numpy())\n",
    "\n",
    "for p in [0.7, 0.9]:\n",
    "    top_p_probs = top_p_sampling(probs, p)\n",
    "    print(f\"Top-P (p={p}) 后的概率: {top_p_probs.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3157d",
   "metadata": {},
   "source": [
    "Top-P 采样的优势在于它能够动态调整候选集的大小，既保证了多样性，又避免了选择概率极低的选项。\n",
    "\n",
    "## 3. 文生图应用实践\n",
    "\n",
    "了解了采样策略的基本原理后，让我们看看它们在实际的多模态生成任务中如何应用。我们将使用 Stable Diffusion 模型进行文本到图像的生成实验。\n",
    "\n",
    "首先，我们需要设置环境并加载模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "import torch\n",
    "\n",
    "# 加载 SDXL 管道\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", \n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "\n",
    "# 将管道移动到 GPU（如果可用）\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79d495",
   "metadata": {},
   "source": [
    "虽然 Stable Diffusion 本身不直接暴露温度参数，但我们可以通过修改生成过程中的随机性来模拟类似效果。实际上，在扩散模型中，类似的随机性控制可以通过调整 guidance_scale 和随机种子来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ca4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_with_variation(prompt, num_images=4, guidance_scale=7.5):\n",
    "    images = []\n",
    "    for i in range(num_images):\n",
    "        # 使用不同的随机种子\n",
    "        generator = torch.Generator(device=device).manual_seed(i)\n",
    "        \n",
    "        # 生成图像\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator,\n",
    "        ).images[0]\n",
    "        \n",
    "        images.append(image)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# 设置生成参数\n",
    "prompt = \"一个美丽的日落海滩，有椰子树和金色的沙滩\"\n",
    "negative_prompt = \"模糊，失真，低质量\"\n",
    "\n",
    "# 生成图像\n",
    "images = generate_images_with_variation(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacac3c",
   "metadata": {},
   "source": [
    "在实际应用中，我们往往需要更精细的控制，而不仅仅是调整随机种子。多模态生成的可控性技术正在不断发展，例如通过 ControlNet 实现空间精准定位，通过 LoRA 注入特定规则，以及通过 CLIP 进行情感校准等方法。\n",
    "\n",
    "## 4. 多模态实验\n",
    "\n",
    "除了文生图应用，采样策略也对多模态语言模型的输出有重要影响。让我们以视觉语言模型为例，看看不同采样参数如何影响模型生成的描述。\n",
    "\n",
    "我们将使用 Qwen-VL 模型进行图文对话实验，观察不同温度和 Top-P 参数如何影响生成的图像描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# 初始化模型和处理器\n",
    "MODEL_PATH = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    limit_mm_per_prompt={\"image\": 10, \"video\": 10},\n",
    ")\n",
    "\n",
    "def generate_descriptions_with_sampling(image_path, prompt_text, sampling_configs):\n",
    "    # 构建消息\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": prompt_text},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # 应用聊天模板\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    descriptions = []\n",
    "    for config in sampling_configs:\n",
    "        # 设置采样参数\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=config[\"temperature\"],\n",
    "            top_p=config[\"top_p\"],\n",
    "            max_tokens=256,\n",
    "        )\n",
    "        \n",
    "        # 生成描述\n",
    "        outputs = llm.generate([{\"prompt\": prompt}], sampling_params=sampling_params)\n",
    "        description = outputs[0].outputs[0].text.strip()\n",
    "        descriptions.append(description)\n",
    "    \n",
    "    return descriptions\n",
    "\n",
    "# 定义不同的采样配置\n",
    "sampling_configs = [\n",
    "    {\"temperature\": 0.1, \"top_p\": 0.9, \"name\": \"低温度，高 Top-P\"},\n",
    "    {\"temperature\": 0.7, \"top_p\": 0.9, \"name\": \"中等温度，高 Top-P\"},\n",
    "    {\"temperature\": 1.2, \"top_p\": 0.9, \"name\": \"高温度，高 Top-P\"},\n",
    "    {\"temperature\": 0.7, \"top_p\": 0.3, \"name\": \"中等温度，低 Top-P\"},\n",
    "]\n",
    "\n",
    "# 生成描述（需要实际图像路径）\n",
    "image_path = \"path_to_your_image.jpg\"\n",
    "prompt_text = \"描述这张图片中的场景和细节\"\n",
    "descriptions = generate_descriptions_with_sampling(image_path, prompt_text, sampling_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da343b",
   "metadata": {},
   "source": [
    "通过这个实验，我们可以观察到不同采样配置下模型生成的描述有何不同。低温度配置往往产生更加保守和确定的描述，而高温度配置则可能产生更加创造性和多样化的描述，但也可能增加不相关或虚构内容的风险。\n",
    "\n",
    "## 5. 评估生成结果\n",
    "\n",
    "评估多模态生成结果的质量是一个复杂的任务，需要从多个维度进行考量。常用的评估指标包括：\n",
    "\n",
    "1.  **模态对齐度（MDA）**：衡量生成内容与输入提示之间的一致性。\n",
    "2.  **细节保真度（DF）**：评估生成内容的细节丰富程度和准确性。\n",
    "3.  **多样性**：衡量不同生成结果之间的差异程度。\n",
    "4.  **创造性**：评估生成内容的新颖性和创新程度。\n",
    "\n",
    "我们可以通过计算一些定量指标来评估生成结果的多样性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def calculate_diversity(descriptions):\n",
    "    \"\"\"\n",
    "    计算生成描述的多样性指标\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for desc in descriptions:\n",
    "        words = desc.lower().split()\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # 计算词汇总量和唯一词汇量\n",
    "    total_words = len(all_words)\n",
    "    unique_words = len(set(all_words))\n",
    "    lexical_diversity = unique_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"lexical_diversity\": lexical_diversity,\n",
    "        \"unique_words\": unique_words,\n",
    "        \"total_words\": total_words\n",
    "    }\n",
    "\n",
    "# 计算生成描述的多样性\n",
    "diversity_metrics = calculate_diversity(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f844d01",
   "metadata": {},
   "source": [
    "除了这些定量指标，人工评估仍然是评估生成质量的重要方式，特别是对于创造性和审美价值的判断。\n",
    "\n",
    "## 6. 总结与思考\n",
    "\n",
    "采样策略在多模态生成中扮演着至关重要的角色，它们像是隐形的艺术家，默默地影响着生成结果的多样性、创造性和质量。通过理解和掌握温度参数和 Top-P 采样等策略，我们能够更好地驾驭多模态生成模型，创造出更加符合期望的内容。\n",
    "\n",
    "需要注意的是，参数调整并非万能，它需要在模型能力、任务需求和使用场景之间找到平衡点。有时候，**创造力的提升可能会以降低精确性为代价**，而**过于追求确定性又可能抑制创新**。这正是多模态生成既是一门科学也是一门艺术的原因。\n",
    "\n",
    "希望本文能够为你理解和应用采样策略提供有益的指导，帮助你在多模态生成的探索之旅中走得更远。记住，最好的参数配置往往来自于不断的实验和调整，而不是一成不变的公式。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
