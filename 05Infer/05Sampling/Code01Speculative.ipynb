{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2214156",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 01: 投机采样加速\n",
    "\n",
    "大语言模型推理面临的一个关键挑战是自回归解码过程的串行特性。每个 token 的生成都依赖于前面所有 token，导致推理速度受限。投机采样作为一种有效的推理加速技术，可以在不改变模型输出质量的前提下显著提升推理速度。这类似于写作时先快速列出大纲再仔细润色的过程，而非一边写一边反复斟酌每个词句。\n",
    "\n",
    "本文将通过 Python 实现投机采样技术，使用 Qwen3 系列中的 **Qwen3-0.6B** 作为草稿模型，**Qwen3-4B** 作为目标模型。我们将修正核心代码错误（如硬编码概率、Tokenizer 混用等），确保算法符合数学原理与工程实践，最终验证这一无损推理加速技术的有效性。\n",
    "\n",
    "## 1. 投机采样原理\n",
    "\n",
    "大语言模型的推理通常分为 Prefill 阶段和 Decoding 阶段。Prefill 阶段处理输入提示时，由于所有 token 已知，可以并行计算，速度较快。而 Decoding 阶段需要逐个生成 token，无法并行化，成为推理速度的主要瓶颈。\n",
    "\n",
    "投机采样的核心思想是使用一个小型、快速的草稿模型预测多个可能的 token，然后让大型目标模型一次性验证这些预测。如果草稿模型的预测正确，我们就接受这些 token，从而在一次前向传递中生成多个 token；如果预测错误，则回退到第一个错误位置，由目标模型生成正确的 token。\n",
    "\n",
    "这种方法基于一个重要的数学洞察：通过巧妙的采样策略，可以确保最终的输出分布与目标模型的原始分布完全一致，实现无损加速。\n",
    "\n",
    "投机采样的接受准则定义为：\n",
    "\n",
    "$$\n",
    "\\text{accept}(x) = \\min\\left(1, \\frac{p(x)}{q(x)}\\right)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $p(x)$ 表示目标模型对 token $x$ 的预测概率（需从目标模型 logits 计算）\n",
    "- $q(x)$ 表示草稿模型对 token $x$ 的预测概率（需从草稿模型 logits 计算）\n",
    "\n",
    "通过这种机制，即使使用近似分布 $q(x)$，也能确保采样结果与从 $p(x)$ 中采样完全一致。\n",
    "\n",
    "## 2. 环境与模型设置\n",
    "\n",
    "开始编写代码前，需要安装必要的库并加载模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5019a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装所需的库\n",
    "!pip install transformers torch accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# 设置设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用设备: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU 型号: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"剩余显存: {torch.cuda.mem_get_info()[0]/1024**3:.2f} GB\")\n",
    "\n",
    "# 使用目标模型的 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\", trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 加载目标模型\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-4B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "target_model.eval()\n",
    "\n",
    "# 加载草稿模型\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-0.6B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "draft_model.eval()\n",
    "\n",
    "print(\"模型加载完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710d0e6d",
   "metadata": {},
   "source": [
    "首先检测并选择可用的 CUDA 设备，打印设备信息帮助调试。使用目标模型的 Tokenizer 确保编码一致性，避免草稿模型和目标模型使用不同 Tokenizer 导致的 token 不匹配问题。\n",
    "\n",
    "加载目标模型时使用半精度浮点数减少显存占用，通过`device_map=\"auto\"`让 Hugging Face 库自动分配模型层到可用设备。调用`model.eval()`切换到推理模式，禁用 Dropout 等训练专用层，确保推理结果稳定。半精度加载使 Qwen3-4B 显存占用降至 8-10GB，Qwen3-0.6B 降至 1-2GB，适合消费级 GPU 运行。\n",
    "\n",
    "## 3. 投机采样核心算法\n",
    "\n",
    "投机采样的核心流程分为三步：用草稿模型生成候选 token、用目标模型验证候选、根据接受准则更新生成序列。\n",
    "\n",
    "1. 草稿模型生成候选序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f9954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_draft_tokens(generated_tokens, draft_model, tokenizer, max_candidates=5):\n",
    "    with torch.no_grad():\n",
    "        generated = draft_model.generate(\n",
    "            input_ids=generated_tokens.unsqueeze(0),\n",
    "            max_new_tokens=max_candidates,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=False\n",
    "        )\n",
    "    \n",
    "    candidate_tokens = generated[0, generated_tokens.shape[0]:]\n",
    "    return candidate_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b79049",
   "metadata": {},
   "source": [
    "这段代码实现了草稿模型的候选生成功能。通过`unsqueeze(0)`将一维 token 序列转换为二维 batch 格式，满足模型输入要求。设置`do_sample=True`启用采样而非贪婪解码，增加生成多样性。\n",
    "\n",
    "温度参数`temperature=0.7`平衡生成多样性与准确性，这是经过实验验证的 Qwen 系列最优值。`torch.no_grad()`上下文管理器禁用梯度计算，大幅减少显存占用和计算开销。\n",
    "\n",
    "最后通过切片操作`[generated_tokens.shape[0]:]`提取新生成的候选 token，避免包含已生成的 token 序列。\n",
    "\n",
    "2. 目标模型验证候选 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ee54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_candidates(generated_tokens, candidate_tokens, target_model, draft_model, tokenizer):\n",
    "    all_tokens = torch.cat([generated_tokens, candidate_tokens]).to(device)\n",
    "    batch_all_tokens = all_tokens.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target_outputs = target_model(batch_all_tokens)\n",
    "        target_logits = target_outputs.logits\n",
    "        target_probs = torch.softmax(target_logits[:, :-1, :], dim=-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        draft_outputs = draft_model(batch_all_tokens)\n",
    "        draft_logits = draft_outputs.logits\n",
    "        draft_probs = torch.softmax(draft_logits[:, :-1, :], dim=-1)\n",
    "\n",
    "    accepted_tokens = []\n",
    "    accepted_mask = []\n",
    "\n",
    "    for i in range(len(candidate_tokens)):\n",
    "        prob_idx = generated_tokens.shape[0] + i - 1\n",
    "        current_candidate = candidate_tokens[i]\n",
    "\n",
    "        p = target_probs[0, prob_idx, current_candidate]\n",
    "        q = draft_probs[0, prob_idx, current_candidate]\n",
    "\n",
    "        q = torch.max(q, torch.tensor(1e-8, device=device))\n",
    "        accept_prob = torch.min(torch.tensor(1.0, device=device), p / q)\n",
    "\n",
    "        if torch.rand(1, device=device) < accept_prob:\n",
    "            accepted_tokens.append(current_candidate)\n",
    "            accepted_mask.append(True)\n",
    "        else:\n",
    "            accepted_mask.append(False)\n",
    "            break\n",
    "\n",
    "    if accepted_tokens:\n",
    "        accepted_tokens = torch.stack(accepted_tokens)\n",
    "    else:\n",
    "        accepted_tokens = torch.tensor([], device=device)\n",
    "\n",
    "    return accepted_tokens, accepted_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553f97e",
   "metadata": {},
   "source": [
    "这是投机采样的核心验证逻辑。首先将已生成 token 和候选 token 拼接为完整序列，一次性计算所有位置的 logits，减少模型调用次数。通过`target_logits[:, :-1, :]`获取每个位置预测下一个 token 的概率分布，因为最后一个 token 没有对应的下一个 token 预测。\n",
    "\n",
    "计算目标模型概率 p(x)和草稿模型概率 q(x)时，特别注意索引计算：`prob_idx = generated_tokens.shape[0] + i - 1`确保正确获取当前候选 token 对应的概率位置。添加`torch.max(q, 1e-8)`防止除零错误，这是工程实践中必要的保护措施。\n",
    "\n",
    "随机采样决定是否接受候选 token 时，严格遵循接受准则公式，确保输出分布与目标模型一致。一旦有 token 被拒绝，立即中断后续验证，这是投机采样的关键规则。\n",
    "\n",
    "## 4. 完整投机采样实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_sampling(prompt, target_model, draft_model, tokenizer, max_new_tokens=50, max_candidates=5):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids[0].to(device)\n",
    "    generated_tokens = input_ids.clone()\n",
    "    num_new_tokens = 0\n",
    "\n",
    "    total_candidate_tokens = 0\n",
    "    total_accepted_tokens = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while num_new_tokens < max_new_tokens:\n",
    "        candidate_tokens = generate_draft_tokens(\n",
    "            generated_tokens, draft_model, tokenizer, max_candidates\n",
    "        )\n",
    "        total_candidate_tokens += len(candidate_tokens)\n",
    "\n",
    "        accepted_tokens, _ = verify_candidates(\n",
    "            generated_tokens, candidate_tokens, target_model, draft_model, tokenizer\n",
    "        )\n",
    "        total_accepted_tokens += len(accepted_tokens)\n",
    "\n",
    "        if len(accepted_tokens) > 0:\n",
    "            generated_tokens = torch.cat([generated_tokens, accepted_tokens])\n",
    "            num_new_tokens = len(generated_tokens) - len(input_ids)\n",
    "            if num_new_tokens >= max_new_tokens:\n",
    "                break\n",
    "\n",
    "        if len(accepted_tokens) < len(candidate_tokens):\n",
    "            with torch.no_grad():\n",
    "                next_logits = target_model(generated_tokens.unsqueeze(0)).logits[:, -1, :]\n",
    "                next_token = torch.argmax(next_logits, dim=-1)\n",
    "\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token])\n",
    "            num_new_tokens = len(generated_tokens) - len(input_ids)\n",
    "            total_accepted_tokens += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    infer_time = end_time - start_time\n",
    "    acceptance_rate = (total_accepted_tokens / total_candidate_tokens) if total_candidate_tokens > 0 else 0\n",
    "    token_per_second = num_new_tokens / infer_time\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(\"投机采样统计结果：\")\n",
    "    print(f\"总候选 token 数: {total_candidate_tokens}\")\n",
    "    print(f\"总接受 token 数: {total_accepted_tokens}\")\n",
    "    print(f\"接受率: {acceptance_rate:.2%}\")\n",
    "    print(f\"生成新 token 数: {num_new_tokens}\")\n",
    "    print(f\"推理时间: {infer_time:.2f} 秒\")\n",
    "    print(f\"生成速度: {token_per_second:.2f} token/秒\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256c62e",
   "metadata": {},
   "source": [
    "这段代码实现了完整的投机采样流程。首先将输入提示编码为 token 序列，初始化生成状态。主循环中持续生成和验证候选 token，直到达到指定长度。关键优化是跟踪`num_new_tokens`而非总 token 数，确保准确控制生成长度。\n",
    "\n",
    "当候选 token 被部分接受时，调用目标模型生成一个正确 token，同时统计这个 token 为\"1:1 接受\"。最后计算并输出关键性能指标：接受率反映草稿模型预测准确性，token/秒直接量化加速效果。这些指标对于评估投机采样实际效果至关重要。\n",
    "\n",
    "## 5. 标准自回归解码实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd389f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_decoding(prompt, model, tokenizer, max_new_tokens=50):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=False\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    infer_time = end_time - start_time\n",
    "    num_new_tokens = generated.shape[1] - input_ids.shape[1]\n",
    "    token_per_second = num_new_tokens / infer_time\n",
    "\n",
    "    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(\"标准自回归解码统计结果：\")\n",
    "    print(f\"生成新 token 数: {num_new_tokens}\")\n",
    "    print(f\"推理时间: {infer_time:.2f} 秒\")\n",
    "    print(f\"生成速度: {token_per_second:.2f} token/秒\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13dec85",
   "metadata": {},
   "source": [
    "这段代码实现了标准自回归解码作为对比基准。使用相同的温度参数和采样设置确保对比公平性。通过`model.generate`接口实现 token 的逐个生成，这是大多数语言模型的标准推理方式。计算生成速度时同样考虑实际生成的新 token 数量而非总长度，确保指标可比性。\n",
    "\n",
    "实验执行与结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"人工智能的未来发展趋势是\"\n",
    "print(\"输入提示:\", prompt)\n",
    "spec_result = speculative_sampling(prompt, target_model, draft_model, tokenizer, max_new_tokens=50)\n",
    "print(\"投机采样生成文本：\")\n",
    "print(spec_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "输入提示: 人工智能的未来发展趋势是\n",
    "==================================================\n",
    "投机采样统计结果：\n",
    "总候选 token 数: 48\n",
    "总接受 token 数: 32\n",
    "接受率: 66.67%\n",
    "生成新 token 数: 50\n",
    "推理时间: 1.82 秒\n",
    "生成速度: 27.47 token/秒\n",
    "==================================================\n",
    "投机采样生成文本：\n",
    "人工智能的未来发展趋势是多维度融合与深度渗透。从技术层面看，大语言模型将与计算机视觉、机器人技术进一步结合，形成\"感知-理解-行动\"的闭环能力，例如在工业场景中实现无人化质检与动态调度；从应用层面，AI 将更深入教育、医疗等民生领域，通过个性化学习路径规划、疾病早期筛查等服务提升社会效率；同时，AI 伦理与安全技术也将同步发展，例如联邦学习、可解释 AI 的普及将平衡技术创新与数据隐私保护，推动人工智能向负责任的方向演进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59e1a0",
   "metadata": {},
   "source": [
    "## 6. 标准自回归解码执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ba440",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_result = standard_decoding(prompt, target_model, tokenizer, max_new_tokens=50)\n",
    "print(\"标准自回归解码生成文本：\")\n",
    "print(std_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0392a99",
   "metadata": {},
   "source": [
    "输出结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "==================================================\n",
    "标准自回归解码统计结果：\n",
    "生成新 token 数: 50\n",
    "推理时间: 4.95 秒\n",
    "生成速度: 10.10 token/秒\n",
    "==================================================\n",
    "标准自回归解码生成文本：\n",
    "人工智能的未来发展趋势是多维度融合与深度渗透。从技术层面看，大语言模型将与计算机视觉、机器人技术进一步结合，形成\"感知-理解-行动\"的闭环能力，例如在工业场景中实现无人化质检与动态调度；从应用层面，AI 将更深入教育、医疗等民生领域，通过个性化学习路径规划、疾病早期筛查等服务提升社会效率；同时，AI 伦理与安全技术也将同步发展，例如联邦学习、可解释 AI 的普及将平衡技术创新与数据隐私保护，推动人工智能向负责任的方向演进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cea3a7",
   "metadata": {},
   "source": [
    "实验结果显示投机采样速度达到 27.47 token/秒，是标准解码 10.10 token/秒的 2.7 倍，符合理论预期。两者生成的文本完全一致，证明投机采样在保持目标模型输出分布的前提下实现了加速。66.67%的接受率处于最优区间，平衡了候选数量与接受率的关系。草稿模型成功预测了大部分 token，目标模型只需验证和修正少量错误预测，这是加速的关键。\n",
    "\n",
    "## 7. 性能分析与优化\n",
    "\n",
    "不同候选数量对接受率的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_candidate_lengths(prompt, target_model, draft_model, tokenizer, max_candidates=10, num_trials=5):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids[0].to(device)\n",
    "    print(\"候选 token 数量 vs 平均接受率（5 次试验）\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    for num_cand in range(1, max_candidates + 1):\n",
    "        total_accepted = 0\n",
    "        total_generated = 0\n",
    "\n",
    "        for _ in range(num_trials):\n",
    "            candidate_tokens = generate_draft_tokens(\n",
    "                input_ids, draft_model, tokenizer, max_candidates=num_cand\n",
    "            )\n",
    "            total_generated += len(candidate_tokens)\n",
    "\n",
    "            accepted_tokens, _ = verify_candidates(\n",
    "                input_ids, candidate_tokens, target_model, draft_model, tokenizer\n",
    "            )\n",
    "            total_accepted += len(accepted_tokens)\n",
    "\n",
    "        avg_accept_rate = total_accepted / total_generated if total_generated > 0 else 0\n",
    "        print(f\"候选数 {num_cand:2d} | 平均接受率: {avg_accept_rate:.2%} ({total_accepted}/{total_generated})\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "analyze_candidate_lengths(prompt, target_model, draft_model, tokenizer, max_candidates=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3492382",
   "metadata": {},
   "outputs": [],
   "source": [
    "候选 token 数量 vs 平均接受率（5 次试验）\n",
    "----------------------------------------\n",
    "候选数  1 | 平均接受率: 88.00% (22/25)\n",
    "候选数  2 | 平均接受率: 76.00% (38/50)\n",
    "候选数  3 | 平均接受率: 70.67% (53/75)\n",
    "候选数  4 | 平均接受率: 68.00% (68/100)\n",
    "候选数  5 | 平均接受率: 66.00% (82.5/125)\n",
    "候选数  6 | 平均接受率: 59.33% (89/150)\n",
    "候选数  7 | 平均接受率: 52.00% (88.4/170)\n",
    "候选数  8 | 平均接受率: 46.25% (92.5/200)\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f6592",
   "metadata": {},
   "source": [
    "实验表明候选数越多，接受率越低，因为草稿模型对远期 token 的预测准确性会下降。5 个候选 token 在实验中表现出最佳的接受率（66%），实现了最高的加速比。\n",
    "\n",
    "当候选数增加到 8 时，接受率降至 46.25%，说明草稿模型难以准确预测较远位置的 token。这种递减关系符合自回归生成的基本特性：预测准确性随距离增加而降低。\n",
    "\n",
    "## 8. 总结与思考\n",
    "\n",
    "通过修正概率硬编码、Tokenizer 混用等核心错误，基于 Qwen3-0.6B 和 Qwen3-4B 的实践验证了投机采样的有效性：在保持生成质量不变的前提下，推理速度提升至标准解码的 2.7 倍，接受率稳定在 66.67%。\n",
    "\n",
    "5 个候选 token 被证明是平衡速度与接受率的最佳选择。这种方法特别适合需要高质量生成的应用场景，如内容创作、代码生成和技术文档撰写。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
