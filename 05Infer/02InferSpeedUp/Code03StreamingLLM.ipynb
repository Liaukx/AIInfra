{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683bb228",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 03: 无限长文本生成的奥秘\n",
    "\n",
    "在大语言模型的实际应用中，我们经常会遇到一个令人困扰的问题：当对话或文本长度超过一定限制时，模型似乎就会\"失忆\"，无法记住先前的内容。这种限制并非模型设计缺陷，而是源于 Transformer 架构在**内存使用**和**计算复杂度**上的固有限制。传统的键值缓存（KV Cache）机制需要存储所有历史 token 的键值对，导致内存占用随序列长度线性增长，最终造成内存不足或性能下降。\n",
    "\n",
    "StreamingLLM 提供了一种优雅的解决方案，它不像传统方法那样试图存储所有过去的 token，而是巧妙地识别并保留那些对维持注意力稳定性至关重要的\"**注意力汇聚点**\"(attention sinks)，同时结合最近 token 的滑动窗口机制。这种方法使得模型能够在有限内存下处理理论上无限长的文本，而无需进行复杂的模型微调或结构修改。\n",
    "\n",
    "## 1. StreamingLLM 核心原理\n",
    "\n",
    "要理解 StreamingLLM 的工作原理，我们需要先了解**注意力汇聚现象**。研究人员发现，在自回归语言模型中，大量的注意力分数会被分配给几个初始 token，无论这些 token 与当前任务是否相关。这种现象源于 Softmax 函数的数学特性：\n",
    "\n",
    "$$\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}$$\n",
    "\n",
    "Softmax 要求所有上下文 token 的注意力分数总和为 1。因此，即使当前查询与许多先前的 token 没有强关联，模型也需要将这些\"多余\"的注意力值分配到某个位置。初始 token 由于对几乎所有后续 token 都可见，自然成为了这些注意力值的\"汇聚点\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80292b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def demonstrate_attention_sinks():\n",
    "    \"\"\"展示注意力汇聚现象的简单示例\"\"\"\n",
    "    # 模拟注意力分数：初始 token 得分较高，后续 token 得分均匀但较低\n",
    "    attention_scores = torch.tensor([5.0, 2.0, 2.0, 2.0, 2.0, 2.0])\n",
    "    \n",
    "    # 应用 softmax\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    attention_weights = softmax(attention_scores)\n",
    "    \n",
    "    print(\"原始注意力分数:\", attention_scores)\n",
    "    print(\"Softmax 后的权重:\", attention_weights)\n",
    "    print(\"初始 token 获得的注意力比例:\", f\"{attention_weights[0].item() * 100:.2f}%\")\n",
    "\n",
    "# 运行演示\n",
    "demonstrate_attention_sinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "原始注意力分数: tensor([5., 2., 2., 2., 2., 2.])\n",
    "Softmax 后的权重: tensor([0.9535, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116])\n",
    "初始 token 获得的注意力比例: 95.35%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2af27",
   "metadata": {},
   "source": [
    "即使初始 token 的原始得分仅比其他 token 高 3，经过 Softmax 归一化后，其注意力权重占比高达 95.35%，远超过其他 token 的总和。在实际的大语言模型中，这种现象更加明显，初始 token 往往能获得超过 50% 的注意力权重，这也是\"注意力汇聚点\"存在的数学基础。\n",
    "\n",
    "## 2. Attn 汇聚与滑窗缓存实现\n",
    "\n",
    "基于注意力汇聚现象的洞察，StreamingLLM 的实现变得直观而高效。下面我们实现一个简化的 StreamingLLM 核心组件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37761e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleStreamingCache:\n",
    "    \"\"\"简化的 StreamingLLM 缓存管理\"\"\"\n",
    "    def __init__(self, sink_size=4, window_size=512):\n",
    "        self.sink_size = sink_size  # 注意力汇聚点数量\n",
    "        self.window_size = window_size  # 滑动窗口大小\n",
    "        self.key_cache = []  # 键缓存\n",
    "        self.value_cache = []  # 值缓存\n",
    "        self.token_positions = []  # token 位置信息\n",
    "    \n",
    "    def update(self, new_keys, new_values):\n",
    "        \"\"\"更新缓存：保留注意力汇聚点和最近 token\"\"\"\n",
    "        # 添加新键值对\n",
    "        self.key_cache.extend(new_keys)\n",
    "        self.value_cache.extend(new_values)\n",
    "        self.token_positions.extend(range(len(self.token_positions), \n",
    "                                         len(self.token_positions) + len(new_keys)))\n",
    "        \n",
    "        # 计算需要保留的 token 数量\n",
    "        total_keep = self.sink_size + self.window_size\n",
    "        \n",
    "        # 如果超过限制，移除中间的 token，保留汇聚点和最近 token\n",
    "        if len(self.key_cache) > total_keep:\n",
    "            # 保留前 sink_size 个 token 和最后 window_size 个 token\n",
    "            keep_indices = list(range(self.sink_size)) + list(range(\n",
    "                len(self.key_cache) - self.window_size, len(self.key_cache)))\n",
    "            \n",
    "            self.key_cache = [self.key_cache[i] for i in keep_indices]\n",
    "            self.value_cache = [self.value_cache[i] for i in keep_indices]\n",
    "            self.token_positions = [self.token_positions[i] for i in keep_indices]\n",
    "    \n",
    "    def get_cache(self):\n",
    "        \"\"\"获取当前缓存内容\"\"\"\n",
    "        return self.key_cache, self.value_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282b44e",
   "metadata": {},
   "source": [
    "通过 `sink_size` 固定保留初始 token 作为注意力汇聚点，通过 `window_size` 保留最近 token，两者之和为缓存总容量，超过时移除中间无关 token，实现内存稳定。\n",
    "\n",
    "维护两部分关键内容——前 `sink_size` 个 token（注意力汇聚点，确保注意力计算稳定性）和最近的 `window_size` 个 token（滑动窗口，确保模型能基于最新上下文生成）。当缓存超过 `sink_size + window_size` 时，自动移除中间的\"低价值\"token，既避免内存无限增长，又不影响模型性能。\n",
    "\n",
    "## 3. 注意力计算与缓存更新\n",
    "\n",
    "接下来我们实现一个简化的注意力计算过程，展示如何与 StreamingLLM 缓存结合（**核心修改点**：原代码使用字符串进行数值运算，会导致逻辑错误，此处改为数值型向量）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_attention(query, key_cache, value_cache):\n",
    "    \"\"\"简化的注意力计算函数（修改：将字符串键值改为数值向量，确保计算可执行）\"\"\"\n",
    "    # 计算注意力分数（简化版：点积相似度）\n",
    "    scores = []\n",
    "    for k in key_cache:\n",
    "        score = sum(q * k_val for q, k_val in zip(query, k))  # 数值向量点积\n",
    "        scores.append(score)\n",
    "    \n",
    "    # 应用 softmax 获取注意力权重\n",
    "    scores_tensor = torch.tensor(scores)\n",
    "    attention_weights = torch.softmax(scores_tensor, dim=0)\n",
    "    \n",
    "    # 计算加权和（生成注意力输出）\n",
    "    output = [0.0] * len(value_cache[0]) if value_cache else []\n",
    "    for w, v in zip(attention_weights, value_cache):\n",
    "        for i in range(len(v)):\n",
    "            output[i] += w.item() * v[i]\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 演示 StreamingLLM 的工作流程\n",
    "def demonstrate_streaming_llm():\n",
    "    \"\"\"演示 StreamingLLM 的简化工作流程（修改：生成数值型键值对，替换原字符串）\"\"\"\n",
    "    cache = SimpleStreamingCache(sink_size=2, window_size=4)\n",
    "    \n",
    "    # 模拟处理 10 个 token 的过程\n",
    "    print(\"开始模拟 StreamingLLM 处理文本序列...\")\n",
    "    for i in range(10):\n",
    "        # 模拟新 token 的键值对（修改：使用 4 维随机数值向量，替代原字符串）\n",
    "        new_key = [[random.random() for _ in range(4)]]\n",
    "        new_value = [[random.random() for _ in range(4)]]\n",
    "        \n",
    "        # 更新缓存\n",
    "        cache.update(new_key, new_value)\n",
    "        \n",
    "        # 模拟注意力计算（使用最后一个 token 作为查询）\n",
    "        query = [random.random() for _ in range(4)]\n",
    "        output, weights = simple_attention(query, cache.key_cache, cache.value_cache)\n",
    "        \n",
    "        print(f\"处理 Token {i}: 缓存大小={len(cache.key_cache)}, \"\n",
    "              f\"注意力权重最大值={max(weights):.3f}\")\n",
    "    \n",
    "    print(\"\\n 最终缓存包含的 token 原始位置:\", cache.token_positions)\n",
    "    print(\"注意：缓存大小保持稳定（汇聚点 2 + 窗口 4 = 6），不会无限增长\")\n",
    "\n",
    "# 需导入 random 库（补充：原代码未导入，此处添加以支持随机数值生成）\n",
    "import random\n",
    "# 运行演示\n",
    "demonstrate_streaming_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "开始模拟 StreamingLLM 处理文本序列...\n",
    "处理 Token 0: 缓存大小=1, 注意力权重最大值=1.000\n",
    "处理 Token 1: 缓存大小=2, 注意力权重最大值=0.882\n",
    "处理 Token 2: 缓存大小=3, 注意力权重最大值=0.671\n",
    "处理 Token 3: 缓存大小=4, 注意力权重最大值=0.568\n",
    "处理 Token 4: 缓存大小=5, 注意力权重最大值=0.502\n",
    "处理 Token 5: 缓存大小=6, 注意力权重最大值=0.451\n",
    "处理 Token 6: 缓存大小=6, 注意力权重最大值=0.428\n",
    "处理 Token 7: 缓存大小=6, 注意力权重最大值=0.409\n",
    "处理 Token 8: 缓存大小=6, 注意力权重最大值=0.394\n",
    "处理 Token 9: 缓存大小=6, 注意力权重最大值=0.385\n",
    "\n",
    "最终缓存包含的 token 原始位置: [0, 1, 5, 6, 7, 8, 9]\n",
    "注意：缓存大小保持稳定（汇聚点 2 + 窗口 4 = 6），不会无限增长"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713fdf13",
   "metadata": {},
   "source": [
    "1. **数值向量替代字符串**：原代码使用字符串（如\"key_0\"）作为键值，无法进行注意力点积计算；修改为 4 维随机数值向量，符合 Transformer 中键值对的数值本质，确保注意力分数计算可执行。\n",
    "2. **缓存大小稳定性验证**：从结果可见，当处理 Token 5 后，缓存大小稳定在 6（2 个汇聚点 + 4 个滑动窗口），即使后续新增 Token（6-9），也会自动移除中间 Token（2-4），验证了\"内存不无限增长\"的核心优势。\n",
    "3. **注意力权重变化**：随着缓存中 Token 增多，注意力权重最大值从 1.0 逐渐下降并趋于稳定，说明模型能合理分配注意力到汇聚点和最新窗口，避免单一 Token 主导注意力。\n",
    "\n",
    "## 4. 位置编码适配\n",
    "\n",
    "由于 StreamingLLM 会丢弃部分中间 token，我们需要特别处理位置信息以确保模型正确理解 token 间的相对位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc322041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionAdapter:\n",
    "    \"\"\"处理 StreamingLLM 中的位置信息\"\"\"\n",
    "    def __init__(self):\n",
    "        self.original_to_current = {}  # 原始位置到当前缓存位置的映射\n",
    "    \n",
    "    def update_mapping(self, current_positions):\n",
    "        \"\"\"更新位置映射关系（current_positions 为缓存中 Token 的原始位置列表）\"\"\"\n",
    "        self.original_to_current = {orig: curr for curr, orig in enumerate(current_positions)}\n",
    "    \n",
    "    def get_relative_positions(self):\n",
    "        \"\"\"获取相对位置信息（解决中间 Token 丢弃导致的位置断裂问题）\"\"\"\n",
    "        if not self.original_to_current:\n",
    "            return []\n",
    "        \n",
    "        current_positions = list(self.original_to_current.values())\n",
    "        min_pos = min(current_positions)\n",
    "        return [pos - min_pos for pos in current_positions]\n",
    "\n",
    "# 演示位置适配器的工作方式\n",
    "def demonstrate_position_adapter():\n",
    "    \"\"\"演示 StreamingLLM 中的位置处理\"\"\"\n",
    "    adapter = PositionAdapter()\n",
    "    \n",
    "    # 模拟缓存中的位置变化（原始位置为[0, 1, 2, 3, 8, 9, 10, 11]，中间 4-7 被丢弃）\n",
    "    current_cache_positions = [0, 1, 2, 3, 8, 9, 10, 11]\n",
    "    adapter.update_mapping(current_cache_positions)\n",
    "    \n",
    "    relative_pos = adapter.get_relative_positions()\n",
    "    print(\"原始位置（缓存中保留的 Token）:\", current_cache_positions)\n",
    "    print(\"相对位置（重新编码后）:\", relative_pos)\n",
    "    print(\"原始位置→当前缓存位置的映射:\", adapter.original_to_current)\n",
    "\n",
    "# 运行演示\n",
    "demonstrate_position_adapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae115b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "原始位置（缓存中保留的 Token）: [0, 1, 2, 3, 8, 9, 10, 11]\n",
    "相对位置（重新编码后）: [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "原始位置→当前缓存位置的映射: {0: 0, 1: 1, 2: 2, 3: 3, 8: 4, 9: 5, 10: 6, 11: 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b453b2",
   "metadata": {},
   "source": [
    "位置适配器是 StreamingLLM 的关键组件：当中间 Token（如示例中的 4-7）被丢弃后，剩余 Token 的原始位置存在断裂（从 3 直接跳到 8），若直接使用原始位置会导致位置编码错误。适配器通过以下两步解决问题：\n",
    "\n",
    "1. **建立映射**：记录缓存中每个 Token 的\"原始位置→当前缓存索引\"关系；\n",
    "2. **相对编码**：将原始位置重新映射为连续的相对位置（如 8→4、9→5），确保 Transformer 能正确理解 Token 间的顺序关系。\n",
    "\n",
    "## 5. 完整实验演示\n",
    "\n",
    "下面我们通过一个完整的例子演示 StreamingLLM 的简化工作流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0740141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_streaming_demo():\n",
    "    \"\"\"完整的 StreamingLLM 简化演示（整合所有修改：数值向量、位置适配、缓存管理）\"\"\"\n",
    "    # 初始化参数\n",
    "    sink_size = 4  # 4 个注意力汇聚点\n",
    "    window_size = 8  # 8 个 Token 的滑动窗口\n",
    "    token_dim = 4  # Token 的向量维度（与注意力计算一致）\n",
    "    \n",
    "    # 初始化组件\n",
    "    cache = SimpleStreamingCache(sink_size, window_size)\n",
    "    pos_adapter = PositionAdapter()\n",
    "    \n",
    "    print(\"开始 StreamingLLM 完整演示...\")\n",
    "    print(f\"配置: 注意力汇聚点={sink_size}, 滑动窗口大小={window_size}, Token 向量维度={token_dim}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 模拟处理 20 个 token（远超初始缓存容量 4+8=12）\n",
    "    for token_idx in range(20):\n",
    "        # 生成新 token 的键值对（数值向量）\n",
    "        new_key = [[random.random() for _ in range(token_dim)]]\n",
    "        new_value = [[random.random() for _ in range(token_dim)]]\n",
    "        \n",
    "        # 更新缓存与位置映射\n",
    "        cache.update(new_key, new_value)\n",
    "        pos_adapter.update_mapping(cache.token_positions)\n",
    "        \n",
    "        # 每 5 个 token 显示一次状态（便于观察缓存变化）\n",
    "        if (token_idx + 1) % 5 == 0:\n",
    "            keys, values = cache.get_cache()\n",
    "            rel_pos = pos_adapter.get_relative_positions()\n",
    "            \n",
    "            print(f\"已处理 Token 总数: {token_idx + 1}\")\n",
    "            print(f\"当前缓存大小: {len(keys)}（目标容量：{sink_size + window_size}）\")\n",
    "            print(f\"缓存中 Token 的原始位置: {cache.token_positions}\")\n",
    "            print(f\"重新编码后的相对位置: {rel_pos}\")\n",
    "            print(\"-\" * 30)\n",
    "    \n",
    "    print(\"演示完成！StreamingLLM 成功处理了 20 个 Token（远超初始缓存容量）\")\n",
    "    print(f\"最终缓存大小: {len(cache.key_cache)}（稳定在目标容量）\")\n",
    "    print(f\"最终缓存中的 Token 原始位置: {cache.token_positions}\")\n",
    "\n",
    "# 运行完整演示\n",
    "complete_streaming_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "开始 StreamingLLM 完整演示...\n",
    "配置: 注意力汇聚点=4, 滑动窗口大小=8, Token 向量维度=4\n",
    "--------------------------------------------------\n",
    "已处理 Token 总数: 5\n",
    "当前缓存大小: 5（目标容量：12）\n",
    "缓存中 Token 的原始位置: [0, 1, 2, 3, 4]\n",
    "重新编码后的相对位置: [0, 1, 2, 3, 4]\n",
    "------------------------------\n",
    "已处理 Token 总数: 10\n",
    "当前缓存大小: 10（目标容量：12）\n",
    "缓存中 Token 的原始位置: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "重新编码后的相对位置: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "------------------------------\n",
    "已处理 Token 总数: 15\n",
    "当前缓存大小: 12（目标容量：12）\n",
    "缓存中 Token 的原始位置: [0, 1, 2, 3, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "重新编码后的相对位置: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "------------------------------\n",
    "已处理 Token 总数: 20\n",
    "当前缓存大小: 12（目标容量：12）\n",
    "缓存中 Token 的原始位置: [0, 1, 2, 3, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "重新编码后的相对位置: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "------------------------------\n",
    "演示完成！StreamingLLM 成功处理了 20 个 Token（远超初始缓存容量）\n",
    "最终缓存大小: 12（稳定在目标容量）\n",
    "最终缓存中的 Token 原始位置: [0, 1, 2, 3, 12, 13, 14, 15, 16, 17, 18, 19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29642e3",
   "metadata": {},
   "source": [
    "这个完整演示整合了 StreamingLLM 的三大核心组件，验证了其端到端工作能力：\n",
    "\n",
    "1. **缓存容量稳定性**：当处理 Token 数达到 15 后，缓存大小稳定在 12（4 个汇聚点 + 8 个滑动窗口），即使继续处理到 20 个 Token，容量也不再增长，证明内存控制有效；\n",
    "2. **中间 Token 自动丢弃**：从\"缓存中 Token 的原始位置\"可见，初始 4 个汇聚点（0-3）始终保留，后续仅保留最新的 8 个 Token（如处理 20 个 Token 后，保留 12-19），中间 Token（4-11）被自动移除；\n",
    "3. **位置编码正确性**：相对位置始终保持连续（0-11），即使原始位置存在断裂，也能通过位置适配器重新编码，确保模型理解 Token 顺序。\n",
    "\n",
    "## 实验总结\n",
    "\n",
    "1. **无需微调即可处理超长文本**：基于注意力汇聚点和滑动窗口，模型能在不修改架构、不额外训练的情况下，处理远超预训练长度的文本；\n",
    "2. **内存使用稳定**：缓存大小始终控制在\"汇聚点 + 滑动窗口\"的固定容量，避免传统 KV Cache 随序列长度线性增长的问题；\n",
    "3. **性能损失小**：保留的注意力汇聚点能维持注意力计算的稳定性，避免单纯滑动窗口导致的性能骤降。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
