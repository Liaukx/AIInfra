{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa2a5c1",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 02: PagedAttention 复现\n",
    "\n",
    "这篇文章将带你深入理解 PagedAttention 的工作原理，并通过简化的 Python 实现来展示其核心机制，帮助你在有限资源下高效运行大模型。\n",
    "\n",
    "## 大模型推理内存挑战\n",
    "\n",
    "随着大语言模型在自然语言处理、图像识别等领域的广泛应用，其庞大的参数规模和高内存需求已成为实际部署的主要瓶颈。特别是在移动设备和资源受限环境中，有限的内存容量往往无法满足大模型运行的基本需求。Transformer 模型在自回归推理过程中需要存储历史计算的键值对（KV Cache），这部分内存占用随着序列长度增加而线性增长，成为显存使用的主要因素。\n",
    "\n",
    "传统 KV Cache 管理方式存在两个突出问题：一是**内存碎片化**，由于不同序列长度变化不可预测，导致显存分配不连续；二是**过度保留**，系统为应对可能的长序列往往预先分配过多显存，实际利用率却很低。研究表明，现有系统中 60%-80% 的显存因此被浪费。\n",
    "\n",
    "PagedAttention 技术借鉴了操作系统中的虚拟内存和分页思想，通过将 KV Cache 划分为固定大小的块（block）并动态管理，显著提高了内存利用效率，使大模型在资源受限环境中的部署成为可能。\n",
    "\n",
    "## 2. PagedAttention 原理\n",
    "\n",
    "PagedAttention 的核心创新在于将操作系统的**分页机制**引入到注意力计算中。与传统方法需要为每个序列分配连续内存空间不同，PagedAttention 将键值缓存分割成固定大小的页面（page），每个页面存储一定数量 token 的键和值。\n",
    "\n",
    "这种设计使得内存分配从连续变为非连续，通过一个块表（block table）来维护逻辑页面到物理页面的映射关系，类似于操作系统中的页表机制。当序列增长需要更多空间时，系统只需分配新的物理页面并更新块表，无需重新分配整个连续内存空间。\n",
    "\n",
    "从数学角度看，传统的注意力计算可以表示为：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "其中 $Q$、$K$、$V$ 分别是查询、键和值矩阵，$d_k$ 是键向量的维度。\n",
    "\n",
    "在 PagedAttention 中，由于键和值被分散存储在多个页面中，注意力计算需要按页面进行：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\sum_{b=1}^{N/B} \\text{softmax}\\left(\\frac{QK_b^T}{\\sqrt{d_k}}\\right)V_b\n",
    "$$\n",
    "\n",
    "这里 $K_b$ 和 $V_b$ 表示第 $b$ 个页面的键和值，$B$ 是每个页面存储的 token 数量。这种分页计算方式虽然增加了页面管理开销，但大大降低了内存碎片化。\n",
    "\n",
    "PagedAttention 的另一个重要优势是支持**高效的内存共享**。在并行采样或波束搜索等场景中，多个输出序列通常由同一个提示（prompt）生成。传统方法需要为每个序列单独存储键值缓存，而 PagedAttention 允许不同序列共享相同的物理页面，只需在块表中设置不同的映射关系即可。\n",
    "\n",
    "这种内存共享机制显著减少了重复内容的内存占用，提高了整体吞吐量，特别是在处理长提示文本时效果更加明显。\n",
    "\n",
    "## 3 页面与块表结构\n",
    "\n",
    "下面我们通过一个简化的 Python 实现来展示 PagedAttention 的核心机制。为了使代码易于理解，我们省略了生产环境中的一些优化措施。\n",
    "\n",
    "首先定义页面（Page）和块表（PageTable）的数据结构，这是 PagedAttention 的基础："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07491838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class Page:\n",
    "    \"\"\"表示一个物理页面，存储固定数量 token 的键值对\"\"\"\n",
    "    def __init__(self, page_size, num_heads, head_dim):\n",
    "        self.page_size = page_size  # 每个页面存储的 token 数量\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        # 初始化键值存储空间\n",
    "        self.keys = torch.zeros(page_size, num_heads, head_dim)\n",
    "        self.values = torch.zeros(page_size, num_heads, head_dim)\n",
    "        self.ref_count = 0  # 引用计数，用于页面复用\n",
    "\n",
    "    def update_access(self):\n",
    "        \"\"\"更新页面访问信息，用于实现缓存替换策略\"\"\"\n",
    "        self.ref_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6c273",
   "metadata": {},
   "source": [
    "`Page` 类表示一个物理页面，类似于操作系统中的内存页。每个页面有固定容量（`page_size`），存储多个 token 的键和值。`ref_count` 字段记录页面被引用的次数，用于实现类似 LRU 的页面置换算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684064db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageTable:\n",
    "    \"\"\"管理逻辑页面到物理页面的映射关系\"\"\"\n",
    "    def __init__(self):\n",
    "        self.logical_to_physical = {}  # 映射表：逻辑页面 ID → 物理页面 ID\n",
    "\n",
    "    def map_page(self, logical_page_id, physical_page_id):\n",
    "        \"\"\"建立逻辑页面到物理页面的映射\"\"\"\n",
    "        self.logical_to_physical[logical_page_id] = physical_page_id\n",
    "\n",
    "    def get_physical_page(self, logical_page_id):\n",
    "        \"\"\"根据逻辑页面 ID 获取物理页面 ID\"\"\"\n",
    "        return self.logical_to_physical.get(logical_page_id, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7b17a",
   "metadata": {},
   "source": [
    "`PageTable` 类管理逻辑页面与物理页面的映射关系，类似于操作系统的页表。当需要访问特定逻辑页面时，通过这个映射表找到对应的物理页面位置。\n",
    "\n",
    "## 4. 块管理器与序列管理\n",
    "\n",
    "接下来实现块管理器（BlockManager）和序列管理器（SequenceManager），负责页面的分配、回收和序列的页面映射："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a80b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockManager:\n",
    "    \"\"\"管理全局页面池，负责页面的分配和回收\"\"\"\n",
    "    def __init__(self, num_pages, page_size, num_heads, head_dim):\n",
    "        self.num_pages = num_pages\n",
    "        self.page_size = page_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        # 初始化页面池\n",
    "        self.pages = [Page(page_size, num_heads, head_dim) for _ in range(num_pages)]\n",
    "        self.free_pages = list(range(num_pages))  # 空闲页面列表\n",
    "        self.allocated_pages = set()  # 已分配页面集合\n",
    "\n",
    "    def allocate_page(self):\n",
    "        \"\"\"从内存池中分配一个物理页面\"\"\"\n",
    "        if not self.free_pages:\n",
    "            raise RuntimeError(\"No free pages available\")\n",
    "        page_id = self.free_pages.pop()\n",
    "        self.allocated_pages.add(page_id)\n",
    "        self.pages[page_id].ref_count += 1\n",
    "        return page_id\n",
    "\n",
    "    def free_page(self, page_id):\n",
    "        \"\"\"释放页面回到内存池\"\"\"\n",
    "        if page_id in self.allocated_pages:\n",
    "            self.pages[page_id].ref_count -= 1\n",
    "            if self.pages[page_id].ref_count == 0:\n",
    "                self.allocated_pages.remove(page_id)\n",
    "                self.free_pages.append(page_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eecca6",
   "metadata": {},
   "source": [
    "`BlockManager` 管理物理页面池，处理页面分配和回收。它维护了一个空闲页面列表和已分配页面集合，采用**引用计数**机制确保页面安全复用。当页面引用计数降为零时，页面被回收至空闲池。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceManager:\n",
    "    \"\"\"管理每个序列的页面映射和令牌存储\"\"\"\n",
    "    def __init__(self, block_manager):\n",
    "        self.block_manager = block_manager\n",
    "        self.sequences = {}  # 存储序列 ID 到页表的映射\n",
    "\n",
    "    def create_sequence(self, seq_id):\n",
    "        \"\"\"创建新序列并初始化页表\"\"\"\n",
    "        page_table = PageTable()\n",
    "        self.sequences[seq_id] = page_table\n",
    "        return page_table\n",
    "\n",
    "    def append_token(self, seq_id, token_pos, key, value):\n",
    "        \"\"\"为序列添加 token 的键值对\"\"\"\n",
    "        page_table = self.sequences[seq_id]\n",
    "        # 计算逻辑页面 ID 和页面内偏移量\n",
    "        logical_page_id = token_pos // self.block_manager.page_size\n",
    "        page_offset = token_pos % self.block_manager.page_size\n",
    "        \n",
    "        # 获取或分配物理页面\n",
    "        physical_page_id = page_table.get_physical_page(logical_page_id)\n",
    "        if physical_page_id == -1:\n",
    "            physical_page_id = self.block_manager.allocate_page()\n",
    "            page_table.map_page(logical_page_id, physical_page_id)\n",
    "        \n",
    "        # 将键值对写入页面\n",
    "        page = self.block_manager.pages[physical_page_id]\n",
    "        page.keys[page_offset] = key\n",
    "        page.values[page_offset] = value\n",
    "        page.update_access()  # 更新页面访问信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7c1ad",
   "metadata": {},
   "source": [
    "`SequenceManager` 为每个序列维护独立的页表。当写入新 token 时，它计算 token 应该所在的逻辑页面和页面内偏移量。如果逻辑页面尚未映射到物理页面，则从块管理器分配新物理页面。这种设计使得不同序列的页面可以混合存储在物理内存中，减少了内存碎片。\n",
    "\n",
    "## 5. PagedAttention 计算\n",
    "\n",
    "现在实现分页注意力计算的核心逻辑，从分散的页面收集键值并执行注意力运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b67cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedAttention:\n",
    "    \"\"\"执行分页注意力计算\"\"\"\n",
    "    def __init__(self, block_manager):\n",
    "        self.block_manager = block_manager\n",
    "\n",
    "    def compute_attention(self, query, page_table, seq_len):\n",
    "        \"\"\"\n",
    "        计算分页注意力\n",
    "        query: [batch_size, num_heads, head_dim]\n",
    "        page_table: 序列的页表\n",
    "        seq_len: 当前序列长度\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, head_dim = query.shape\n",
    "        scale = 1.0 / math.sqrt(head_dim)\n",
    "        \n",
    "        # 收集所有页面的键值\n",
    "        all_keys, all_values = [], []\n",
    "        num_pages = (seq_len + self.block_manager.page_size - 1) // self.block_manager.page_size\n",
    "        \n",
    "        for logical_page_id in range(num_pages):\n",
    "            physical_page_id = page_table.get_physical_page(logical_page_id)\n",
    "            if physical_page_id == -1:\n",
    "                continue  # 跳过未分配的页面\n",
    "            page = self.block_manager.pages[physical_page_id]\n",
    "            # 计算当前页面有效的 token 数量（最后一页可能不满）\n",
    "            start_idx = logical_page_id * self.block_manager.page_size\n",
    "            valid_tokens = min(self.block_manager.page_size, seq_len - start_idx)\n",
    "            # 提取有效键值\n",
    "            page_keys = page.keys[:valid_tokens]\n",
    "            page_values = page.values[:valid_tokens]\n",
    "            all_keys.append(page_keys)\n",
    "            all_values.append(page_values)\n",
    "        \n",
    "        if not all_keys:\n",
    "            return torch.zeros(batch_size, num_heads, head_dim)\n",
    "        \n",
    "        # 拼接所有键值\n",
    "        keys = torch.cat(all_keys, dim=0)  # [seq_len, num_heads, head_dim]\n",
    "        values = torch.cat(all_values, dim=0)  # [seq_len, num_heads, head_dim]\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        query = query.unsqueeze(2)  # [batch_size, num_heads, 1, head_dim]\n",
    "        keys = keys.transpose(0, 1).unsqueeze(0)  # [1, num_heads, seq_len, head_dim]\n",
    "        scores = torch.matmul(query, keys.transpose(-2, -1)) * scale  # [batch_size, num_heads, 1, seq_len]\n",
    "        scores = scores.squeeze(2)  # [batch_size, num_heads, seq_len]\n",
    "        \n",
    "        # Softmax 和加权求和\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        values = values.transpose(0, 1).unsqueeze(0)  # [1, num_heads, seq_len, head_dim]\n",
    "        output = torch.sum(attention_weights.unsqueeze(-1) * values, dim=2)\n",
    "        return output  # [batch_size, num_heads, head_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9fe22",
   "metadata": {},
   "source": [
    "`PagedAttention` 类负责实际的分页注意力计算。它首先根据页表收集所有相关的键值页面，考虑到最后一页可能不满的情况。然后将这些页面拼接成完整的键值矩阵，执行标准的注意力计算。这种实现虽然增加了页面收集的开销，但大大降低了内存碎片化，提高了内存利用率。\n",
    "\n",
    "## 6. 实验验证\n",
    "\n",
    "以下代码演示了 PagedAttention 的完整工作流程，模拟了实际推理场景："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验设置和参数初始化\n",
    "num_pages = 100\n",
    "page_size = 8  # 每个页面存储 8 个 token\n",
    "num_heads = 4\n",
    "head_dim = 16\n",
    "batch_size = 1\n",
    "\n",
    "# 初始化管理器和注意力模块\n",
    "block_manager = BlockManager(num_pages, page_size, num_heads, head_dim)\n",
    "seq_manager = SequenceManager(block_manager)\n",
    "paged_attn = PagedAttention(block_manager)\n",
    "\n",
    "# 创建序列\n",
    "seq_id = 0\n",
    "page_table = seq_manager.create_sequence(seq_id)\n",
    "seq_len = 0\n",
    "\n",
    "# 模拟生成 20 个 token 的过程\n",
    "print(\"开始模拟生成 20 个 token 的过程...\")\n",
    "for token_pos in range(20):\n",
    "    # 生成随机键值（模拟 Transformer 层的输出）\n",
    "    key = torch.randn(num_heads, head_dim)\n",
    "    value = torch.randn(num_heads, head_dim)\n",
    "    # 存储到 Cache\n",
    "    seq_manager.append_token(seq_id, token_pos, key, value)\n",
    "    seq_len += 1\n",
    "    \n",
    "    # 每 5 个 token 计算一次注意力\n",
    "    if (token_pos + 1) % 5 == 0:\n",
    "        query = torch.randn(batch_size, num_heads, head_dim)\n",
    "        output = paged_attn.compute_attention(query, page_table, seq_len)\n",
    "        print(f\"已处理 Token 数量: {token_pos+1}, 注意力输出形状: {output.shape}\")\n",
    "        print(f\"当前使用的物理页面数: {len([pid for pid in page_table.logical_to_physical.values() if pid != -1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b2309f",
   "metadata": {},
   "source": [
    "这段实验代码展示了 PagedAttention 的完整工作流程。它模拟了生成 20 个 token 的过程，每生成 5 个 token 计算一次注意力。通过输出信息可以看到内存使用情况，验证了 PagedAttention 的动态内存分配特性。\n",
    "\n",
    "## 7. 总结与思考\n",
    "\n",
    "PagedAttention 技术已广泛应用于多种大模型推理场景。在**长序列处理**中，如长文档生成、代码补全等任务，PagedAttention 通过分页机制有效支持了万级别 token 长度的序列，突破了传统方法的内存限制。\n",
    "\n",
    "在高并发推理服务中，PagedAttention 的**内存共享特性**特别有价值。多个请求可以共享相同的提示页面，显著提高了吞吐量和资源利用率。vLLM 框架基于 PagedAttention 实现了高达 23 倍的吞吐量提升。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
