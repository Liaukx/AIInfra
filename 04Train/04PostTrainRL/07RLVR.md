## RLVR（Reinforcement Learning with Verifiable Rewards）

Author by: 潘江


RLVR 通过使用可验证的、基于规则的奖励函数，为模型提供明确的二元反馈（正确为 1，错误为 0），从而优化其性能。与传统的强化学习从人类反馈（RLHF）不同，RLVR 避免了主观人类评估或复杂奖励模型的依赖，使训练过程更加透明、高效。这种方法特别适用于数学推理、代码生成等具有明确正确性标准的任务。

RLVR 的概念由 Nathan Lambert 在 Tulu3 项目中提出，具体见 2024 年发表的论文《Tulu 3: 推动开放语言模型后训练的前沿》（Tulu 3 Paper）。

RLVR 的核心在于使用可验证的奖励函数，这些函数通过确定性规则自动评估模型输出的正确性，提供二元奖励信号（1 表示正确，0 表示错误）。这种方法与 RLHF 形成鲜明对比，后者依赖于人类反馈或训练的奖励模型，可能引入主观性或复杂性。

#### RLVR 的实现步骤

1 数据收集与整理：准备包含正确答案或解决方案的数据集。例如，在数学推理任务中，可以使用 GSM8K 数据集，其中每个问题都有明确的正确答案。

2 奖励函数设计：定义一个基于规则的奖励函数，用于自动验证模型输出。奖励函数必须明确且可重复，例如：

数学正确性：如果模型的答案与正确答案匹配，奖励为 1；否则为 0。
代码执行：如果生成的代码通过所有测试用例，奖励为+1；如果任一测试用例失败，奖励为-1；如果代码无效，奖励为-0.2。
指令遵循：如果输出符合指定格式，奖励为 1；否则为 0。


3 奖励模型验证：确保奖励函数准确反映期望行为，避免错误奖励或漏洞。这一步骤对于确保训练过程的可靠性至关重要。

4 与强化学习算法集成：将奖励函数融入强化学习框架，通常使用近端策略优化（PPO）算法，通过最大化期望奖励来训练模型。

下面是一些常见的奖励

| 任务类型       | 奖励函数描述                                                                 | 示例奖励值                     |
|----------------|----------------------------------------------------------------------------|-------------------------------|
| **数学推理**   | 检查模型答案是否与正确答案匹配                                               | 正确：`+1.0`，错误：`0.0`       |
| **代码生成**   | 执行代码并验证是否通过所有测试用例                                           | 全通过：`+1.0`，部分失败：`-1.0`，无效代码：`-0.2` |
| **指令遵循**   | 检查输出是否符合指定格式（如 JSON/XML 结构、关键词包含等）                     | 符合：`+1.0`，不符合：`0.0`     |
| **安全对话**   | 检测是否包含危险内容（暴力/歧视/隐私泄露等）                                 | 安全：`+1.0`，危险：`-10.0`     |
| **多步推理**   | 分步验证推导过程逻辑正确性（需中间步骤验证）                                 | 每正确步骤：`+0.2`，最终答案正确额外`+0.5` |
| **创意写作**   | 评估创意性（n-gram 多样性）和基础质量（语法正确性）                           | 语法正确：`+0.5`，高创意性额外`+0.5` |


#### 应用
数学推理：通过验证模型答案与正确答案的一致性，RLVR 可训练模型解决复杂的数学问题。例如，在 GSM8K 数据集上，模型可以通过奖励函数学习提供正确答案和格式。

代码生成：RLVR 通过执行生成的代码并检查是否通过测试用例，确保代码的正确性和功能性。这在软件开发和自动化测试中尤为重要。

指令遵循：RLVR 可用于训练模型严格遵循特定指令格式，例如在问答系统中输出特定结构的答案。

事实准确性：通过与已验证的知识库对比，RLVR 可提高模型生成内容的准确性，适用于新闻验证或教育内容生成。

逻辑一致性：RLVR 可确保模型输出符合逻辑规则，适用于法律分析或科学推理。

法规遵从：在医疗或金融领域，RLVR 可通过验证模型输出是否符合法规要求（如处理敏感数据），提高模型的可靠性。
