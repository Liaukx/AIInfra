# DAPO

Author by: 潘江

论文：《DAPO: an Open-Source LLM Reinforcement Learning System at Scale》
## 一、为什么需要 DAPO？

在大语言模型（LLM）的发展中，强化学习（RL）是推动模型复杂推理能力的核心技术。然而，现有先进模型（如 OpenAI 的 O1、DeepSeek 的 R1）的强化学习细节往往被隐藏，导致社区难以复现其结果。

字节跳动提出的**DAPO（解耦裁剪和动态采样策略优化）** 算法，通过完全开源的方式，揭示了大规模 LLM 强化学习的关键技术。该系统基于 Qwen2.5-32B 基础模型，在 AIME 2024（数学竞赛任务）上达到 50 分，仅用 50%的训练步骤就超过了此前最先进的 DeepSeek-R1 结果（47 分）。


## 二、基础知识铺垫

### 2.1 近端策略优化（PPO）

PPO 是强化学习中广泛使用的算法，其核心思想是通过**裁剪机制**限制策略更新的幅度，保证训练稳定性：

目标函数：
$$L_{CLIP}(\theta) = \mathbb{E}[min(r_t(\theta)\hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]$$

其中：
- $r_t(\theta)$ 是重要性采样比率
- $\hat{A}_t$ 是优势估计（使用 GAE 计算）
- $\epsilon$ 是裁剪范围（通常取 0.2）

### 2.2 群体相对策略优化（GRPO）

GRPO 的改进点：
- 移除价值函数，以群体相对方式估计优势
- 对特定问题-答案对，采样 G 个独立响应，通过归一化群体奖励计算优势：

$$\hat{A}_{i,t} = \frac{r_i - \text{mean}(\{R_i\}_{i=1}^G)}{\text{std}(\{R_i\}_{i=1}^G)}$$

- 采用裁剪目标并施加 KL 惩罚项

### 2.3 DAPO 的基础改进

DAPO 在 GRPO 基础上的关键调整：
- 移除 KL 散度惩罚项（长思维链推理不需要限制与初始模型的偏离）
- 使用基于规则的奖励模型：
  $$R(\hat{y}, y) = \begin{cases} 1, & \text{is\_equivalent}(\hat{y}, y) \\ -1, & \text{otherwise} \end{cases}$$

## 三、DAPO 核心技术详解

### 3.1 裁剪偏移（Clip-Shifting）

**问题背景**：
朴素 PPO/GRPO 中，策略熵会随训练迅速降低，导致探索不足（采样响应趋同）。原因是默认裁剪范围（$\epsilon=0.2$）限制了低概率 token 的概率提升空间。

**解决方案**：
将下裁剪和上裁剪范围解耦为 $\epsilon_{\text{low}}$ 和 $\epsilon_{\text{high}}$：
- 增大 $\epsilon_{\text{high}}$，为低概率 token 的概率提升留出空间
- 保持 $\epsilon_{\text{low}}$ 较小，避免抑制 token 概率导致采样空间崩塌

**效果**：有效提高策略熵，促进生成更多样化的样本。

### 3.2 动态采样（Dynamic Sampling）

**问题背景**：
当特定提示的所有输出都正确（奖励均为 1）时，优势为零，导致无梯度更新，降低样本效率。

**解决方案**：
- 对准确率为 1 的提示进行过采样和过滤
- 确保批次中只保留有有效梯度的样本（准确率既不为 0 也不为 1）
- 保持批次中提示数量的一致性

**效果**：解决梯度减少问题，虽然需要采样更多数据，但因所需训练步骤减少，总体收敛时间反而缩短。

### 3.3 Token 级策略梯度损失

**问题背景**：
GRPO 采用样本级损失计算（先按 token 平均，再汇总样本），导致：
- 长响应中的 token 对整体损失贡献不成比例地低
- 无法有效惩罚长样本中的不良模式（如胡言乱语、重复）

**解决方案**：
采用 Token 级损失计算，使：
- 较长序列对梯度更新的影响更大
- 单个 token 的生成模式无论出现在哪种长度的响应中，都会被同等地促进或抑制

**效果**：增强训练稳定性，使响应长度增长更健康。

### 3.4 溢出奖励塑造（Overflowing Reward Shaping）

**问题背景**：
对超长截断样本分配固定惩罚性奖励会引入噪声（合理推理可能因长度被惩罚）。

**解决方案**：
1. 超长过滤：屏蔽截断样本的损失
2. 软超长惩罚：对截断样本采用长度感知惩罚机制
   - 定义惩罚区间，响应越长，惩罚越大
   - 将惩罚添加到原始正确性奖励中

**效果**：显著稳定训练并提高性能。

## 四、实验配置与结果

### 4.1 关键配置

| 项目 | 详情 |
|------|------|
| 基础模型 | Qwen2.5-32B |
| 框架 | verl |
| 优化器 | AdamW，学习率 1×10⁻⁶，20 步线性预热 |
| Rollout 配置 | 提示批量大小 512，每个提示采样 16 个响应 |
| 训练批量 | 小批量大小 512（每次 rollout 16 次梯度更新） |
| 最大 Token 数 | 20,480（含 4,906 个软惩罚缓存） |
| 裁剪参数 | $c_{\text{low}}=0.2$，$c_{\text{high}}=0.28$ |

### 4.2 性能提升分解

| 模型/配置 | AIME24_avg@32 |
|-----------|---------------|
| DeepSeek-R1-Zero-Qwen-32B | 47 |
| 朴素 GRPO | 30 |
| + 超长过滤 | 36 |
| + 更高裁剪 | 38 |
| + 软超长惩罚 | 41 |
| + Token 级损失 | 42 |
| + 动态采样（DAPO） | 50 |

## 五、训练动态监测指标

在大规模 LLM 强化学习中，需密切关注以下指标：

1. **生成响应长度**：
   - 与训练稳定性和性能密切相关
   - 长度增加提供更大探索空间，但需与验证准确率共同评估

2. **奖励动态**：
   - 训练集上奖励增加趋势通常稳定
   - 注意：训练集最终奖励与验证集准确率相关性低，需警惕过拟合

3. **模型熵和生成概率**：
   - 熵需保持在适当范围（过低：探索不足；过高：不稳定）
   - 监测生成概率分布变化

4. **推理模式演变**：
   - 算法会强化现有有效推理模式，并产生全新模式（如训练后期出现反思行为）

## 六、数据集处理

DAPO 使用的 DAPO-Math-17K 数据集处理流程：

1. 数据来源：AoPS 网站和官方竞赛主页（网络爬取+人工注释）
2. 关键处理：将数学答案统一转换为整数
   - 例：将 $\frac{a+\sqrt{b}}{c}$ 转换为 $a + b + c$
   - 目的：简化奖励计算，减少公式解析错误



## 七、常见疑问解答

**Q1：Token 级损失如何在基于结果的奖励中发挥作用？**  
A1：虽然奖励基于最终结果，但 Token 级损失确保长序列中的每个 token 都能获得适当的梯度信号，解决了样本级损失中长序列 token 贡献被稀释的问题（信用分配问题）。

**Q2：为什么 Clip-Higher 能提高熵？**  
A2：通过增大上裁剪阈值，低概率 token 有更多机会提升概率，增加了策略的多样性，从而提高熵。若仅限制下裁剪，会抑制高概率 token 的过度优势。

**Q3：动态采样是否会降低训练效率？**  
A3：不会。虽然需要采样更多数据，但过滤掉零梯度样本后，有效训练信号增强，所需总训练步骤减少，整体收敛时间缩短。



