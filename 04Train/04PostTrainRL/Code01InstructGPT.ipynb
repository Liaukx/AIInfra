{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a13519",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 01: 经典 InstructGPT 复现\n",
    "\n",
    "> Author by: 张玉霖\n",
    "\n",
    "本实验将使用简化代码复现 InstructGPT 的核心训练流程：监督微调(SFT)→奖励模型训练(RM)→PPO 强化学习优化。通过这个实验，我们可以深入理解如何通过人类反馈强化学习将人类偏好传递给语言模型。\n",
    "\n",
    "实验以 **教学用最小实现（toy）** 的方式演示 InstructGPT 的训练三阶段：**监督微调 (SFT)** → **奖励模型 (RM)** → **PPO 强化学习**。我们不追求大模型效果，而是聚焦**关键接口与损失函数**，用小数据集展示 RLHF 的核心思路：**先通过 SFT 学会跟随指令，再用奖励模型评分，最后用 PPO 让策略在不偏离过远的前提下朝更高奖励更新**。\n",
    "\n",
    "\n",
    "## 1. 环境设置\n",
    "\n",
    "首先安装必要的依赖库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch numpy tqdm matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e0a29",
   "metadata": {},
   "source": [
    "现在导入实验所需的模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff28328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f162e",
   "metadata": {},
   "source": [
    "## 2. Stage1：监督微调 SFT\n",
    "\n",
    "监督微调阶段使用人类标注的指令-回答对来微调预训练语言模型，使其初步学会遵循指令。\n",
    "\n",
    "在 SFT 阶段，我们使用标准的监督学习方式，使用人类标注的高质量问答对数据来微调预训练语言模型。模型的训练目标是最大化在给定指令情况下，生成人类期望回答的似然概率。\n",
    "\n",
    "数学上，这个目标可以表示为：\n",
    "\n",
    "$$ \\mathcal{L}_{SFT} = -\\mathbb{E}_{(x,y)\\sim D}[\\log P_{\\theta}(y|x)] $$\n",
    "\n",
    "其中 $x$ 是指令，$y$ 是人类标注的回答，$\\theta$ 是模型参数。\n",
    "\n",
    "### 2.1 数据准备\n",
    "\n",
    "我们创建一个简化的指令-回答数据集，并实现一个简单的文本编码器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19772221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"字节级分词器，用于将文本转换为模型可处理的 token 序列。\n",
    "    \n",
    "    该分词器将文本编码为 UTF-8 字节序列，并使用特殊标记组织输入格式。\n",
    "    支持中文、英文等各种字符，采用[BOS]指令[SEP]回答[EOS][PAD...]模板结构。\n",
    "    \n",
    "    Attributes:\n",
    "        PAD (int): 填充标记 ID，值为 0\n",
    "        BOS (int): 开始标记 ID，值为 1\n",
    "        SEP (int): 分隔符标记 ID，值为 2\n",
    "        EOS (int): 结束标记 ID，值为 3\n",
    "        max_len (int): 最大序列长度\n",
    "        vocab_size (int): 词汇表大小（256 个 UTF-8 字节 + 4 个特殊标记）\n",
    "    \"\"\"\n",
    "    PAD, BOS, SEP, EOS = 0, 1, 2, 3  # 特殊符号固定 0~3\n",
    "\n",
    "    def __init__(self, max_len: int = 128):\n",
    "        \"\"\"初始化 ByteTokenizer。\n",
    "        \n",
    "        Args:\n",
    "            max_len (int): 序列最大长度，默认为 128\n",
    "        \"\"\"\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = 256 + 4\n",
    "\n",
    "    def _to_bytes(self, text: str):\n",
    "        \"\"\"将文本转换为 UTF-8 字节序列。\n",
    "        \n",
    "        Args:\n",
    "            text (str): 输入文本\n",
    "            \n",
    "        Returns:\n",
    "            list: UTF-8 字节值列表\n",
    "        \"\"\"\n",
    "        return list(text.encode(\"utf-8\", errors=\"replace\"))\n",
    "\n",
    "    def encode_dialog(self, instr: str, resp: str):\n",
    "        \"\"\"将指令-回答对话编码为模型输入格式。\n",
    "        \n",
    "        使用[BOS]指令[SEP]回答[EOS][PAD...]模板编码对话，\n",
    "        并生成相应的注意力掩码和训练标签。\n",
    "        \n",
    "        Args:\n",
    "            instr (str): 指令文本\n",
    "            resp (str): 回答文本\n",
    "            \n",
    "        Returns:\n",
    "            dict: 包含以下键值的字典：\n",
    "                - input_ids (torch.Tensor): 输入 token ID 序列，形状[T]\n",
    "                - attention_mask (torch.Tensor): 注意力掩码，形状[T]\n",
    "                - labels (torch.Tensor): 训练标签，形状[T]，非回答段为-100\n",
    "        \"\"\"\n",
    "        # 将指令和回答文本转换为字节序列\n",
    "        instr_bytes = self._to_bytes(f\"Instruct:{instr}\")\n",
    "\n",
    "        # 将 Answer:前缀去掉，只保留纯回答内容\n",
    "        # 因为根据实验发现，加上 Answer:前缀会导致模型生成时重复输出 Answer\n",
    "        # resp_bytes  = self._to_bytes(f\"Answer:{resp}\")\n",
    "        resp_bytes  = self._to_bytes(f\"{resp}\")\n",
    "\n",
    "        # 构建完整的 token ID 序列\n",
    "        ids = [self.BOS]    # 开始标记\n",
    "        # 指令部分：每个字节偏移+4 以避开特殊符号\n",
    "        ids += [b + 4 for b in instr_bytes]\n",
    "        ids += [self.SEP]   # 分隔符，用于分隔指令和回答\n",
    "        # 回答部分：每个字节偏移+4 以避开特殊符号\n",
    "        ids += [b + 4 for b in resp_bytes]\n",
    "        ids += [self.EOS]   # 结束标记\n",
    "\n",
    "        # 截断处理：如果序列超过最大长度，保留 EOS 在末端\n",
    "        if len(ids) > self.max_len:\n",
    "            ids = ids[: self.max_len - 1] + [self.EOS]\n",
    "\n",
    "        # 构建注意力掩码：有效位置为 1，填充位置为 0\n",
    "        attn = [1] * len(ids)\n",
    "\n",
    "        # 填充处理：如果序列长度不足最大长度，用 PAD 填充\n",
    "        if len(ids) < self.max_len:\n",
    "            pad = self.max_len - len(ids)\n",
    "            ids += [self.PAD] * pad\n",
    "            attn += [0] * pad\n",
    "        \n",
    "        input_ids = torch.tensor(ids, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(attn, dtype=torch.long)\n",
    "\n",
    "        # 构建语言模型训练标签（左移一位）\n",
    "        labels = input_ids.clone()\n",
    "        labels[:-1] = input_ids[1:]\n",
    "        labels[-1] = self.EOS\n",
    "\n",
    "        # 找到 SEP 标记的位置，用于确定回答的开始位置\n",
    "        sep_positions = (input_ids == self.SEP).nonzero(as_tuple=True)[0]\n",
    "        sep_idx = int(sep_positions[0].item()) if len(sep_positions) > 0 else 0\n",
    "\n",
    "        # 屏蔽不需要计算损失的位置：\n",
    "        # 1. 指令段和 SEP 标记之前的部分设置为-100\n",
    "        labels[: sep_idx + 1] = -100               \n",
    "        # 2. PAD 填充部分设置为-100\n",
    "        labels[input_ids == self.PAD] = -100       \n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,           \n",
    "            \"attention_mask\": attention_mask, \n",
    "            \"labels\": labels,                 \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67af0ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([  1,  77, 114, 119, 120, 118, 121, 103, 120,  62,  91, 108, 101, 120,\n",
       "           36, 109, 119,  36, 113, 101, 103, 108, 109, 114, 105,  36, 112, 105,\n",
       "          101, 118, 114, 109, 114, 107,  67,   2,  81, 101, 103, 108, 109, 114,\n",
       "          105,  36, 112, 105, 101, 118, 114, 109, 114, 107,  36, 109, 119,  36,\n",
       "          120, 105, 103, 108, 114, 115, 112, 115, 107, 125,  36, 120, 108, 101,\n",
       "          120,  36, 105, 114, 101, 102, 112, 105, 119,  36, 103, 115, 113, 116,\n",
       "          121, 120, 105, 118, 119,  36, 120, 115,  36, 112, 105, 101, 118, 114,\n",
       "           36, 106, 118, 115, 113,  36, 104, 101, 120, 101,  50,   3,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "           101,  103,  108,  109,  114,  105,   36,  112,  105,  101,  118,  114,\n",
       "           109,  114,  107,   36,  109,  119,   36,  120,  105,  103,  108,  114,\n",
       "           115,  112,  115,  107,  125,   36,  120,  108,  101,  120,   36,  105,\n",
       "           114,  101,  102,  112,  105,  119,   36,  103,  115,  113,  116,  121,\n",
       "           120,  105,  118,  119,   36,  120,  115,   36,  112,  105,  101,  118,\n",
       "           114,   36,  106,  118,  115,  113,   36,  104,  101,  120,  101,   50,\n",
       "             3,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100])},\n",
       " torch.Size([128]),\n",
       " torch.Size([128]),\n",
       " torch.Size([128]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 测试编码的效果\n",
    "tok = SimpleTokenizer(max_len=128)\n",
    "pack = tok.encode_dialog(\"What is machine learning?\", \"Machine learning is technology that enables computers to learn from data.\")\n",
    "pack, pack[\"input_ids\"].shape, pack[\"attention_mask\"].shape, pack[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4768e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 260)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SFT Dataset & DataLoader\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer: SimpleTokenizer):\n",
    "        self.pairs = pairs\n",
    "        self.tok = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instr, resp = self.pairs[idx]\n",
    "        return self.tok.encode_dialog(instr, resp)\n",
    "\n",
    "def build_sft_dataloader(pairs, tokenizer: SimpleTokenizer, batch_size=4, shuffle=True):\n",
    "    ds = SFTDataset(pairs, tokenizer)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# def create_sft_dataset():\n",
    "#     return [\n",
    "#         (\"什么是 AI？\", \"AI 是能模拟人类智能的技术。\"),\n",
    "#         (\"1+1 等于几？\", \"1+1 等于 2。\"),\n",
    "#         (\"写一句问候语\", \"你好，很高兴见到你！\"),\n",
    "#         (\"什么是机器学习？\", \"机器学习是让计算机从数据中学习的技术。\"),\n",
    "#         (\"如何保护环境？\", \"减少浪费，节约能源，绿色出行。\"),\n",
    "#     ]\n",
    "def create_sft_dataset():\n",
    "    return [\n",
    "        (\"What is AI?\", \"AI is technology that can simulate human intelligence.\"),\n",
    "        (\"What is 1+1?\", \"1+1 equals 2.\"),\n",
    "        (\"Write a greeting\", \"Hello, nice to meet you!\"),\n",
    "        (\"What is machine learning?\", \"Machine learning is technology that enables computers to learn from data.\"),\n",
    "        (\"How to protect the environment?\", \"Reduce waste, save energy, and travel green.\")\n",
    "    ]\n",
    "\n",
    "# 初始化 SFT 数据\n",
    "tokenizer = SimpleTokenizer(max_len=128)\n",
    "pairs = create_sft_dataset()\n",
    "sft_loader = build_sft_dataloader(pairs, tokenizer, batch_size=2, shuffle=True)\n",
    "\n",
    "len(pairs), tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c73f3b",
   "metadata": {},
   "source": [
    "这里创建了一个简单的字节级分词器`SimpleTokenizer`，将文本编码为 UTF-8 字节并使用 [BOS]…[SEP]…[EOS] 模板；SFTDataset 仅在回答区间计算交叉熵，这是条件语言建模的标准做法。\n",
    "\n",
    "### 2.2 模型定义\n",
    "\n",
    "定义一个简化的 Transformer 模型，包含必要的位置编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87fad4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerLM(nn.Module):\n",
    "    \"\"\"Decoder-only（用 TransformerEncoder + 因果掩码模拟）语言模型。\"\"\"\n",
    "    def __init__(self, vocab_size, max_len=128, d_model=128, nhead=4, num_layers=4,\n",
    "                 dropout=0.1, tie_weights=True, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 指定 padding_idx，避免 PAD embedding 被更新\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
    "\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        if tie_weights:\n",
    "            self.lm_head.weight = self.embed.weight\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 因果掩码：上三角 True 表示被遮蔽（看不到未来）\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\",\n",
    "            torch.triu(torch.ones(max_len, max_len), diagonal=1).bool()\n",
    "        )\n",
    "\n",
    "        nn.init.normal_(self.embed.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed.weight, std=0.02)\n",
    "        if not tie_weights:\n",
    "            nn.init.normal_(self.lm_head.weight, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        B, T = input_ids.shape\n",
    "        if T > self.max_len:\n",
    "            raise ValueError(f\"seq_len {T} > max_len {self.max_len}\")\n",
    "\n",
    "        tok = self.embed(input_ids) * math.sqrt(self.d_model)\n",
    "        pos_ids = torch.arange(T, device=input_ids.device, dtype=torch.long)\\\n",
    "                       .unsqueeze(0).expand(B, T)\n",
    "        pos = self.pos_embed(pos_ids)\n",
    "        x = self.ln(self.dropout(tok + pos))\n",
    "\n",
    "        cmask = self.causal_mask[:T, :T]  # [T,T] True=mask\n",
    "        kpmask = (~attention_mask.bool()) if attention_mask is not None else None  # [B,T] True=pad\n",
    "\n",
    "        h = self.transformer(x, mask=cmask, src_key_padding_mask=kpmask)  # [B,T,H]\n",
    "        logits = self.lm_head(h)  # [B,T,V]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0aa9dc",
   "metadata": {},
   "source": [
    "简化 Transformer 模型包含嵌入层、位置编码和 Transformer 编码器。我们使用了正弦/余弦位置编码，位置编码让模型能够理解文本中的顺序信息，是 Transformer 架构的关键组件。\n",
    "\n",
    "### 2.3 SFT 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e602785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始 SFT 训练阶段…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 1/30: 100%|██████████| 3/3 [00:00<00:00, 56.04it/s, loss=5.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 1 平均损失: 5.2533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 2/30: 100%|██████████| 3/3 [00:00<00:00, 54.54it/s, loss=4.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 2 平均损失: 4.5989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 3/30: 100%|██████████| 3/3 [00:00<00:00, 55.91it/s, loss=4.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 3 平均损失: 4.2027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 4/30: 100%|██████████| 3/3 [00:00<00:00, 65.22it/s, loss=3.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 4 平均损失: 3.8092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 5/30: 100%|██████████| 3/3 [00:00<00:00, 65.22it/s, loss=3.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 5 平均损失: 3.5364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 6/30: 100%|██████████| 3/3 [00:00<00:00, 65.22it/s, loss=3.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 6 平均损失: 3.3203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 7/30: 100%|██████████| 3/3 [00:00<00:00, 64.49it/s, loss=3.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 7 平均损失: 3.2076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 8/30: 100%|██████████| 3/3 [00:00<00:00, 107.14it/s, loss=3.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 8 平均损失: 3.2397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 9/30: 100%|██████████| 3/3 [00:00<00:00, 107.14it/s, loss=2.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 9 平均损失: 3.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 10/30: 100%|██████████| 3/3 [00:00<00:00, 111.11it/s, loss=2.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 10 平均损失: 2.8621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 11/30: 100%|██████████| 3/3 [00:00<00:00, 100.01it/s, loss=2.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 11 平均损失: 2.7614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 12/30: 100%|██████████| 3/3 [00:00<00:00, 93.75it/s, loss=2.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 12 平均损失: 2.6557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 13/30: 100%|██████████| 3/3 [00:00<00:00, 93.75it/s, loss=2.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 13 平均损失: 2.6135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 14/30: 100%|██████████| 3/3 [00:00<00:00, 96.78it/s, loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 14 平均损失: 2.4779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 15/30: 100%|██████████| 3/3 [00:00<00:00, 96.77it/s, loss=2.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 15 平均损失: 2.4282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 16/30: 100%|██████████| 3/3 [00:00<00:00, 95.08it/s, loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 16 平均损失: 2.3240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 17/30: 100%|██████████| 3/3 [00:00<00:00, 86.75it/s, loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 17 平均损失: 2.2388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 18/30: 100%|██████████| 3/3 [00:00<00:00, 76.92it/s, loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 18 平均损失: 2.1665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 19/30: 100%|██████████| 3/3 [00:00<00:00, 107.14it/s, loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 19 平均损失: 2.0518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 20/30: 100%|██████████| 3/3 [00:00<00:00, 100.00it/s, loss=2.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 20 平均损失: 2.0638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 21/30: 100%|██████████| 3/3 [00:00<00:00, 103.44it/s, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 21 平均损失: 1.8580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 22/30: 100%|██████████| 3/3 [00:00<00:00, 100.00it/s, loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 22 平均损失: 1.8680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 23/30: 100%|██████████| 3/3 [00:00<00:00, 103.44it/s, loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 23 平均损失: 1.7941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 24/30: 100%|██████████| 3/3 [00:00<00:00, 100.00it/s, loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 24 平均损失: 1.7444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 25/30: 100%|██████████| 3/3 [00:00<00:00, 103.44it/s, loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 25 平均损失: 1.6805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 26/30: 100%|██████████| 3/3 [00:00<00:00, 93.75it/s, loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 26 平均损失: 1.6553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 27/30: 100%|██████████| 3/3 [00:00<00:00, 93.75it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 27 平均损失: 1.4357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 28/30: 100%|██████████| 3/3 [00:00<00:00, 85.71it/s, loss=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 28 平均损失: 1.6636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 29/30: 100%|██████████| 3/3 [00:00<00:00, 100.01it/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 29 平均损失: 1.4517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 30/30: 100%|██████████| 3/3 [00:00<00:00, 100.00it/s, loss=1.58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SFT] Epoch 30 平均损失: 1.5323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = SimpleTransformerLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len=128,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_layers=4,\n",
    "    dropout=0.1,\n",
    "    tie_weights=True,\n",
    "    pad_id=tokenizer.PAD,   \n",
    ").to(device)\n",
    "\n",
    "sft_criterion = nn.CrossEntropyLoss(ignore_index=-100)  # 只在回答段计算\n",
    "sft_optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_sft(model, dataloader, epochs=30, grad_clip=1.0):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for ep in range(epochs):\n",
    "        ep_loss = 0.0\n",
    "        pbar = tqdm(dataloader, desc=f\"[SFT] Epoch {ep+1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "            sft_optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attn_mask)  # [B,T,V]\n",
    "            loss = sft_criterion(\n",
    "                logits.view(-1, model.vocab_size),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            sft_optimizer.step()\n",
    "\n",
    "            ep_loss += float(loss.item())\n",
    "            pbar.set_postfix(loss=float(loss.item()))\n",
    "        avg = ep_loss / max(1, len(dataloader))\n",
    "        losses.append(avg)\n",
    "        print(f\"[SFT] Epoch {ep+1} 平均损失: {avg:.4f}\")\n",
    "    return losses\n",
    "\n",
    "print(\"开始 SFT 训练阶段…\")\n",
    "sft_losses = train_sft(model, sft_loader, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93b0b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成函数（贪心/采样简化版）\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt: str, max_new_tokens=32):\n",
    "    pack = tokenizer.encode_dialog(prompt, \"\")\n",
    "    input_ids = pack[\"input_ids\"].unsqueeze(0).to(device)\n",
    "    attn_mask = pack[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "    # 去掉右侧 PAD\n",
    "    true_len = int(attn_mask.sum(dim=1).item())\n",
    "    input_ids = input_ids[:, :true_len]\n",
    "    attn_mask = attn_mask[:, :true_len]\n",
    "\n",
    "    for i in range(max_new_tokens):\n",
    "        logits = model(input_ids, attention_mask=attn_mask)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # # 将 logits 转换到 cpu 并转为 numpy 数组\n",
    "        # last_logits = logits[0, -1, :].cpu().numpy()\n",
    "        # plt.figure(figsize=(12, 4))\n",
    "        # plt.plot(last_logits)\n",
    "        # plt.title(f\"Logits Distribution for Token {i+1}\")\n",
    "        # plt.xlabel(\"Token ID\")\n",
    "        # plt.ylabel(\"Logit Value\")\n",
    "        # # 标记出最大值的位置\n",
    "        # max_logit_idx = last_logits.argmax()\n",
    "        # max_logit_val = last_logits[max_logit_idx]\n",
    "        # plt.scatter(max_logit_idx, max_logit_val, color='red', zorder=5, label=f'Max Logit at ID {max_logit_idx}')\n",
    "        # plt.legend()\n",
    "        # plt.grid(True)\n",
    "        # plt.show()\n",
    "        # # --- 可视化结束 ---\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        attn_mask = torch.cat([attn_mask, torch.ones_like(next_token)], dim=1)\n",
    "        if next_token.item() == tokenizer.EOS:\n",
    "            break\n",
    "\n",
    "    # decode：去掉 BOS/SEP/EOS，只取回答区间\n",
    "    ids = input_ids[0].tolist()\n",
    "    if tokenizer.SEP in ids:\n",
    "        sep_idx = ids.index(tokenizer.SEP)\n",
    "        ans_ids = ids[sep_idx+1:]\n",
    "    else:\n",
    "        ans_ids = ids\n",
    "    # 去掉 PAD 和 EOS\n",
    "    ans_ids = [i for i in ans_ids if i not in (tokenizer.PAD, tokenizer.EOS)]\n",
    "    # 减 4 还原字节\n",
    "    text = bytes([i-4 for i in ans_ids]).decode(\"utf-8\", errors=\"ignore\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fbec767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SFT 模型回答 ===\n",
      "Q: What is AI?\n",
      "A: lo tellllololinin te t tolat t \n",
      "\n",
      "Q: What is 1+1?\n",
      "A: als e e e e e e e e e te e e \n",
      "\n",
      "Q: Write a greeting\n",
      "A: lllo te te t te t togy te te \n",
      "\n",
      "Q: What is machine learning?\n",
      "A: le e e  e\n",
      "\n",
      "Q: Write a greeting\n",
      "A: lllo toll to tett toutt t t tte\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据可视化发现，模型会先输出 0（也就是 PAD），然后输出其他\n",
    "# 模型效果\n",
    "test_prompts = [\n",
    "    \"What is AI?\",\n",
    "    \"What is 1+1?\",\n",
    "    \"Write a greeting\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Write a greeting\"\n",
    "]\n",
    "\n",
    "print(\"=== SFT 模型回答 ===\")\n",
    "for p in test_prompts:\n",
    "    ans = generate_text(model, tokenizer, p)\n",
    "    print(f\"Q: {p}\\nA: {ans}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64cb2d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65, 110, 115, 119, 101, 114, 58]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 \"Answer:\" 对应的编码\n",
    "text = \"Answer:\"\n",
    "list(text.encode(\"utf-8\", errors=\"replace\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bc218",
   "metadata": {},
   "source": [
    "训练过程中，我们使用交叉熵损失，让模型学会在给定指令条件下预测下一个 token。\n",
    "在数据构造时，非回答区间与 PAD 的标签被设为 -100，配合 ignore_index=-100 只在回答区间计算损失；同时 attention_mask 让注意力忽略 PAD 位。随着训练进行，损失应逐步下降，表明模型正在学习指令→回答的条件生成关系。\n",
    "\n",
    "## 3. Stage2：奖励模型 RM\n",
    "\n",
    "奖励模型用于学习人类对模型回答的偏好，为强化学习阶段提供奖励信号。\n",
    "\n",
    "奖励模型的目标是学习人类的偏好判断，即对于同一个指令的多个回答，哪个回答更符合人类偏好。训练数据由人类标注员对模型生成回答的质量进行排名。\n",
    "\n",
    "奖励模型的训练通常使用对比学习框架，通过比较不同回答的相对质量来学习一个标量奖励函数。常用的损失函数是 pairwise ranking loss：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{RM} = -\\mathbb{E}_{(x,y_w,y_l)\\sim D}[\\log(\\sigma(r_{\\phi}(x,y_w) - r_{\\phi}(x,y_l)))]\n",
    "$$\n",
    "\n",
    "其中 $y_w$ 是偏好的回答，$y_l$ 是不偏好的回答，$r_{\\phi}$ 是奖励模型。\n",
    "\n",
    "### 3.1 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34496af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2.1 — RM 数据集（pairwise）\n",
    "class RMDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer: SimpleTokenizer):\n",
    "        self.data = data\n",
    "        self.tok = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        good_pack = self.tok.encode_dialog(item[\"instr\"], item[\"good\"])\n",
    "        bad_pack  = self.tok.encode_dialog(item[\"instr\"], item[\"bad\"])\n",
    "        return {\n",
    "            \"good_input_ids\": good_pack[\"input_ids\"],\n",
    "            \"good_attention_mask\": good_pack[\"attention_mask\"],\n",
    "            \"bad_input_ids\": bad_pack[\"input_ids\"],\n",
    "            \"bad_attention_mask\": bad_pack[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "def create_rm_dataset():\n",
    "    # return [\n",
    "    #     {\"instr\": \"什么是 AI？\",\n",
    "    #      \"good\": \"AI 是能模拟人类智能的技术，比如语音识别和图像识别。\",\n",
    "    #      \"bad\":  \"AI 就是机器人。\"},\n",
    "    #     {\"instr\": \"1+1 等于几？\",\n",
    "    #      \"good\": \"1+1 的结果是 2。\",\n",
    "    #      \"bad\":  \"1+1 可能等于 3，看情况。\"},\n",
    "    #     {\"instr\": \"写一句问候语\",\n",
    "    #      \"good\": \"你好！很高兴有机会与你交流。\",\n",
    "    #      \"bad\":  \"喂，你好。\"},\n",
    "    #     {\"instr\": \"什么是机器学习？\",\n",
    "    #      \"good\": \"机器学习是人工智能的一个分支，让计算机能从数据中学习并改进。\",\n",
    "    #      \"bad\":  \"机器学习就是教机器读书。\"},\n",
    "    # ]\n",
    "    return [\n",
    "        {\"instr\": \"What is AI?\",\n",
    "         \"good\": \"AI is technology that can simulate human intelligence, such as speech recognition and image recognition.\",\n",
    "         \"bad\":  \"AI is just robots.\"},\n",
    "        {\"instr\": \"What is 1+1?\",\n",
    "         \"good\": \"The result of 1+1 is 2.\",\n",
    "         \"bad\":  \"1+1 might equal 3, depending on the situation.\"},\n",
    "        {\"instr\": \"Write a greeting\",\n",
    "         \"good\": \"Hello! I'm glad to have the opportunity to communicate with you.\",\n",
    "         \"bad\":  \"Hey, hello.\"},\n",
    "        {\"instr\": \"What is machine learning?\",\n",
    "         \"good\": \"Machine learning is a branch of artificial intelligence that enables computers to learn and improve from data.\",\n",
    "         \"bad\":  \"Machine learning is teaching machines to read.\"},\n",
    "    ]\n",
    "\n",
    "rm_data = create_rm_dataset()\n",
    "rm_loader = DataLoader(RMDataset(rm_data, tokenizer), batch_size=4, shuffle=True)\n",
    "len(rm_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc33b40",
   "metadata": {},
   "source": [
    "奖励模型需要成对的偏好数据，每个数据点包含一个指令、一个高质量回答和一个低质量回答。这种数据结构允许我们训练模型区分回答的质量差异。\n",
    "\n",
    "### 3.2 奖励模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "443ceb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 奖励模型（与 SFT 同构 backbone，只在回答区间池化）\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, backbone_cfg, sep_token_id: int):\n",
    "        super().__init__()\n",
    "        self.sep_id = sep_token_id\n",
    "        self.backbone = SimpleTransformerLM(**backbone_cfg)  # 复用结构，不用 lm_head\n",
    "        self.reward_head = nn.Linear(self.backbone.d_model, 1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def load_backbone_from_sft(self, sft_state_dict):\n",
    "        # 只加载非 lm_head 参数\n",
    "        state = {k: v for k, v in sft_state_dict.items() if not k.startswith(\"lm_head.\")}\n",
    "        missing, unexpected = self.backbone.load_state_dict(state, strict=False)\n",
    "        return missing, unexpected\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        B, T = input_ids.shape\n",
    "\n",
    "        # 取得 backbone 隐状态（与 LM 的前向一致，但这里我们只要隐藏层，不需要 logits）\n",
    "        tok = self.backbone.embed(input_ids) * math.sqrt(self.backbone.d_model)\n",
    "        pos_ids = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, T)\n",
    "        pos = self.backbone.pos_embed(pos_ids)\n",
    "        x = self.backbone.ln(self.backbone.dropout(tok + pos))\n",
    "        h = self.backbone.transformer(\n",
    "            x,\n",
    "            mask=self.backbone.causal_mask[:T,:T],\n",
    "            src_key_padding_mask=(~attention_mask.bool())\n",
    "        )  # [B,T,H]\n",
    "\n",
    "        # 仅在回答区间池化：pos > SEP 且有效\n",
    "        is_sep = (input_ids == self.sep_id)\n",
    "        sep_idx = torch.where(is_sep.any(dim=1),\n",
    "                              is_sep.float().argmax(dim=1),\n",
    "                              torch.zeros(B, device=input_ids.device, dtype=torch.long))\n",
    "        pos_ids_b = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, T)\n",
    "        resp_mask = (pos_ids_b > sep_idx.unsqueeze(1)) & (attention_mask.bool())  # [B,T]\n",
    "\n",
    "        # 若极端截断导致全 0，兜底给最后一个有效位\n",
    "        fallback = (resp_mask.sum(dim=1) == 0)\n",
    "        if fallback.any():\n",
    "            last_valid = attention_mask.size(1) - 1 - torch.flip(attention_mask, dims=[1]).long().argmax(dim=1)\n",
    "            resp_mask[fallback, last_valid[fallback]] = True\n",
    "\n",
    "        m = resp_mask.float().unsqueeze(-1)          # [B,T,1]\n",
    "        pooled = (h * m).sum(dim=1) / m.sum(dim=1).clamp(min=1.0)  # [B,H]\n",
    "\n",
    "        reward = self.reward_head(pooled).squeeze(-1)            # [B]\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304e435",
   "metadata": {},
   "source": [
    "奖励模型基于第一阶段训练的 SFT 模型构建，这样可以利用已经学到的指令理解能力。奖励模型在 SFT backbone 的隐藏状态上，对 回答区间（SEP 之后、非 PAD）做平均池化 得到句级表示，再通过一层线性层 reward_head 映射为 标量奖励。\n",
    "\n",
    "### 3.3 RM 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a719a038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] load_from_sft missing: ['lm_head.weight'] | unexpected: []\n",
      "开始 奖励模型(RM) 训练阶段…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 1/50: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s, acc=0.25, loss=0.709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 1 平均损失: 0.7093 | Pairwise Acc: 0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 2/50: 100%|██████████| 1/1 [00:00<00:00, 18.87it/s, acc=0.75, loss=0.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 2 平均损失: 0.6604 | Pairwise Acc: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 21.28it/s, acc=1, loss=0.617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 3 平均损失: 0.6173 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 21.74it/s, acc=0.5, loss=0.606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 4 平均损失: 0.6056 | Pairwise Acc: 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 22.22it/s, acc=1, loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 5 平均损失: 0.5518 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 6/50: 100%|██████████| 1/1 [00:00<00:00, 22.22it/s, acc=1, loss=0.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 6 平均损失: 0.5303 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 7/50: 100%|██████████| 1/1 [00:00<00:00, 22.22it/s, acc=1, loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 7 平均损失: 0.4945 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 8/50: 100%|██████████| 1/1 [00:00<00:00, 38.46it/s, acc=1, loss=0.474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 8 平均损失: 0.4742 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 58.82it/s, acc=1, loss=0.461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 9 平均损失: 0.4610 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s, acc=1, loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 10 平均损失: 0.4462 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 55.55it/s, acc=1, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 11 平均损失: 0.4022 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 58.83it/s, acc=1, loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 12 平均损失: 0.3852 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s, acc=1, loss=0.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 13 平均损失: 0.3696 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 14/50: 100%|██████████| 1/1 [00:00<00:00, 40.71it/s, acc=1, loss=0.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 14 平均损失: 0.3497 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 15/50: 100%|██████████| 1/1 [00:00<00:00, 47.62it/s, acc=1, loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 15 平均损失: 0.3247 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 16/50: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s, acc=1, loss=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 16 平均损失: 0.3210 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s, acc=1, loss=0.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 17 平均损失: 0.2403 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 58.83it/s, acc=1, loss=0.273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 18 平均损失: 0.2728 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s, acc=1, loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 19 平均损失: 0.2209 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 20/50: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s, acc=1, loss=0.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 20 平均损失: 0.2205 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s, acc=1, loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 21 平均损失: 0.2230 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 22/50: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s, acc=1, loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 22 平均损失: 0.1610 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 23/50: 100%|██████████| 1/1 [00:00<00:00, 41.67it/s, acc=1, loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 23 平均损失: 0.1745 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 24/50: 100%|██████████| 1/1 [00:00<00:00, 58.83it/s, acc=1, loss=0.134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 24 平均损失: 0.1342 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s, acc=1, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 25 平均损失: 0.1654 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 52.64it/s, acc=1, loss=0.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 26 平均损失: 0.1215 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s, acc=1, loss=0.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 27 平均损失: 0.1404 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 28/50: 100%|██████████| 1/1 [00:00<00:00, 51.59it/s, acc=1, loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 28 平均损失: 0.1112 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 29/50: 100%|██████████| 1/1 [00:00<00:00, 50.00it/s, acc=1, loss=0.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 29 平均损失: 0.0940 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 30/50: 100%|██████████| 1/1 [00:00<00:00, 50.00it/s, acc=1, loss=0.0884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 30 平均损失: 0.0884 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 31/50: 100%|██████████| 1/1 [00:00<00:00, 43.48it/s, acc=1, loss=0.0831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 31 平均损失: 0.0831 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 32/50: 100%|██████████| 1/1 [00:00<00:00, 47.62it/s, acc=1, loss=0.0541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 32 平均损失: 0.0541 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 33/50: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s, acc=1, loss=0.0608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 33 平均损失: 0.0608 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 34/50: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s, acc=1, loss=0.0504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 34 平均损失: 0.0504 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 35/50: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s, acc=1, loss=0.0469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 35 平均损失: 0.0469 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 36/50: 100%|██████████| 1/1 [00:00<00:00, 55.55it/s, acc=1, loss=0.057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 36 平均损失: 0.0570 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 37/50: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s, acc=1, loss=0.0272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 37 平均损失: 0.0272 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 38/50: 100%|██████████| 1/1 [00:00<00:00, 55.54it/s, acc=1, loss=0.0279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 38 平均损失: 0.0279 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 39/50: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s, acc=1, loss=0.0199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 39 平均损失: 0.0199 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 40/50: 100%|██████████| 1/1 [00:00<00:00, 58.83it/s, acc=1, loss=0.0187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 40 平均损失: 0.0187 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 41/50: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s, acc=1, loss=0.0152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 41 平均损失: 0.0152 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 42/50: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s, acc=1, loss=0.0126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 42 平均损失: 0.0126 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 43/50: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s, acc=1, loss=0.0101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 43 平均损失: 0.0101 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 44/50: 100%|██████████| 1/1 [00:00<00:00, 47.62it/s, acc=1, loss=0.0108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 44 平均损失: 0.0108 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 45/50: 100%|██████████| 1/1 [00:00<00:00, 50.00it/s, acc=1, loss=0.00916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 45 平均损失: 0.0092 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 46/50: 100%|██████████| 1/1 [00:00<00:00, 43.48it/s, acc=1, loss=0.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 46 平均损失: 0.0110 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 47/50: 100%|██████████| 1/1 [00:00<00:00, 58.83it/s, acc=1, loss=0.00793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 47 平均损失: 0.0079 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 48/50: 100%|██████████| 1/1 [00:00<00:00, 52.64it/s, acc=1, loss=0.00802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 48 平均损失: 0.0080 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 49/50: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s, acc=1, loss=0.00702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 49 平均损失: 0.0070 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 50/50: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s, acc=1, loss=0.00864]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] Epoch 50 平均损失: 0.0086 | Pairwise Acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# RM 训练（pairwise）\n",
    "def rm_pairwise_loss(r_good: torch.Tensor, r_bad: torch.Tensor) -> torch.Tensor:\n",
    "    delta = r_good - r_bad  # [B]\n",
    "    # softplus(-Δ) = -log σ(Δ)，更稳\n",
    "    return F.softplus(-delta).mean()\n",
    "\n",
    "def train_reward_model(rm_model, rm_loader, epochs=3, lr=1e-4):\n",
    "    rm_model.train()\n",
    "    opt = optim.Adam(rm_model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        ep_loss, correct, total = 0.0, 0, 0\n",
    "        pbar = tqdm(rm_loader, desc=f\"[RM] Epoch {ep+1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            good_ids = batch[\"good_input_ids\"].to(device)\n",
    "            good_msk = batch[\"good_attention_mask\"].to(device)\n",
    "            bad_ids  = batch[\"bad_input_ids\"].to(device)\n",
    "            bad_msk  = batch[\"bad_attention_mask\"].to(device)\n",
    "            \n",
    "            # 前向\n",
    "            r_good = rm_model(good_ids, good_msk)  # [B]\n",
    "            r_bad  = rm_model(bad_ids,  bad_msk)   # [B]\n",
    "\n",
    "            # 损失\n",
    "            loss = rm_pairwise_loss(r_good, r_bad)\n",
    "\n",
    "            # 反向与更新\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(rm_model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            # 统计\n",
    "            with torch.no_grad():\n",
    "                correct += int((r_good > r_bad).sum().item())\n",
    "                total   += good_ids.size(0)\n",
    "\n",
    "            ep_loss += float(loss.item())\n",
    "            pbar.set_postfix(loss=float(loss.item()), acc=correct/max(1,total))\n",
    "\n",
    "        print(f\"[RM] Epoch {ep+1} 平均损失: {ep_loss/len(rm_loader):.4f} | \"\n",
    "              f\"Pairwise Acc: {correct/max(1,total):.3f}\")\n",
    "\n",
    "# 构建 RM 并从 SFT 初始化\n",
    "backbone_cfg = dict(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len=128,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_layers=4,\n",
    "    dropout=0.1,\n",
    "    tie_weights=False,  # RM 不需要输出头共享\n",
    "    pad_id=tokenizer.PAD\n",
    ")\n",
    "reward_model = RewardModel(backbone_cfg, sep_token_id=tokenizer.SEP).to(device)\n",
    "missing, unexpected = reward_model.load_backbone_from_sft(model.state_dict())\n",
    "print(\"[RM] load_from_sft missing:\", missing, \"| unexpected:\", unexpected)\n",
    "\n",
    "print(\"开始 奖励模型(RM) 训练阶段…\")\n",
    "train_reward_model(reward_model, rm_loader, epochs=50, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5c066",
   "metadata": {},
   "source": [
    "奖励模型的训练使用对比损失函数，目标是让模型对高质量回答给出更高的奖励值，对低质量回答给出更低的奖励值。这种成对比较的方式能够有效学习人类的偏好。随着训练进行，损失应该下降，表明模型逐渐学会了区分回答质量。\n",
    "\n",
    "## 4. Stage3：PPO 强化学习\n",
    "\n",
    "在 PPO 阶段，我们使用奖励模型提供的奖励信号来进一步优化语言模型，使其生成更符合人类偏好的回答。\n",
    "\n",
    "### 4.1 技术原理\n",
    "\n",
    "PPO（Proximal Policy Optimization）是一种强化学习算法，它通过限制策略更新的幅度来保证训练稳定性。  \n",
    "在 RLHF 中，PPO 的目标是：**让语言模型生成的回答获得更高的奖励模型评分，同时避免策略更新过大而导致崩溃**。\n",
    "\n",
    "核心损失函数为：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{PPO} = \\mathbb{E}\\Big[\\min\\big(r_t(\\theta)\\hat{A}_t,\\ \\text{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t\\big)\\Big]\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\text{old}}(a_t|s_t)}$ 为策略比值  \n",
    "- $\\hat{A}_t$ 是优势估计（本实现使用 GAE 计算，而不是直接用 RM 打分）  \n",
    "- $\\epsilon$ 是裁剪系数\n",
    "\n",
    "此外，我们还引入 **KL penalty**，限制当前策略与参考策略（SFT 模型）的偏移，进一步增强训练稳定性。\n",
    "\n",
    "为了简化实现，这里将奖励 **只分配到生成的最后一步**，前面的 token 奖励为 0。这保证了实现简洁，同时仍能驱动 PPO 学习。\n",
    "\n",
    "### 4.2 PPO 实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68cc3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= PPO =========\n",
    "import math, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ---- Policy with value head ----\n",
    "class PolicyWithValue(SimpleTransformerLM):\n",
    "    \"\"\"\n",
    "    在 SimpleTransformerLM 上加一个 value_head，用于估计 V(s_t)。\n",
    "    - forward() 仍旧返回 logits（用于 LM）\n",
    "    - forward_with_value() 同时返回 logits 和逐时刻的 V 值\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)       # 继承你的 LM（含因果掩码等）\n",
    "        self.value_head = nn.Linear(self.d_model, 1)\n",
    "        # 轻微初始化更稳\n",
    "        nn.init.zeros_(self.value_head.bias)\n",
    "        nn.init.normal_(self.value_head.weight, std=1e-2)\n",
    "\n",
    "    def forward_with_value(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        返回:\n",
    "          - logits: [B,T,V]\n",
    "          - values: [B,T]\n",
    "        \"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        # 1) LM logits\n",
    "        logits = super().forward(input_ids, attention_mask=attention_mask)  # [B,T,V]\n",
    "\n",
    "        # 2) 复用 backbone 计算隐藏状态，再经 value_head\n",
    "        tok = self.embed(input_ids) * math.sqrt(self.d_model)\n",
    "        pos_ids = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, T)\n",
    "        pos = self.pos_embed(pos_ids)\n",
    "        x = self.ln(self.dropout(tok + pos))\n",
    "        h = self.transformer(\n",
    "            x,\n",
    "            mask=self.causal_mask[:T, :T],\n",
    "            src_key_padding_mask=(~attention_mask.bool()) if attention_mask is not None else None,\n",
    "        )  # [B,T,H]\n",
    "        values = self.value_head(h).squeeze(-1)  # [B,T]\n",
    "        return logits, values\n",
    "    \n",
    "policy = PolicyWithValue(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len=128,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_layers=4,\n",
    ").to(device)\n",
    "\n",
    "# 构建 reference policy：从已经训练好的 SFT 模型复制\n",
    "ref_policy = deepcopy(policy).eval()\n",
    "for p in ref_policy.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12126a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 采样函数 ----\n",
    "@torch.no_grad()\n",
    "def sample_response(policy, prompt_ids, prompt_mask, max_new_tokens=64, temperature=1.0, top_p=0.9):\n",
    "    \"\"\"\n",
    "    返回:\n",
    "      full_ids: [B, T_total]\n",
    "      full_mask: [B, T_total]\n",
    "      gen_mask: [B, T_total]  (生成区为 True)\n",
    "      logp_old: [B, L]        (逐步旧策略 log prob)\n",
    "      values:   [B, L]        (逐步旧策略 V(s_t))\n",
    "    约束:\n",
    "      - 先按 mask 裁右侧 PAD\n",
    "      - 不超过 policy.max_len\n",
    "    \"\"\"\n",
    "    policy.eval()\n",
    "    B = prompt_ids.size(0)\n",
    "    assert B == 1, \"此最小实现按 batch=1 编写；批量扩展很简单，但此处保持清晰。\"\n",
    "\n",
    "    # 1) 去右侧 PAD\n",
    "    true_len = int(prompt_mask.sum(dim=1).item())\n",
    "    input_ids = prompt_ids[:, :true_len].clone()\n",
    "    attn_mask = prompt_mask[:, :true_len].clone()\n",
    "\n",
    "    # 2) 允许生成的步数\n",
    "    room = int(policy.max_len - input_ids.size(1))\n",
    "    if room <= 0:\n",
    "        gen_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "        empty = torch.zeros(B, 0, device=input_ids.device)\n",
    "        return input_ids, attn_mask, gen_mask, empty, empty\n",
    "    steps = min(max_new_tokens, room)\n",
    "\n",
    "    taken_logp = []\n",
    "    values_seq = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        logits, values = policy.forward_with_value(input_ids, attention_mask=attn_mask)\n",
    "        last_logits = logits[:, -1, :] / max(1e-8, temperature)\n",
    "        probs = F.softmax(last_logits, dim=-1)\n",
    "\n",
    "        # nucleus (top-p) 采样\n",
    "        sorted_p, sorted_idx = torch.sort(probs, descending=True, dim=-1)\n",
    "        cumsum = torch.cumsum(sorted_p, dim=-1)\n",
    "        cutoff = (cumsum > top_p).float().argmax(dim=-1)\n",
    "        mask = torch.arange(probs.size(-1), device=probs.device).unsqueeze(0) > cutoff.unsqueeze(1)\n",
    "        filtered = sorted_p.masked_fill(mask, 0.0)\n",
    "        filtered = filtered / (filtered.sum(dim=-1, keepdim=True) + 1e-12)\n",
    "        next_sorted_idx = torch.multinomial(filtered, 1)         # [B,1]\n",
    "        next_token = sorted_idx.gather(1, next_sorted_idx)       # [B,1]\n",
    "\n",
    "        # 记录旧策略 logp 与 value\n",
    "        next_logp = torch.log(probs.gather(1, next_token))       # [B,1]\n",
    "        taken_logp.append(next_logp)\n",
    "        values_seq.append(values[:, -1])                         # [B]\n",
    "\n",
    "        # 拼接\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        attn_mask = torch.cat([attn_mask, torch.ones_like(next_token)], dim=1)\n",
    "\n",
    "        # 提前终止（遇到 EOS）\n",
    "        if (next_token.squeeze(1) == tokenizer.EOS).all():\n",
    "            break\n",
    "\n",
    "        if input_ids.size(1) >= policy.max_len:\n",
    "            break\n",
    "\n",
    "    logp_old = torch.cat(taken_logp, dim=1) if taken_logp else torch.zeros(B,0, device=input_ids.device)\n",
    "    values_seq = torch.stack(values_seq, dim=1) if values_seq else torch.zeros(B,0, device=input_ids.device)\n",
    "\n",
    "    gen_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "    gen_mask[:, true_len:] = True\n",
    "    return input_ids, attn_mask, gen_mask, logp_old, values_seq\n",
    "\n",
    "# ---- 在“生成区”一次性取 log-prob（用整序前向 + 位移）----\n",
    "@torch.no_grad()\n",
    "def selected_logprobs(model, full_ids, full_mask, gen_mask):\n",
    "    \"\"\"\n",
    "    计算“生成区”每一步 token 的 log-prob（在给定完整序列上前向一次）。\n",
    "    返回 [B, L]，与 sample_response 得到的 old_values 对齐。\n",
    "    \"\"\"\n",
    "    logits = model(full_ids, attention_mask=full_mask)           # [B,T,V]\n",
    "    logprobs = F.log_softmax(logits, dim=-1)                     # [B,T,V]\n",
    "\n",
    "    # 概率用于预测位置 t 的 token = full_ids[:, t]，它由 logits[:, t-1] 给出\n",
    "    logp_shift = logprobs[:, :-1, :]                             # [B,T-1,V]\n",
    "    tgt_shift  = full_ids[:, 1:]                                 # [B,T-1]\n",
    "    mask_shift = gen_mask[:, 1:]                                 # [B,T-1] —— 生成 token 的位置\n",
    "\n",
    "    sel = logp_shift.gather(2, tgt_shift.unsqueeze(-1)).squeeze(-1)  # [B,T-1]\n",
    "    # 选择生成区的条目并还原成 [B,L]\n",
    "    out = torch.masked_select(sel, mask_shift)\n",
    "    L = mask_shift.sum(dim=1)                                    # [B]\n",
    "    return out.view(full_ids.size(0), int(L.item()))             # B=1 → [1,L]\n",
    "\n",
    "\n",
    "# ---- GAE（用 old_values）----\n",
    "def compute_gae(rewards, values, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    rewards: [B,L]  这里只在末步非 0（toy）\n",
    "    values:  [B,L]  old_values（来自采样时的 policy）\n",
    "    返回:\n",
    "      adv:     [B,L]\n",
    "      returns: [B,L]  = adv + values\n",
    "    \"\"\"\n",
    "    B, L = rewards.shape\n",
    "    adv = torch.zeros_like(rewards)\n",
    "    lastgaelam = torch.zeros(B, device=rewards.device)\n",
    "\n",
    "    # 末步的 V_{t+1} 取 0（终止）\n",
    "    for t in reversed(range(L)):\n",
    "        v   = values[:, t]\n",
    "        v_n = torch.zeros_like(v) if t == L-1 else values[:, t+1]\n",
    "        r   = rewards[:, t]\n",
    "        delta = r + gamma * v_n - v\n",
    "        lastgaelam = delta + gamma * lam * lastgaelam\n",
    "        adv[:, t] = lastgaelam\n",
    "\n",
    "    returns = adv + values\n",
    "    # 按样本行标准化优势（更稳）\n",
    "    adv = (adv - adv.mean(dim=1, keepdim=True)) / (adv.std(dim=1, keepdim=True) + 1e-8)\n",
    "    return adv, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b2e0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PPO 训练主循环 ===\n",
    "def ppo_train(\n",
    "    policy, reward_model, ref_policy, tokenizer, prompts,\n",
    "    epochs=2, rollouts_per_epoch=8, max_new_tokens=64,\n",
    "    clip_low=0.8, clip_high=1.2,        # 策略 ratio 裁剪\n",
    "    ppo_epochs=4, vf_clip=0.2,          # value 裁剪\n",
    "    gamma=0.99, lam=0.95,               # GAE 超参\n",
    "    lr=5e-5, ent_coef=0.0, vf_coef=0.5, kl_coef=0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    policy:      当前可训练策略（Actor + Critic）\n",
    "    reward_model:冻结的 RM，打分奖励\n",
    "    ref_policy:  冻结的参考模型，用于 KL penalty\n",
    "    tokenizer:   数据编码器\n",
    "    prompts:     prompt 池\n",
    "    \"\"\"\n",
    "\n",
    "    # 冻结 RM 和 Ref 参数\n",
    "    for p in reward_model.parameters(): p.requires_grad = False\n",
    "    for p in ref_policy.parameters():   p.requires_grad = False\n",
    "\n",
    "    opt = optim.Adam(policy.parameters(), lr=lr)\n",
    "    all_logs = []\n",
    "\n",
    "    # ========= Epoch 外层循环 =========\n",
    "    for ep in range(epochs):\n",
    "        logs = []\n",
    "        pbar = tqdm(range(rollouts_per_epoch), desc=f\"[PPO] Epoch {ep+1}/{epochs}\")\n",
    "\n",
    "        # =============================\n",
    "        # 1) Rollout：采样一条轨迹 (Actor old)\n",
    "        # =============================\n",
    "        for _ in pbar:\n",
    "            instr = prompts[torch.randint(0, len(prompts), (1,)).item()]\n",
    "            pack = tokenizer.encode_dialog(instr, \"\")\n",
    "            prompt_ids = pack[\"input_ids\"].unsqueeze(0).to(device)\n",
    "            prompt_msk = pack[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "            # 1.1 用当前 policy 生成回答（old 策略）\n",
    "            with torch.no_grad():\n",
    "                full_ids, full_msk, gen_mask, logp_old, v_old = sample_response(\n",
    "                    policy, prompt_ids, prompt_msk,\n",
    "                    max_new_tokens=max_new_tokens, temperature=1.0, top_p=0.9\n",
    "                )\n",
    "\n",
    "            # =============================\n",
    "            # 2) 计算奖励 R & KL(old, ref)\n",
    "            # =============================\n",
    "            with torch.no_grad():\n",
    "                # 奖励模型给分 (简化：只在末步给一个标量 R)\n",
    "                R = reward_model(full_ids, full_msk)   # [B]\n",
    "                rewards = torch.zeros_like(logp_old)\n",
    "                rewards[:, -1] = R\n",
    "\n",
    "                # KL penalty: KL(π_old || π_ref)\n",
    "                logp_ref = selected_logprobs(ref_policy, full_ids, full_msk, gen_mask)\n",
    "                approx_kl_ref = (logp_old - logp_ref).mean().clamp_min(0)\n",
    "\n",
    "                # 用 GAE 计算优势\n",
    "                adv, ret = compute_gae(rewards, v_old, gamma=gamma, lam=lam)\n",
    "                adv = adv.detach(); ret = ret.detach()\n",
    "\n",
    "            # 生成序列的长度，用于还原 mask\n",
    "            lengths = gen_mask.sum(dim=1)\n",
    "\n",
    "            # =============================\n",
    "            # 3) PPO 更新 (在同一批数据上重复 ppo_epochs 次)\n",
    "            # =============================\n",
    "            for _upd in range(ppo_epochs):\n",
    "                # (a) 用新策略重新前向 → logp_new, v_new\n",
    "                # print(gen_mask)\n",
    "                logp_new = selected_logprobs(policy, full_ids, full_msk, gen_mask)\n",
    "                # print(logp_new)\n",
    "                ratio    = (logp_new - logp_old).exp()      # π_new / π_old\n",
    "                # print(ratio.shape)\n",
    "                # print(adv.shape)\n",
    "                ratio_c  = ratio.clamp(clip_low, clip_high)\n",
    "\n",
    "                # ---- Actor loss (剪裁版公式) ----\n",
    "                # L_actor = -E[min(ratio*A, ratio_clipped*A)]\n",
    "                L_actor  = -(torch.min(ratio * adv, ratio_c * adv)).mean()\n",
    "\n",
    "                # (b) Critic value clipping\n",
    "                _, values_full = policy.forward_with_value(full_ids, full_msk)\n",
    "                v_new = torch.masked_select(values_full, gen_mask).view_as(v_old)\n",
    "                v_clip = torch.clamp(v_new, v_old - vf_clip, v_old + vf_clip)\n",
    "                L_v_un = (v_new - ret).pow(2)\n",
    "                L_v_cl = (v_clip - ret).pow(2)\n",
    "                L_value = 0.5 * torch.max(L_v_un, L_v_cl).mean()\n",
    "\n",
    "                # (c) 熵 bonus (可选，鼓励探索)\n",
    "                entropy = -(logp_new.exp() * logp_new).mean()\n",
    "\n",
    "                # (d) KL penalty (限制与参考模型漂移)\n",
    "                L_kl = kl_coef * approx_kl_ref\n",
    "\n",
    "                # ---- 总损失 ----\n",
    "                loss = L_actor + vf_coef * L_value - ent_coef * entropy + L_kl\n",
    "\n",
    "                # 反向传播\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
    "                opt.step()\n",
    "\n",
    "            # =============================\n",
    "            # 4) 日志记录\n",
    "            # =============================\n",
    "            logs.append({\n",
    "                \"reward\": float(R.mean()),\n",
    "                \"kl_ref\": float(approx_kl_ref.item()),\n",
    "                \"loss\": float(loss.item()),\n",
    "            })\n",
    "            pbar.set_postfix({\"R\": float(R.mean()), \"KL\": float(approx_kl_ref)})\n",
    "\n",
    "        all_logs.append(logs)\n",
    "\n",
    "    return all_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4332970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PPO] Epoch 1/2: 100%|██████████| 8/8 [00:01<00:00,  4.21it/s, R=-1.69, KL=0.0363]  \n",
      "[PPO] Epoch 2/2: 100%|██████████| 8/8 [00:01<00:00,  4.42it/s, R=-1.85, KL=0.0324]\n"
     ]
    }
   ],
   "source": [
    "# 运行最小 PPO\n",
    "# prompts = [\"什么是 AI？\", \"1+1 等于几？\", \"写一句问候语\", \"什么是机器学习？\", \"如何保护环境？\"]\n",
    "prompts = [\"What is AI?\", \"What is 1+1?\", \"Write a greeting\", \"What is machine learning?\", \n",
    "           \"How to protect the environment?\"]\n",
    "ppo_logs = ppo_train(\n",
    "    policy,          # 可训练策略 (actor+critic)\n",
    "    reward_model,    # 奖励模型 (冻结)\n",
    "    ref_policy,      # 参考模型 (冻结)\n",
    "    tokenizer,\n",
    "    prompts,\n",
    "    epochs=2,\n",
    "    rollouts_per_epoch=8,\n",
    "    max_new_tokens=32\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "534405c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "优势函数:\n",
      "tensor([[ 0.9801,  0.2000,  0.4423,  0.7085,  1.0000],\n",
      "        [ 0.6623, -0.0326,  0.1886,  0.4323,  0.7000]])\n",
      "\n",
      "回报:\n",
      "tensor([[0.9801, 1.0000, 1.0423, 1.1085, 1.2000],\n",
      "        [0.6623, 0.6674, 0.6886, 0.7323, 0.8000]])\n"
     ]
    }
   ],
   "source": [
    "## 计算 GAE 的例子\n",
    "def get_advantages_and_returns_example():\n",
    "    # 输入数据\n",
    "    values = torch.tensor([\n",
    "        [0.0, 0.8, 0.6, 0.4, 0.2],\n",
    "        [0.0, 0.7, 0.5, 0.3, 0.1]\n",
    "    ])\n",
    "    rewards = torch.tensor([\n",
    "        [0.0, -0.01, -0.02, -0.03, 1.2],\n",
    "        [0.0, -0.005, -0.015, -0.025, 0.8]\n",
    "    ])\n",
    "    action_mask = torch.tensor([\n",
    "        [0, 1, 1, 1, 1],\n",
    "        [0, 1, 1, 1, 1]\n",
    "    ])\n",
    "    gamma = 0.99\n",
    "    lambd = 0.95\n",
    "    \n",
    "    # 应用 mask\n",
    "    values = action_mask * values\n",
    "    rewards = action_mask * rewards\n",
    "    \n",
    "    lastgaelam = torch.zeros(2)  # 为每个序列初始化\n",
    "    advantages_reversed = []\n",
    "    response_length = rewards.size(1)\n",
    "    \n",
    "    for t in reversed(range(response_length)):\n",
    "        nextvalues = values[:, t + 1] if t < response_length - 1 else torch.zeros(2)\n",
    "        delta = rewards[:, t] + gamma * nextvalues - values[:, t]\n",
    "        lastgaelam = delta + gamma * lambd * lastgaelam\n",
    "        advantages_reversed.append(lastgaelam.clone())\n",
    "    print(len(advantages_reversed))\n",
    "    advantages = torch.stack(advantages_reversed[::-1], dim=1)\n",
    "    returns = advantages + values\n",
    "    \n",
    "    print(\"优势函数:\")\n",
    "    print(advantages)\n",
    "    print(\"\\n 回报:\")\n",
    "    print(returns)\n",
    "    \n",
    "    return advantages, returns\n",
    "\n",
    "# 运行示例\n",
    "advantages, returns = get_advantages_and_returns_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe035f7",
   "metadata": {},
   "source": [
    "PPO 训练的核心是通过 **奖励模型 (RM)** 提供的反馈来优化语言模型 (Policy)。  \n",
    "- 在训练过程中，我们首先冻结奖励模型参数，保证奖励信号的稳定性。  \n",
    "- 每次迭代中，策略模型根据给定指令生成回答，并由奖励模型打分。  \n",
    "- 然后通过 **PPO 的裁剪目标** 来更新策略，限制参数变化幅度，防止训练不稳定。  \n",
    "\n",
    "随着训练的推进，策略生成的回答逐渐趋向于人类偏好，平均奖励也会提升。\n",
    "\n",
    "\n",
    "## 5. 实验结果与分析\n",
    "\n",
    "通过上述三个阶段的训练，我们完成了一个**简化版 InstructGPT** 的最小实现流程。虽然在数据规模和模型复杂度上大幅缩减，但核心思想完整保留：\n",
    "\n",
    "1. **监督微调 (SFT)**  \n",
    "   模型在少量人工构造的指令-回答对上进行训练，学会了最基本的“指令跟随”能力。\n",
    "\n",
    "2. **奖励模型 (RM)**  \n",
    "   奖励模型通过成对比较的方式学习区分“好回答”和“差回答”，并输出一个标量分数，作为后续强化学习的优化目标。\n",
    "\n",
    "3. **PPO 强化学习**  \n",
    "   策略模型根据 RM 的反馈进行优化。在裁剪机制和 KL penalty 的约束下，模型更新更加稳定，生成结果更符合人类偏好。\n",
    "\n",
    "\n",
    "整体来看，虽然这是一个实验性质的最小实现，但它已经展示了 **RLHF 的三大核心环节**：  \n",
    "- 指令跟随的基础能力 (SFT)  \n",
    "- 人类偏好建模 (RM)  \n",
    "- 强化学习驱动的策略优化 (PPO)  \n",
    "\n",
    "这样的流程为进一步扩展到真实大规模数据和更强大的模型奠定了基础。\n",
    "\n",
    "### 5.1 训练过程可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eba608a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 绘制训练损失曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# SFT 损失曲线\n",
    "plt.plot(range(1, len(sft_losses)+1), sft_losses, marker='o', color='blue')\n",
    "plt.title('SFT loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6128e",
   "metadata": {},
   "source": [
    "**预期结果**：SFT 阶段的损失应该呈现稳步下降趋势，表明模型正在学习指令与回答之间的关系。RM 阶段的损失也应该下降，表明奖励模型学会了区分好回答和差回答。PPO 阶段的平均奖励应该上升，表明模型生成的回答越来越符合奖励模型定义的\"好\"标准。\n",
    "\n",
    "### 5.2 模型效果对比\n",
    "\n",
    "我们可以对比 SFT 阶段和 PPO 阶段模型的生成效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0aaea688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成函数（贪心/采样简化版）\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt: str, max_new_tokens=32):\n",
    "    pack = tokenizer.encode_dialog(prompt, \"\")\n",
    "    input_ids = pack[\"input_ids\"].unsqueeze(0).to(device)\n",
    "    attn_mask = pack[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "    # 去掉右侧 PAD\n",
    "    true_len = int(attn_mask.sum(dim=1).item())\n",
    "    input_ids = input_ids[:, :true_len]\n",
    "    attn_mask = attn_mask[:, :true_len]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids, attention_mask=attn_mask)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        attn_mask = torch.cat([attn_mask, torch.ones_like(next_token)], dim=1)\n",
    "        if next_token.item() == tokenizer.EOS:\n",
    "            break\n",
    "\n",
    "    # decode：去掉 BOS/SEP/EOS，只取回答区间\n",
    "    ids = input_ids[0].tolist()\n",
    "    if tokenizer.SEP in ids:\n",
    "        sep_idx = ids.index(tokenizer.SEP)\n",
    "        ans_ids = ids[sep_idx+1:]\n",
    "    else:\n",
    "        ans_ids = ids\n",
    "    # 去掉 PAD 和 EOS\n",
    "    ans_ids = [i for i in ans_ids if i not in (tokenizer.PAD, tokenizer.EOS)]\n",
    "    # 减 4 还原字节\n",
    "    text = bytes([i-4 for i in ans_ids]).decode(\"utf-8\", errors=\"ignore\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "906dd5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SFT 模型回答 ===\n",
      "Q: Write a greeting\n",
      "A: llololollinint te togy te t tet\n",
      "\n",
      "=== PPO 优化后策略回答 ===\n",
      "Q: Write a greeting\n",
      "A: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型效果对比\n",
    "test_prompts = [\n",
    "    \"Write a greeting\"\n",
    "]\n",
    "\n",
    "print(\"=== SFT 模型回答 ===\")\n",
    "for p in test_prompts:\n",
    "    ans = generate_text(model, tokenizer, p)\n",
    "    print(f\"Q: {p}\\nA: {ans}\\n\")\n",
    "\n",
    "print(\"=== PPO 优化后策略回答 ===\")\n",
    "for p in test_prompts:\n",
    "    ans = generate_text(policy, tokenizer, p)\n",
    "    print(f\"Q: {p}\\nA: {ans}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41794438",
   "metadata": {},
   "source": [
    "经过 PPO 优化的模型生成的回答应该比仅经过 SFT 的模型更符合人类偏好。\n",
    "\n",
    "## 6. 总结与思考\n",
    "\n",
    "本实验通过简化实现复现了 InstructGPT 的 RLHF 三阶段流程。虽然实际应用中的实现更加复杂，但核心思想是一致的：\n",
    "\n",
    "1. **监督微调**为模型提供基础的指令跟随能力\n",
    "2. **奖励模型**学习人类偏好并提供量化反馈\n",
    "3. **PPO 强化学习**利用奖励信号进一步优化模型\n",
    "\n",
    "这种方法的优势在于能够将人类的主观偏好有效地传递给模型，使模型生成更加符合人类期望的内容。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
