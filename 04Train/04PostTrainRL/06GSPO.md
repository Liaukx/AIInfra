# GSPO

Author by: 潘江


## 一、引言：为什么需要学习 GSPO？

在大语言模型（LLM）的训练流程中，强化学习（RL）是提升模型高阶能力（如推理、复杂指令遵循）的关键环节。Qwen3 作为当前主流的大模型之一，其性能提升的核心秘诀在于采用了**组序列策略优化（Group Sequence Policy Optimization, GSPO）** 算法。

本文档将系统解析 GSPO 的原理、创新点及实验效果，下面是具体内容
- 大模型强化学习的核心挑战
- GSPO 如何突破传统算法的局限
- 序列级优化对模型训练的重要意义

## 二、LLM 强化学习基础

### 2.1 大模型训练的三个阶段
1. **预训练**：通过海量文本学习语言规律和知识
2. **监督微调**：使用高质量问答数据调教模型的对话能力
3. **强化学习**：提升推理、复杂指令遵循等高阶能力（本文重点）

### 2.2 PPO：强化学习的主流框架
PPO（Proximal Policy Optimization）的核心思想是**"小步快跑，保持稳定"**：
- 基于模型生成的回答（动作）和对应的奖励信号优化模型
- 通过限制新策略与旧策略的差异（近端优化）保证训练稳定
- 关键工具：**重要性采样**（解决用旧数据训练新模型的偏差问题）

#### 重要性采样公式

$$
\text{权重} = \frac{\text{某个数据点在「新」分布下出现的概率}}{\text{它在「旧」分布下出现的概率}} = \frac{\pi_{\text{新}}(\text{动作})}{\pi_{\text{旧}}(\text{动作})}
$$

### 2.3 PPO 的局限
- 需要额外的**价值模型（Critic Model）** 预测未来期望回报
- 价值模型与策略模型规模相当，训练成本高且估算不准
- 是整个系统中最脆弱的环节之一

## 三、前人算法的探索与局限

### 3.1 GRPO：摆脱价值模型的尝试
GRPO（Group Relative Policy Optimization）的创新点：
- 用**组内奖励平均值**替代价值模型作为基准
- 对同一问题生成多个回答，通过相对排名计算优势（Advantage）
  - 例：4 个回答得分 95、70、85、80，平均分 82.5
  - 优势 = 回答得分 - 平均分（A 的优势为+12.5，B 为-12.5）

### 3.2 GRPO 的核心缺陷
- 在**token 级别计算重要性权重**，违背重要性采样理论
- 单 token 的权重充满随机噪声，且随序列长度增加累积
- 导致训练不稳定，甚至**模型崩溃**
- 在混合专家（MoE）模型上问题更严重

### 3.3 路由回放（Routing Replay）的无奈
为缓解 GRPO 在 MoE 上的问题而设计的补丁：
- 记录每个 token 由哪些专家处理
- 优化时强制新模型使用相同的专家指派
- 增加系统复杂性，限制模型探索能力

## 四、GSPO 的核心机制

### 4.1 核心思想
**奖励的单位应与优化的单位相匹配**：
- 奖励是针对**整个回答序列**的，因此优化也应在序列层面进行
- 类比：老师批改作文是给全文打分，而非逐字打分

### 4.2 序列级重要性权重
$$
s_i(\theta) = \left( \frac{\pi_{\theta}(y_i|x)}{\pi_{\theta_{\text{old}}}(y_i|x)} \right)^{\frac{1}{|y_i|}}
$$


- $\pi_{\theta}(y_i|x)$ 表示在新策略参数 $\theta$ 下，输入 $x$ 生成序列 $y_i$ 的概率；
- $|y_i|$ 表示序列 $y_i$ 的长度（token 数）；
- $\pi_{\theta_{\text{old}}}(y_i|x)$ 表示在旧策略参数 $\theta_{\text{old}}$ 下的概率。

### 4.3 为什么 GSPO 更稳定？
| 算法 | 指挥信号特点 | 训练效果 |
|------|------------|---------|
| GRPO | 对每个词元单独赋值（如"A 词 1.2 倍，B 词 0.9 倍"） | 指令矛盾，噪声大 |
| GSPO | 对整个序列统一赋值（如"整体 1.1 倍"） | 方向一致，更稳定 |

## 五、GSPO 的实验效果

### 5.1 核心发现一：更稳、更快、更强
- 在相同资源下，训练奖励和下游任务性能持续优于 GRPO
- 收敛速度更快，最终性能更高

### 5.2 核心发现二：反直觉的"裁剪悖论"
- GSPO 裁剪掉的词元比例远高于 GRPO（近两个数量级）
- 但学习效果更好，证明：**学习信号的质量远比数量重要**
- 类比：经验丰富的投资者只重仓优质项目，而非分散投资

### 5.3 核心发现三：MoE 模型上的优势
- 无需路由回放即可稳定训练 MoE 模型
- 对底层专家组合变化不敏感（关注整体序列概率）
- 显著简化训练流程，降低工程复杂度

## 六、GSPO 的贡献与局限

### 6.1 主要贡献
1. **理论创新**：指出词元级重要性采样的缺陷，回归序列级优化
2. **算法提升**：通过序列级权重和长度归一化提升稳定性与效率
3. **工程简化**：移除对路由回放等复杂技巧的依赖
4. **实际验证**：在 Qwen3 中成功应用，展现工业级有效性

### 6.2 局限性
- 仅用单一奖励评价整个序列，无法区分长回复中的优劣部分
- 性能依赖奖励模型质量，可能放大奖励模型的偏见
- 在需要词元级反馈的任务（如代码调试）中可能不占优
- 在创意写作等主观领域的效果尚需验证
