{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e39f293",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 02: DPO 与 PPO 在 LLM 对比\n",
    "\n",
    "在大语言模型和多模态大模型的发展中，如何让模型生成的内容更好地符合人类价值观和偏好是一个核心挑战。\n",
    "\n",
    "近端策略优化（PPO）作为强化学习的主流方法，通过奖励模型引导模型优化，在人类反馈的强化学习（RLHF）中取得了显著成果。然而，PPO 需要复杂的奖励模型设计和多阶段训练流程。直接偏好优化（DPO）则提供了一种更直接的解决方案，它通过比较不同响应的偏好数据来优化策略，避免了显式奖励模型的设计。\n",
    "\n",
    "本实验将使用 Hugging Face 的 Qwen-1.8B 模型作为基础模型，通过一个简化的文本生成任务，深入对比分析这两种方法在大语言模型场景下的表现。\n",
    "\n",
    "## 1. 实验环境设置\n",
    "\n",
    "首先，我们需要加载 Qwen-1.8B 模型并创建文本生成环境。Qwen 系列模型是由阿里巴巴开发的开源大语言模型，1.8B 版本在保持较好性能的同时计算资源需求适中，适合实验环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 设置设备 - 优先使用 GPU 加速计算\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 加载 Qwen-1.8B 模型和分词器\n",
    "model_name = \"Qwen/Qwen-1_8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 设置填充标记\n",
    "\n",
    "# 加载基础模型，使用 bfloat16 精度减少内存占用\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ").to(device)\n",
    "print(\"Qwen-1.8B 模型加载完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e933d2d",
   "metadata": {},
   "source": [
    "## 2. 文本生成环境\n",
    "\n",
    "为了对比 PPO 和 DPO，我们创建一个简化的文本生成环境。这个环境模拟了对话系统或文本补全任务的基本流程，其中模型需要根据给定的提示生成合适的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae645e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationEnv:\n",
    "    def __init__(self, prompt_list, max_length=30):\n",
    "        \"\"\"\n",
    "        文本生成环境\n",
    "        :param prompt_list: 提示文本列表\n",
    "        :param max_length: 生成文本的最大长度\n",
    "        \"\"\"\n",
    "        self.prompts = prompt_list\n",
    "        self.max_length = max_length\n",
    "        self.current_prompt = None\n",
    "        self.generated_text = \"\"\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"重置环境，随机选择一个提示\"\"\"\n",
    "        self.current_prompt = np.random.choice(self.prompts)\n",
    "        self.generated_text = \"\"\n",
    "        return self.current_prompt\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行一个动作（生成一个 token）\n",
    "        :param action: token ID\n",
    "        :return: 生成文本, 奖励, 是否完成\n",
    "        \"\"\"\n",
    "        # 解码 token 并添加到生成文本\n",
    "        token = tokenizer.decode([action])\n",
    "        self.generated_text += token\n",
    "        \n",
    "        # 检查终止条件：达到最大长度或生成结束标记\n",
    "        done = (len(self.generated_text) >= self.max_length or \n",
    "                action == tokenizer.eos_token_id)\n",
    "        \n",
    "        # 计算奖励\n",
    "        reward = self._calculate_reward()\n",
    "        \n",
    "        return self.generated_text, reward, done\n",
    "    \n",
    "    def _calculate_reward(self):\n",
    "        \"\"\"计算生成文本的奖励（简化版本）\"\"\"\n",
    "        # 在实际应用中，这里可以使用奖励模型或人工评估\n",
    "        # 这里使用简单的启发式规则评估生成质量\n",
    "        text = self.generated_text.lower()\n",
    "        prompt = self.current_prompt.lower()\n",
    "        \n",
    "        # 1. 长度奖励：鼓励生成长文本\n",
    "        length_reward = min(len(text) / self.max_length, 1.0)\n",
    "        \n",
    "        # 2. 多样性奖励：鼓励使用不同的词汇\n",
    "        unique_words = len(set(text.split()))\n",
    "        diversity_reward = min(unique_words / 10, 1.0)\n",
    "        \n",
    "        # 3. 相关性奖励：检查是否与提示相关\n",
    "        prompt_words = set(prompt.split())\n",
    "        response_words = set(text.split())\n",
    "        common_words = prompt_words & response_words\n",
    "        relevance_reward = min(len(common_words) / max(1, len(prompt_words)), 1.0)\n",
    "        \n",
    "        # 4. 流畅性奖励：简单检查常见连接词\n",
    "        fluency_reward = 0.5  # 基础值\n",
    "        for word in [\"and\", \"the\", \"but\", \"however\"]:\n",
    "            if word in text:\n",
    "                fluency_reward += 0.1\n",
    "        \n",
    "        # 加权组合各项奖励\n",
    "        total_reward = (length_reward * 0.3 + \n",
    "                        diversity_reward * 0.2 + \n",
    "                        relevance_reward * 0.3 + \n",
    "                        min(fluency_reward, 1.0) * 0.2)\n",
    "        \n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54de7d1",
   "metadata": {},
   "source": [
    "## 3. PPO 原理与实现\n",
    "\n",
    "PPO 算法的核心思想是通过限制策略更新的幅度来保证训练的稳定性。它使用一个裁剪函数来防止策略更新过大，从而避免训练过程中的剧烈波动。PPO 的目标函数可以表示为：\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}_t[\\min(r_t(\\theta)A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ 是策略比\n",
    "- $A_t$ 是优势函数，表示当前动作相对于平均水平的优势\n",
    "- $\\epsilon$ 是裁剪参数，通常设为 0.1-0.3\n",
    "\n",
    "这个目标函数的核心思想是：当策略比 $r_t(\\theta)$ 偏离 1 太远时，通过裁剪限制其影响，从而避免过大的策略更新。\n",
    "\n",
    "在大语言模型场景中，PPO 通常用于 RLHF 流程，通过奖励模型来优化策略。我们实现一个简化的 PPO 训练器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04714426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOPolicy(nn.Module):\n",
    "    \"\"\"包装语言模型作为策略网络\"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super(PPOPolicy, self).__init__()\n",
    "        self.model = base_model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        return self.model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def get_logits(self, input_ids, attention_mask=None):\n",
    "        \"\"\"获取语言模型的输出 logits\"\"\"\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "class PPO:\n",
    "    \"\"\"PPO 算法实现\"\"\"\n",
    "    def __init__(self, policy_model, value_model, ppo_epochs=4, lr=1e-5, gamma=0.99, epsilon=0.2):\n",
    "        \"\"\"\n",
    "        :param policy_model: 策略模型\n",
    "        :param value_model: 价值模型\n",
    "        :param ppo_epochs: PPO 更新轮数\n",
    "        :param lr: 学习率\n",
    "        :param gamma: 折扣因子\n",
    "        :param epsilon: 裁剪参数\n",
    "        \"\"\"\n",
    "        self.policy = policy_model\n",
    "        self.value_model = value_model\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # 创建优化器\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_model.parameters(), lr=lr)\n",
    "    \n",
    "    def generate(self, prompt, max_length=20):\n",
    "        \"\"\"使用当前策略生成文本\"\"\"\n",
    "        # 编码提示文本\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        generated = input_ids\n",
    "        log_probs = []  # 记录每个动作的对数概率\n",
    "        values = []     # 记录每个状态的价值\n",
    "        \n",
    "        # 逐步生成文本\n",
    "        for _ in range(max_length):\n",
    "            with torch.no_grad():\n",
    "                # 获取当前策略的输出 logits\n",
    "                logits = self.policy.get_logits(generated)\n",
    "                next_token_logits = logits[:, -1, :]\n",
    "                \n",
    "                # 创建分类分布并采样\n",
    "                dist = Categorical(logits=next_token_logits)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "                \n",
    "                # 获取当前状态的价值\n",
    "                value = self.value_model(generated).squeeze(-1)\n",
    "                \n",
    "            # 将新 token 添加到生成序列\n",
    "            generated = torch.cat([generated, action.unsqueeze(0)], dim=-1)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            \n",
    "            # 如果生成结束标记则提前终止\n",
    "            if action.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "        \n",
    "        return generated, torch.stack(log_probs), torch.stack(values)\n",
    "    \n",
    "    def update(self, prompts, rewards, old_log_probs, values):\n",
    "        \"\"\"更新策略和价值模型\"\"\"\n",
    "        # 计算折扣回报\n",
    "        returns = self._calculate_returns(rewards, values)\n",
    "        # 计算优势函数：回报 - 价值估计\n",
    "        advantages = returns - values\n",
    "        \n",
    "        # 多轮 PPO 更新\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            # 重新计算新策略的对数概率\n",
    "            new_log_probs = []\n",
    "            for prompt in prompts:\n",
    "                input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    logits = self.policy.get_logits(input_ids)\n",
    "                    # 只考虑最后一个 token 的分布\n",
    "                    dist = Categorical(logits=logits[:, -1, :])\n",
    "                    new_log_probs.append(dist.log_prob(input_ids[:, -1]))\n",
    "            \n",
    "            new_log_probs = torch.stack(new_log_probs)\n",
    "            \n",
    "            # 计算策略比率\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            \n",
    "            # 计算 PPO 裁剪目标函数\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # 更新策略网络\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            # 更新价值函数\n",
    "            value_loss = nn.MSELoss()(self.value_model(prompts), returns)\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()\n",
    "    \n",
    "    def _calculate_returns(self, rewards, values):\n",
    "        \"\"\"计算折扣回报\"\"\"\n",
    "        returns = []\n",
    "        R = 0\n",
    "        # 从后向前计算累积回报\n",
    "        for r in reversed(rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return torch.tensor(returns, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c8378c",
   "metadata": {},
   "source": [
    "## 4. DPO 原理与实现\n",
    "\n",
    "DPO 算法直接从人类偏好中学习策略，避免了显式奖励函数的设计。它基于一个关键洞见：最优策略可以通过 Bradley-Terry 模型表示：\n",
    "\n",
    "$$\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*(x,y)\\right)$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $\\pi_{ref}$ 是参考策略\n",
    "- $r^*$ 是最优奖励函数\n",
    "- $\\beta$ 是温度参数\n",
    "- $Z(x)$ 是归一化常数\n",
    "\n",
    "DPO 通过优化以下目标函数来学习策略：\n",
    "\n",
    "$$L_{DPO}(\\pi_\\theta) = -\\mathbb{E}_{(x,y_w,y_l)\\sim D}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)\\right]$$\n",
    "\n",
    "这个目标函数的核心思想是：对于给定的提示 $x$，偏好响应 $y_w$ 的对数概率应该高于非偏好响应 $y_l$ 的对数概率。\n",
    "\n",
    "DPO 不需要单独的价值函数或奖励模型，直接使用偏好数据优化策略："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0abc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPO:\n",
    "    \"\"\"DPO 算法实现\"\"\"\n",
    "    def __init__(self, policy_model, reference_model, beta=0.1, lr=1e-5):\n",
    "        \"\"\"\n",
    "        :param policy_model: 待优化的策略模型\n",
    "        :param reference_model: 参考模型（通常固定）\n",
    "        :param beta: 温度参数\n",
    "        :param lr: 学习率\n",
    "        \"\"\"\n",
    "        self.policy = policy_model\n",
    "        self.reference = reference_model\n",
    "        self.beta = beta\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "    \n",
    "    def update(self, prompts, preferred_responses, dispreferred_responses):\n",
    "        \"\"\"使用偏好数据更新策略\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        # 遍历每个偏好样本\n",
    "        for prompt, preferred, dispreferred in zip(prompts, preferred_responses, dispreferred_responses):\n",
    "            # 编码提示和响应\n",
    "            prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "            preferred_ids = tokenizer.encode(preferred, return_tensors=\"pt\").to(device)\n",
    "            dispreferred_ids = tokenizer.encode(dispreferred, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # 计算策略模型对偏好响应的对数概率\n",
    "            policy_logits = self.policy(torch.cat([prompt_ids, preferred_ids], dim=-1))\n",
    "            policy_log_probs = self._get_log_probs(policy_logits.logits, preferred_ids)\n",
    "            \n",
    "            # 计算参考模型对偏好响应的对数概率\n",
    "            ref_logits = self.reference(torch.cat([prompt_ids, preferred_ids], dim=-1))\n",
    "            ref_log_probs = self._get_log_probs(ref_logits.logits, preferred_ids)\n",
    "            \n",
    "            # 计算策略模型对非偏好响应的对数概率\n",
    "            policy_dis_logits = self.policy(torch.cat([prompt_ids, dispreferred_ids], dim=-1))\n",
    "            policy_dis_log_probs = self._get_log_probs(policy_dis_logits.logits, dispreferred_ids)\n",
    "            \n",
    "            # 计算参考模型对非偏好响应的对数概率\n",
    "            ref_dis_logits = self.reference(torch.cat([prompt_ids, dispreferred_ids], dim=-1))\n",
    "            ref_dis_log_probs = self._get_log_probs(ref_dis_logits.logits, dispreferred_ids)\n",
    "            \n",
    "            # 计算对数比值\n",
    "            log_ratio_preferred = (policy_log_probs - ref_log_probs).sum()\n",
    "            log_ratio_dispreferred = (policy_dis_log_probs - ref_dis_log_probs).sum()\n",
    "            \n",
    "            # 计算 DPO 损失\n",
    "            loss = -torch.log(\n",
    "                torch.sigmoid(\n",
    "                    self.beta * (log_ratio_preferred - log_ratio_dispreferred)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            losses.append(loss)\n",
    "        \n",
    "        # 平均损失并更新策略\n",
    "        total_loss = torch.stack(losses).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "    \n",
    "    def _get_log_probs(self, logits, labels):\n",
    "        \"\"\"计算标签序列的对数概率\"\"\"\n",
    "        # 将 logits 和 labels 对齐\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "        \n",
    "        # 计算每个 token 的对数概率\n",
    "        return nn.functional.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "            reduction='none'\n",
    "        ).view(shift_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47866c23",
   "metadata": {},
   "source": [
    "## 5. 准备训练数据\n",
    "\n",
    "我们创建一组多样化的提示文本，并生成模拟的偏好数据用于训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练提示\n",
    "prompts = [\n",
    "    \"The weather today is\",\n",
    "    \"I really enjoy\",\n",
    "    \"In my opinion,\",\n",
    "    \"The best thing about\",\n",
    "    \"I think that\",\n",
    "    \"Artificial intelligence\",\n",
    "    \"Machine learning models\",\n",
    "    \"Deep reinforcement learning\",\n",
    "    \"Natural language processing\",\n",
    "    \"The future of AI\",\n",
    "    \"Climate change is\",\n",
    "    \"Renewable energy sources\",\n",
    "    \"The impact of technology\",\n",
    "    \"Education in the digital age\",\n",
    "    \"Cultural diversity means\"\n",
    "]\n",
    "\n",
    "# 生成模拟偏好数据\n",
    "def generate_preference_data(num_samples=100):\n",
    "    \"\"\"生成模拟的偏好数据\"\"\"\n",
    "    preferences = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        prompt = np.random.choice(prompts)\n",
    "        \n",
    "        # 生成两种可能的回应\n",
    "        response_options = [\n",
    "            \"nice and sunny, perfect for outdoor activities.\",\n",
    "            \"quite unpredictable, with a chance of rain later.\",\n",
    "            \"a fascinating field with immense potential.\",\n",
    "            \"challenging but rewarding to study and apply.\",\n",
    "            \"essential for addressing global challenges.\",\n",
    "            \"a fundamental aspect of human society.\"\n",
    "        ]\n",
    "        \n",
    "        # 随机选择两个不同的回应\n",
    "        idx1, idx2 = np.random.choice(len(response_options), 2, replace=False)\n",
    "        response1 = response_options[idx1]\n",
    "        response2 = response_options[idx2]\n",
    "        \n",
    "        # 随机分配偏好（实际应用中来自人类标注）\n",
    "        if np.random.random() > 0.5:\n",
    "            preferred = response1\n",
    "            dispreferred = response2\n",
    "        else:\n",
    "            preferred = response2\n",
    "            dispreferred = response1\n",
    "        \n",
    "        preferences.append((prompt, preferred, dispreferred))\n",
    "    \n",
    "    return preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9471b0",
   "metadata": {},
   "source": [
    "## 6. 模型初始化\n",
    "\n",
    "我们初始化策略模型、价值模型（用于 PPO）和参考模型（用于 DPO）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化策略模型（将用于两种算法）\n",
    "policy_model = PPOPolicy(base_model).to(device)\n",
    "\n",
    "# 价值模型（用于 PPO）\n",
    "# 这是一个简单的神经网络，用于估计状态价值\n",
    "value_model = nn.Sequential(\n",
    "    nn.Linear(base_model.config.hidden_size, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1)\n",
    ").to(device)\n",
    "\n",
    "# 参考模型（用于 DPO）\n",
    "# 我们加载一个新的模型实例作为参考模型\n",
    "reference_model = PPOPolicy(AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ").to(device))\n",
    "\n",
    "# 冻结参考模型参数\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 初始化训练器\n",
    "ppo_trainer = PPO(policy_model, value_model)\n",
    "dpo_trainer = DPO(policy_model, reference_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eca338",
   "metadata": {},
   "source": [
    "## 7. 模型训练循环\n",
    "\n",
    "我们分别实现 PPO 和 DPO 的训练循环："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(ppo_trainer, env, num_episodes=50):\n",
    "    \"\"\"PPO 训练循环\"\"\"\n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # 重置环境\n",
    "        prompt = env.reset()\n",
    "        \n",
    "        # 生成文本\n",
    "        generated, log_probs, values = ppo_trainer.generate(prompt)\n",
    "        generated_text = tokenizer.decode(generated[0])\n",
    "        \n",
    "        # 计算奖励（使用环境中的奖励函数）\n",
    "        # 注意：这里我们只取生成部分（不包括提示）\n",
    "        env.generated_text = generated_text[len(prompt):]\n",
    "        reward = env._calculate_reward()\n",
    "        \n",
    "        # 更新策略\n",
    "        ppo_trainer.update([prompt], [reward], log_probs, values)\n",
    "        \n",
    "        # 记录奖励历史\n",
    "        rewards_history.append(reward)\n",
    "        \n",
    "        # 定期输出进度\n",
    "        if episode % 5 == 0:\n",
    "            print(f\"PPO Episode {episode}: 奖励={reward:.3f}\")\n",
    "            print(f\"  提示: '{prompt}'\")\n",
    "            print(f\"  生成: '{generated_text}'\\n\")\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "def train_dpo(dpo_trainer, preference_data, num_epochs=10):\n",
    "    \"\"\"DPO 训练循环\"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 打乱数据\n",
    "        np.random.shuffle(preference_data)\n",
    "        \n",
    "        # 拆分数据\n",
    "        prompts = [d[0] for d in preference_data]\n",
    "        preferred = [d[1] for d in preference_data]\n",
    "        dispreferred = [d[2] for d in preference_data]\n",
    "        \n",
    "        # 更新策略\n",
    "        loss = dpo_trainer.update(prompts, preferred, dispreferred)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # 定期输出进度\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"DPO Epoch {epoch}: 损失={loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 创建环境\n",
    "env = TextGenerationEnv(prompts)\n",
    "\n",
    "# 生成偏好数据\n",
    "preference_data = generate_preference_data(num_samples=100)\n",
    "\n",
    "# 运行训练\n",
    "print(\"开始 PPO 训练...\")\n",
    "ppo_rewards = train_ppo(ppo_trainer, env)\n",
    "\n",
    "print(\"\\n 开始 DPO 训练...\")\n",
    "dpo_losses = train_dpo(dpo_trainer, preference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26106c33",
   "metadata": {},
   "source": [
    "## 8. 结果分析\n",
    "\n",
    "训练完成后，我们可视化结果并比较生成文本的质量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# PPO 奖励曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ppo_rewards, label='PPO 奖励', color='blue')\n",
    "plt.xlabel('训练轮次')\n",
    "plt.ylabel('奖励')\n",
    "plt.title('PPO 训练奖励变化')\n",
    "plt.grid(True)\n",
    "\n",
    "# DPO 损失曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(dpo_losses, label='DPO 损失', color='red')\n",
    "plt.xlabel('训练轮次')\n",
    "plt.ylabel('损失')\n",
    "plt.title('DPO 训练损失变化')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 测试生成质量\n",
    "def test_generation(model, prompts, num_samples=3):\n",
    "    \"\"\"测试模型生成质量\"\"\"\n",
    "    print(\"\\n 生成文本质量测试:\")\n",
    "    for i, prompt in enumerate(prompts[:num_samples]):\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 使用采样生成更自然的文本\n",
    "            outputs = model.model.generate(\n",
    "                input_ids,\n",
    "                max_length=50,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"样本 {i+1}:\")\n",
    "        print(f\"  提示: '{prompt}'\")\n",
    "        print(f\"  生成: '{generated_text}'\\n\")\n",
    "\n",
    "# 测试基础模型\n",
    "print(\"基础模型生成结果:\")\n",
    "test_generation(policy_model, prompts)\n",
    "\n",
    "# 测试 PPO 微调后的模型\n",
    "print(\"PPO 微调后模型生成结果:\")\n",
    "test_generation(ppo_trainer.policy, prompts)\n",
    "\n",
    "# 测试 DPO 微调后的模型\n",
    "print(\"DPO 微调后模型生成结果:\")\n",
    "test_generation(dpo_trainer.policy, prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd16516",
   "metadata": {},
   "source": [
    "## 9. 讨论与结论\n",
    "\n",
    "PPO 训练过程中奖励值逐渐提高，表明模型学会了生成更符合奖励函数定义的文本。PPO 的优势在于它能够直接从环境中学习，但需要精心设计奖励函数。在文本生成任务中，设计一个全面评估文本质量的奖励函数本身就是一项挑战。\n",
    "\n",
    "DPO 训练过程中损失值逐渐降低，表明模型学会了区分偏好和非偏好响应。DPO 避免了奖励函数的设计问题，但需要高质量的偏好数据。在实际应用中，获取大规模高质量的偏好数据可能需要大量人工标注工作。\n",
    "\n",
    "在生成质量方面，基础模型生成的文本通常较为通用，缺乏针对性；PPO 微调后的模型生成的文本更符合奖励函数的定义（如长度、多样性、相关性）；而 DPO 微调后的模型生成的文本更符合人类偏好，表现出更好的主观质量。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
