{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16934c3c",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 02: 梯度检查点内存优化\n",
    "\n",
    "本文将探讨深度学习训练过程中的内存瓶颈问题，重点介绍梯度检查点技术及其实现。通过实际代码演示，我们将展示如何通过重计算策略减少内存占用，并分析内存与计算之间的权衡关系。\n",
    "\n",
    "## 1. 内存瓶颈分析\n",
    "\n",
    "在深度学习训练过程中，内存消耗是一个关键限制因素。当我们训练大型神经网络时，前向传播过程中产生的中间激活值需要被保存，以便在反向传播时计算梯度。这些中间激活值占用了大量的显存，尤其是在处理长序列或大批次数据时。\n",
    "\n",
    "具体来说，对于一个有 L 层的神经网络，前向传播需要存储 L 个中间激活值。反向传播则需要这些激活值来计算梯度，导致内存使用量与网络深度成线性增长关系。这使得训练深层网络时经常遇到内存不足的问题。\n",
    "\n",
    "从数学角度看，标准反向传播算法需要保存所有中间激活值，其内存复杂度为 O(L)，其中 L 是网络层数。当网络深度增加时，这种线性增长的内存需求成为训练深度模型的主要瓶颈。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714973a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# 设置随机种子以确保实验可重复\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6967ac4",
   "metadata": {},
   "source": [
    "## 2. 梯度检查点原理\n",
    "\n",
    "梯度检查点技术（Gradient Checkpointing）通过**选择性保存**中间结果来优化内存使用。其核心思想是：不保存所有中间激活值，而是在反向传播时重新计算部分激活值。\n",
    "\n",
    "在前向传播过程中，我们只保存部分关键层的激活值（检查点），其他层的激活值则在反向传播需要时重新计算。这样虽然增加了计算量，但显著减少了内存使用。\n",
    "\n",
    "数学上，设网络有 L 层，如果我们每√L 层设置一个检查点，则内存使用量从 O(L)降低到 O(√L)，而计算量大约增加一倍（每个前向传播计算两次）。这种权衡可以用以下公式表示：\n",
    "\n",
    "内存节省率 ≈ 1 - (√L)/L\n",
    "计算增加率 ≈ (2√L)/L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17926da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, num_layers=10, hidden_size=512):\n",
    "        super(DeepMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # 输入层\n",
    "        self.layers.append(nn.Linear(1024, hidden_size))\n",
    "        \n",
    "        # 隐藏层 - 使用循环创建多个线性层\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        # 输出层\n",
    "        self.layers.append(nn.Linear(hidden_size, 10))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, use_checkpoint=False):\n",
    "        if use_checkpoint:\n",
    "            # 使用 PyTorch 内置的检查点功能\n",
    "            # checkpoint_sequential 会自动将网络分成多个段，并在需要时重新计算\n",
    "            return checkpoint_sequential(self.layers, 3, x)\n",
    "        else:\n",
    "            # 标准前向传播 - 保存所有中间激活值\n",
    "            for layer in self.layers[:-1]:\n",
    "                x = self.relu(layer(x))\n",
    "            return self.layersx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1858c",
   "metadata": {},
   "source": [
    "## 3. 检查点调度算法实现\n",
    "\n",
    "PyTorch 的`checkpoint_sequential`函数实现了检查点调度算法。它将网络分成多个段（segments），在前向传播时只保存每个段的输出，而不是所有中间结果。在反向传播时，它会重新计算每个段内的中间激活值。\n",
    "\n",
    "为了更好地理解这一过程，我们可以手动实现一个简化版本的检查点策略："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c64c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_checkpoint_forward(model, x, segments=3):\n",
    "    # 计算每段应该包含的层数\n",
    "    layers_per_segment = len(model.layers) // segments\n",
    "    checkpoints = []\n",
    "    \n",
    "    # 逐段处理\n",
    "    for i in range(segments):\n",
    "        start_idx = i * layers_per_segment\n",
    "        end_idx = (i + 1) * layers_per_segment if i < segments - 1 else len(model.layers)\n",
    "        \n",
    "        # 处理当前段\n",
    "        for j in range(start_idx, end_idx):\n",
    "            x = model.layersx\n",
    "            if j < end_idx - 1:  # 不是段的最后一层\n",
    "                x = model.relu(x)\n",
    "        \n",
    "        # 保存当前段的输出作为检查点\n",
    "        if i < segments - 1:  # 不是最后一段\n",
    "            checkpoints.append(x)\n",
    "            # 从计算图中分离，但保留梯度计算所需信息\n",
    "            x = x.detach()\n",
    "            x.requires_grad = True\n",
    "    \n",
    "    return x, checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebebc88",
   "metadata": {},
   "source": [
    "## 4. 内存-计算权衡分析\n",
    "\n",
    "现在我们来实际比较使用梯度检查点前后的内存使用情况。首先创建一个辅助函数来监控内存使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024 / 1024  # MB\n",
    "        return allocated, reserved\n",
    "    return 0, 0\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 创建模型和示例数据\n",
    "model = DeepMLP(num_layers=20, hidden_size=1024).to(device)\n",
    "input_data = torch.randn(32, 1024).to(device)  # 批次大小 32，输入维度 1024\n",
    "target = torch.randint(0, 10, (32,)).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7fea51",
   "metadata": {},
   "source": [
    "### 4.1 标准训练内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9afd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 标准训练模式 ===\")\n",
    "print(\"在前向传播中保存所有中间激活值，内存使用较高\")\n",
    "\n",
    "start_mem_alloc, start_mem_reserved = get_memory_usage()\n",
    "print(f\"初始内存 - 已分配: {start_mem_alloc:.2f} MB, 保留: {start_mem_reserved:.2f} MB\")\n",
    "\n",
    "# 前向传播\n",
    "output = model(input_data, use_checkpoint=False)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# 前向传播后内存使用会显著增加，因为保存了所有中间激活值\n",
    "forward_mem_alloc, forward_mem_reserved = get_memory_usage()\n",
    "print(f\"前向传播后内存 - 已分配: {forward_mem_alloc:.2f} MB, 保留: {forward_mem_reserved:.2f} MB\")\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# 反向传播后内存使用会减少，因为释放了部分中间结果\n",
    "end_mem_alloc, end_mem_reserved = get_memory_usage()\n",
    "print(f\"反向传播后内存 - 已分配: {end_mem_alloc:.2f} MB, 保留: {end_mem_reserved:.2f} MB\")\n",
    "\n",
    "standard_memory_peak = forward_mem_alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df2503",
   "metadata": {},
   "source": [
    "### 4.2 梯度检查点训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a31ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空 GPU 缓存以获得准确测量\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n=== 使用梯度检查点 ===\")\n",
    "print(\"只保存部分检查点，在反向传播时重新计算中间激活值\")\n",
    "\n",
    "start_mem_alloc, start_mem_reserved = get_memory_usage()\n",
    "print(f\"初始内存 - 已分配: {start_mem_alloc:.2f} MB, 保留: {start_mem_reserved:.2f} MB\")\n",
    "\n",
    "# 前向传播（使用检查点）\n",
    "output = model(input_data, use_checkpoint=True)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# 前向传播后内存使用增加较少，因为只保存了检查点\n",
    "forward_mem_alloc, forward_mem_reserved = get_memory_usage()\n",
    "print(f\"前向传播后内存 - 已分配: {forward_mem_alloc:.2f} MB, 保留: {forward_mem_reserved:.2f} MB\")\n",
    "\n",
    "# 反向传播 - 需要重新计算部分中间结果\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "end_mem_alloc, end_mem_reserved = get_memory_usage()\n",
    "print(f\"反向传播后内存 - 已分配: {end_mem_alloc:.2 极速 f} MB, 保留: {end_mem_reserved:.2f} MB\")\n",
    "\n",
    "checkpoint_memory_peak = forward_mem_alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9457c5",
   "metadata": {},
   "source": [
    "### 4.3 性能比较与分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3440f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能比较\n",
    "print(\"\\n=== 性能比较 ===\")\n",
    "print(f\"标准训练峰值内存: {standard_memory_peak:.2f} MB\")\n",
    "print(f\"检查点训练峰值内存: {checkpoint_memory_peak:.2f} MB\")\n",
    "memory_reduction = standard_memory_peak - checkpoint_memory_peak\n",
    "memory_reduction_percent = (memory_reduction / standard_memory_peak) * 100\n",
    "print(f\"内存减少: {memory_reduction:.2f} MB ({memory_reduction_percent:.1f}%)\")\n",
    "\n",
    "# 时间性能比较\n",
    "print(\"\\n=== 时间性能比较 ===\")\n",
    "\n",
    "# 标准训练时间\n",
    "start_time = time.time()\n",
    "for _ in range(5):  # 减少迭代次数以节省时间\n",
    "    output = model(input_data, use_checkpoint=False)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "standard_duration = time.time() - start_time\n",
    "\n",
    "# 检查点训练时间\n",
    "start_time = time.time()\n",
    "for _ in range(5):\n",
    "    output = model(input_data, use_checkpoint=True)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "checkpoint_duration = time.time() - start_time\n",
    "\n",
    "print(f\"标准训练时间 (5 次迭代): {standard_duration:.4f} 秒\")\n",
    "print(f\"检查点训练时间 (5 次迭代): {checkpoint_duration:.4f} 秒\")\n",
    "time_increase = checkpoint_duration - standard_duration\n",
    "time_increase_percent = (time_increase / standard_duration) * 100\n",
    "print(f\"时间增加: {time_increase:.4f} 秒 ({time_increase_percent:.1f}%)\")\n",
    "\n",
    "# 计算内存-计算权衡比\n",
    "tradeoff_ratio = memory_reduction / time_increase\n",
    "print(f\"内存-计算权衡比: {tradeoff_ratio:.2f} MB/秒 (每增加 1 秒训练时间节省的内存)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b09d1",
   "metadata": {},
   "source": [
    "### 4.4 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a66269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果可视化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 内存使用对比\n",
    "memory_data = [standard_memory_peak, checkpoint_memory_peak]\n",
    "labels = ['标准训练', '梯度检查点']\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "bars = ax1.bar(labels, memory_data, color=colors)\n",
    "ax1.set_ylabel('峰值内存使用 (MB)', fontsize=12)\n",
    "ax1.set_title('内存使用对比', fontsize=14)\n",
    "\n",
    "# 在柱状图上添加数值标签\n",
    "for bar, value in zip(bars, memory_data):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "             f'{value:.1f} MB', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "# 训练时间对比\n",
    "time_data = [standard_duration, checkpoint_duration]\n",
    "bars = ax2.bar(labels, time_data, color=colors)\n",
    "ax2.set_ylabel('训练时间 (秒)', fontsize=12)\n",
    "ax2.set_title('训练时间对比 (5 次迭代)', fontsize=14)\n",
    "\n",
    "# 在柱状图上添加数值标签\n",
    "for bar, value in zip(bars, time_data):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{value:.3f} s', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 绘制内存-计算权衡图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_data, memory_data, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('训练时间 (秒)', fontsize=12)\n",
    "plt.ylabel('峰值内存使用 (MB)', fontsize=12)\n",
    "plt.title('内存-计算权衡分析', fontsize=14)\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (time_data[i], memory_data[i]), \n",
    "                 textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f1196",
   "metadata": {},
   "source": [
    "## 5. 总结与思考\n",
    "\n",
    "从实验结果可以看出，梯度检查点技术显著减少了内存使用，但付出了计算时间增加的代价。这种内存-计算权衡（Memory-Computation Trade-off）是深度学习中常见的优化策略。\n",
    "\n",
    "梯度检查点的核心价值在于：**它使我们能够训练更深的网络或使用更大的批次大小，而这些在原本的内存限制下是不可能的**。虽然计算时间增加了，但内存使用减少了，这对于内存受限的环境非常有价值。\n",
    "\n",
    "通过本实验，我们深入理解了梯度检查点技术的原理和实现，并亲身体验了深度学习中的内存-计算权衡关系。这种优化技术是现代深度学习框架中不可或缺的一部分，对于高效训练大型模型至关重要。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
