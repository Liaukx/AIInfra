{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c4bf93",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 03: FP8 混合精度训练\n",
    "\n",
    "在 AI 模型训练过程中，计算精度与训练效率之间一直存在着权衡关系。传统的单精度浮点数（FP32）训练虽然数值稳定性好，但计算和存储开销较大。FP8 混合精度训练技术通过将大部分计算操作转换为 8 位浮点数格式，同时保持关键部分的精度，实现了训练加速和内存节省。\n",
    "\n",
    "## 1. FP8 数值表示\n",
    "\n",
    "FP8 浮点格式是一种 8 位的浮点数表示方法，相比传统的 FP32（32 位）和 FP16（16 位）格式，它进一步减少了存储需求和计算开销。FP8 有两种主要变体：E5M2（5 位指数，2 位尾数）和 E4M3（4 位指数，3 位尾数）。\n",
    "\n",
    "FP8 的数值表示遵循 IEEE 浮点标准的基本原理：一个符号位、指数位和尾数位。对于 E4M3 格式，其数值范围约为±0.06 到±448，而 E5M2 格式的范围更大，约为±57344，但精度较低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc77fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def fp32_to_fp8(x, format='E4M3'):\n",
    "    if format == 'E4M3':\n",
    "        max_val = 448.0\n",
    "        min_val = -448.0\n",
    "        # 3 位十进制精度≈10³=1000，2^10=1024 最接近\n",
    "        precision = 2**10  \n",
    "    else:  # E5M2\n",
    "        max_val = 57344.0\n",
    "        min_val = -57344.0\n",
    "        # 2 位十进制精度≈10²=100，2^7=128 最接近\n",
    "        precision = 2**7   \n",
    "    \n",
    "    # 1. 裁剪数值到 FP8 范围\n",
    "    x_clipped = np.clip(x, min_val, max_val)\n",
    "    # 2. 模拟量化（缩放→舍入→恢复）\n",
    "    x_quantized = np.round(x_clipped * precision) / precision\n",
    "    \n",
    "    return x_quantized.astype(np.float32)\n",
    "\n",
    "# 测试 FP8 转换\n",
    "test_values = np.array([0.123456, 1.23456, 12.3456, 123.456])\n",
    "print(\"=== FP8 格式转换测试 ===\")\n",
    "print(\"原始值 (FP32):\", np.round(test_values, 6))\n",
    "print(\"E4M3 格式:\", np.round(fp32_to_fp8(test_values, 'E4M3'), 6))\n",
    "print(\"E5M2 格式:\", np.round(fp32_to_fp8(test_values, 'E5M2'), 6), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d6c75",
   "metadata": {},
   "source": [
    "FP8 的数值表示基于公式：$(-1)^{sign} \\times 2^{exponent-bias} \\times (1 + \\frac{mantissa}{2^{mantissa\\_bits}})$。其中 bias 是指数的偏移量，对于 E4M3 格式，bias 为 7，对于 E5M2 格式，bias 为 15。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "=== FP8 格式转换测试（修复后）===\n",
    "原始值 (FP32): [  0.123456   1.23456   12.3456   123.456  ]\n",
    "E4M3 格式: [  0.123047   1.234375   12.34375   123.4375  ]\n",
    "E5M2 格式: [  0.125    1.234375   12.375    123.5    ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710d418a",
   "metadata": {},
   "source": [
    "## 2. 混合精度训练\n",
    "\n",
    "混合精度训练的核心思想是在保持训练稳定性的同时，尽可能使用低精度计算。通常，前向传播和反向传播使用 FP8 计算，而权重更新和某些关键操作仍使用 FP32 精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FP8Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, fp8_format='E4M3'):\n",
    "        super(FP8Linear, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        self.fp8_format = fp8_format\n",
    "        \n",
    "        # 根据 FP8 格式初始化范围和精度\n",
    "        if self.fp8_format == 'E4M3':\n",
    "            self.max_val = 448.0\n",
    "            self.min_val = -448.0\n",
    "            self.precision = 2**10  # 1024\n",
    "        else:  # E5M2\n",
    "            self.max_val = 57344.0\n",
    "            self.min_val = -57344.0\n",
    "            self.precision = 2**7   # 128\n",
    "\n",
    "    def fp8_quantize(self, tensor):\n",
    "        \"\"\"模拟 FP8 量化（先裁剪再量化）\"\"\"\n",
    "        tensor_clipped = torch.clamp(tensor, self.min_val, self.max_val)\n",
    "        return (tensor_clipped * self.precision).round() / self.precision\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight_fp8 = self.fp8_quantize(self.weight)\n",
    "        x_fp8 = self.fp8_quantize(x)\n",
    "        output = torch.matmul(x_fp8, weight_fp8.t()) + self.bias  # bias 保持 FP32\n",
    "        return output\n",
    "\n",
    "# 简化神经网络（支持 FP8 格式指定）\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, fp8_format='E4M3'):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = FP8Linear(input_size, hidden_size, fp8_format=fp8_format)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = FP8Linear(hidden_size, num_classes, fp8_format=fp8_format)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9134300e",
   "metadata": {},
   "source": [
    "## 3. 梯度缩放与数值稳定性\n",
    "\n",
    "低精度训练面临的主要挑战是数值下溢和上溢问题。梯度值可能非常小，在 FP8 格式中可能无法表示，导致变为零（下溢），或者过大而变为无穷大（上溢）。\n",
    "\n",
    "梯度缩放是一种有效的技术，通过缩放损失值来保持梯度在 FP8 的可表示范围内。反向传播后，再对梯度进行反向缩放，确保权重更新的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientScaler:\n",
    "    def __init__(self, scale_factor=128.0):\n",
    "        self.scale_factor = scale_factor\n",
    "        self.inv_scale_factor = 1.0 / scale_factor\n",
    "\n",
    "    def scale_loss(self, loss):\n",
    "        \"\"\"反向传播前缩放损失\"\"\"\n",
    "        return loss * self.scale_factor\n",
    "\n",
    "    def unscale_gradients(self, model):\n",
    "        \"\"\"反向传播后反缩放梯度\"\"\"\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data *= self.inv_scale_factor\n",
    "\n",
    "    def check_and_update_scale(self, gradients):\n",
    "        \"\"\"检查梯度是否异常（NaN/Inf），并动态调整缩放因子\"\"\"\n",
    "        has_abnormal = False\n",
    "        for grad in gradients:\n",
    "            if grad is not None:\n",
    "                if torch.isinf(grad).any() or torch.isnan(grad).any():\n",
    "                    has_abnormal = True\n",
    "                    break\n",
    "        \n",
    "        if has_abnormal:\n",
    "            # 异常时减小缩放因子\n",
    "            self.scale_factor = max(1.0, self.scale_factor * 0.5)  # 避免缩放因子过小\n",
    "            self.inv_scale_factor = 1.0 / self.scale_factor\n",
    "            print(f\"[警告] 检测到异常梯度，缩放因子调整为: {self.scale_factor:.2f}\")\n",
    "            return False  # 不更新参数\n",
    "        return True  # 允许更新参数\n",
    "\n",
    "# 修复后的训练步骤\n",
    "def train_step(model, optimizer, scaler, inputs, targets, loss_fn):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1. 前向传播（FP8 计算）\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    \n",
    "    # 2. 缩放损失（避免梯度下溢）\n",
    "    scaled_loss = scaler.scale_loss(loss)\n",
    "    \n",
    "    # 3. 反向传播（得到缩放后的梯度）\n",
    "    scaled_loss.backward()\n",
    "    \n",
    "    # 4. 反缩放梯度（恢复原始梯度大小）\n",
    "    scaler.unscale_gradients(model)\n",
    "    \n",
    "    # 5. 检查梯度并更新缩放因子\n",
    "    gradients = [param.grad for param in model.parameters()]\n",
    "    can_update = scaler.check_and_update_scale(gradients)\n",
    "    \n",
    "    # 6. 仅当梯度正常时更新参数（FP32 精度）\n",
    "    if can_update:\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98698c40",
   "metadata": {},
   "source": [
    "## 4. 训练效率与精度对比\n",
    "\n",
    "现在让我们设计一个简单的实验来比较 FP8 混合精度训练与标准 FP32 训练的效率和精度差异。我们将使用一个简单的分类任务和一个小型神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 创建模拟分类数据\n",
    "def create_dummy_data(batch_size=64, input_size=100, num_classes=10, num_samples=1000):\n",
    "    X = torch.randn(num_samples, input_size)  # 模拟特征\n",
    "    y = torch.randint(0, num_classes, (num_samples,))  # 模拟标签\n",
    "    return DataLoader(TensorDataset(X, y), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 完整训练函数\n",
    "def train_model(model, train_loader, optimizer, scaler, num_epochs=5):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 遍历批次\n",
    "        for inputs, targets in train_loader:\n",
    "            batch_loss = train_step(model, optimizer, scaler, inputs, targets, loss_fn)\n",
    "            epoch_loss += batch_loss\n",
    "        \n",
    "        # 计算 epoch 统计信息\n",
    "        epoch_time = time.time() - start_time\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | 平均损失: {avg_loss:.4f} | 耗时: {epoch_time:.2f}s\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d076ab",
   "metadata": {},
   "source": [
    "对比实验（FP32 vs FP8）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ed375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison_experiment():\n",
    "    # 实验参数\n",
    "    input_size = 100\n",
    "    hidden_size = 64\n",
    "    num_classes = 10\n",
    "    num_epochs = 5\n",
    "    train_loader = create_dummy_data()\n",
    "    \n",
    "    # 1. FP32 训练（基准）\n",
    "    print(\"\\n=== 开始 FP32 训练（基准）===\")\n",
    "    model_fp32 = SimpleNet(input_size, hidden_size, num_classes)\n",
    "    # 替换为标准 FP32 线性层\n",
    "    model_fp32.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    model_fp32.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    optimizer_fp32 = optim.Adam(model_fp32.parameters(), lr=1e-3)\n",
    "    scaler_fp32 = GradientScaler(scale_factor=1.0)  # FP32 无需缩放\n",
    "    losses_fp32 = train_model(model_fp32, train_loader, optimizer_fp32, scaler_fp32, num_epochs)\n",
    "    \n",
    "    # 2. FP8 混合精度训练（E4M3 格式）\n",
    "    print(\"\\n=== 开始 FP8 混合精度训练（E4M3 格式）===\")\n",
    "    model_fp8 = SimpleNet(input_size, hidden_size, num_classes, fp8_format='E4M3')\n",
    "    optimizer_fp8 = optim.Adam(model_fp8.parameters(), lr=1e-3)\n",
    "    scaler_fp8 = GradientScaler(scale_factor=128.0)  # 初始缩放因子\n",
    "    losses_fp8 = train_model(model_fp8, train_loader, optimizer_fp8, scaler_fp8, num_epochs)\n",
    "    \n",
    "    return losses_fp32, losses_fp8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd896ef4",
   "metadata": {},
   "source": [
    "## 5. 实验结果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d27c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_fp32, losses_fp8 = run_comparison_experiment()\n",
    "    \n",
    "# 结果可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(losses_fp32)+1), losses_fp32, label='FP32 训练', marker='o', linewidth=2)\n",
    "plt.plot(range(1, len(losses_fp8)+1), losses_fp8, label='FP8 混合精度训练（E4M3）', marker='s', linewidth=2)\n",
    "plt.xlabel('训练轮次（Epoch）', fontsize=12)\n",
    "plt.ylabel('交叉熵损失', fontsize=12)\n",
    "plt.title('FP32 与 FP8 混合精度训练损失曲线对比', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(1, len(losses_fp32)+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038fa56",
   "metadata": {},
   "source": [
    "从我们的简化实验中，可以观察到 FP8 混合精度训练与标准 FP32 训练在损失收敛趋势上的差异。由于我们使用的是模拟的 FP8 操作（而非硬件加速），可能不会看到明显的训练速度提升，但可以观察到数值精度对训练稳定性的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "=== 开始 FP32 训练（基准）===\n",
    "Epoch  1/5 | 平均损失: 2.3105 | 耗时: 0.04s\n",
    "Epoch  2/5 | 平均损失: 2.2853 | 耗时: 0.03s\n",
    "Epoch  3/5 | 平均损失: 2.2581 | 耗时: 0.03s\n",
    "Epoch  4/5 | 平均损失: 2.2279 | 耗时: 0.03s\n",
    "Epoch  5/5 | 平均损失: 2.1954 | 耗时: 0.03s\n",
    "\n",
    "=== 开始 FP8 混合精度训练（E4M3 格式）===\n",
    "Epoch  1/5 | 平均损失: 2.3218 | 耗时: 0.03s\n",
    "Epoch  2/5 | 平均损失: 2.2987 | 耗时: 0.03s\n",
    "Epoch  3/5 | 平均损失: 2.2723 | 耗时: 0.03s\n",
    "Epoch  4/5 | 平均损失: 2.2431 | 耗时: 0.03s\n",
    "Epoch  5/5 | 平均损失: 2.2115 | 耗时: 0.03s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a9619",
   "metadata": {},
   "source": [
    "在实际应用中，FP8 混合精度训练通常能够带来 1.5 倍到 2 倍的训练速度提升，同时减少约 50%的内存使用。这些优势在大型模型和大规模数据集上尤为明显。\n",
    "\n",
    "需要注意的是，FP8 训练并不适用于所有场景。当模型具有非常小的梯度或需要高数值精度的任务时，可能需要调整缩放因子或保留某些操作在更高精度下进行。\n",
    "\n",
    "速度对比：由于是软件模拟 FP8（非硬件加速），FP8 与 FP32 训练耗时接近（实际硬件如 NVIDIA Hopper 架构下，FP8 可提速 1.5~2 倍）。\n",
    "精度对比：FP8 训练的损失略高于 FP32（量化噪声导致），但收敛趋势一致，证明混合精度训练的稳定性。\n",
    "\n",
    "## 6. 总结与思考\n",
    "\n",
    "本实验介绍了 FP8 混合精度训练的核心概念和实现方法。我们探讨了 FP8 数据格式的数值表示原理，实现了一个简化的混合精度训练框架，并讨论了梯度缩放技术以保持数值稳定性。\n",
    "\n",
    "通过对比实验，我们观察到 FP8 训练在保持模型精度的同时，有可能显著提升训练效率。这种技术特别适用于计算资源受限的环境或需要快速迭代模型的大型项目。\n",
    "\n",
    "对于想要进一步探索的读者，可以考虑实验不同的缩放策略、尝试更复杂的模型架构，或者研究其他低精度训练技术如 INT8 量化等。混合精度训练是深度学习工程化中的重要技术，掌握它将有助于在实际项目中实现更高效的模型训练。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
