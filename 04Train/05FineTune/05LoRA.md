<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->

# 04.参数高效微调算法原理

> Author by: 李亚鹏

!!!!!!
1）一定要注意格式，看这个 PR 的改动；2）合并 LoRA 基本介绍和原理，内容太少了；3）算法解读不够深刻

## LoRA 核心原理

!!!!!!
重点解释低秩微调

**LoRA**，Low-Rank Adaptation：LoRA 调整的参数主要是 Self-Attention 里的四个权重矩阵 $W_q$（query)、$W_k$（key)、$W_v(value)$、$W_o$ (output)，以及前馈层的 $W_{up}$ 和 $W_{down}$ 。

将权重矩阵记为*W*，$d×d $ 维，输入特征为 $x$。LoRA 初始化两个矩阵 $A$ 和 $B$，对 $A$ 使用随机高斯初始化，将 $B$ 初始化为 0。$A$ 的维度为 $r×d$ 维，*B* 的维度为 $d×r$ 维。其中 $r$ 是 rank 秩，远小于 $d$ 。

![](images\04LoRA01.gif)

在前向传播的过程中，输出 $h$ 不再是原始权重矩阵 $W_0$，而是加上了 $BAx$，并加入参数 $α$ 缩放 $\Delta W_x$ ，如下式：

$$
h=W_0x+\alpha\Delta Wx=W_0x+\alpha BAx
$$

在参数更新时，LoRA 冻结了 $W_0$ 权重，只更新 $B$ 和 $A$。微调结束后，保留 $A$ 和 $B$ 作为 LoRA 矩阵，推理时用。为了减少推理延迟，可以将 $BA$ 矩阵与 $W_0$ 合并。以 query 值的计算为例，使用 LoRA 权重后，计算方式如下：

$$
query=(W_q+\alpha BA)x
$$

LoRA 的高效性主要来源于低秩。通过低秩近似，LoRA 能够捕捉到数据的主要变化方向，并在保证模型表达能力的前提下，通过调整少量参数实现对模型的微调。

因此，如何为不同任务选择合适的秩 r 是一个关键问题，r 的取值一般为 4 到 64 不等。一般来说简单任务需要的秩不大，而高难度任务需要较高的秩。

LoRA 不需要在模型中引入新的模块或网络结构，而是直接对现有的线性层进行调整。这使得 LoRA 的参数效率更高，且不增加模型的计算复杂度。与 prompt-base 微调算法相比，LoRA 的低秩矩阵分解能够在更深层次上调整模型的表示能力，而不仅仅是影响输入层或局部区域。

* **训练效率提升：**训练参数量一般低于总体参数的 1%，训练速度可提升数倍至数十倍，显存占用大幅降低，消费级显卡即可微调。

* **存储节省：**LoRA 模型文件极小，通常仅为几 MB 到几百 MB，便于分发管理。

* **即插即用：**一个基础模型可搭配多个不同 LoRA 模型文件，实现多种效果。

* **高质量微调：**性能媲美全量微调，且不易“遗忘”。

## 主流 LoRA 算法解读

### QLoRA 算法原理

!!!!!!!!!!!
补充内容

### AdapterLoRA 算法原理

LoRA 及其一些变体方法，通常为模型中所有需要修改的权重矩阵分配相同数量的可训练参数（例如相同的秩 r）。因此，调优性能不是最优的。

**AdaLoRA**，Adaptive Budget Allocation LoRA：将有限的参数预算（budget）动态地、不均匀地分配给那些更重要的权重矩阵，从而在总参数量不变或更少的情况下，达到更好的性能。

基于奇异值分解（Singular Value Decomposition ，SVD）的参数化 ：与 LoRA 不同， AdaLoRA 采用了另一种受 SVD 启发的参数化形式。它将增量 $\Delta$ 分解为左右正交矩阵 $P$ 和 $Q$，以及奇异值对角矩阵 $\Lambda$ 。

$$
W=W^{(0)}+\Delta=W^{(0)}+P\Lambda Q
$$

SVD 参数化有两大优势：

1. 通过控制对角矩阵 $Λ$ 中非零元素的个数，就可以直接、精确地控制增量矩阵 $\Delta$ 的秩。
2. 在 LoRA 中，如果要降低秩，就需要移除 B 的一列和 A 的一行，被剪掉的参数完全丢失。而在 AdaLoRA 中，剪枝操作只作用于奇异值，对应的奇异向量 P 和 Q 的部分依然保留并继续训练，使训练更稳定。

AdaLoRA 设计了一套新的重要性评分机制及剪枝策略。

评分的基本单元是由 $P_{k,i}$、$Q_{k,i}$、$\Lambda_{k,i}$ 构成的三元组。一个三元组的重要性 $S_{k,i}$ 由 $\Lambda_{k,i}$ 本身的重要性、 $P_{k,i}$ 中所有参数的平均重要性、$Q_{k,i}$ 中所有参数的平均重要性构成。（k 代表第 k 个权重矩阵，i 代表第 i 个奇异值分量）

$$
S_{k,i}=s(\lambda_{k,i})+\frac{1}{d_1}\sum_{j=1}^{d_1}s(P_{k,ji})+\frac{1}{d_2}\sum_{j=1}^{d_2}s(Q_{k,ij})
$$

单个参数的重要性 $s(w)$ 采用了考虑不确定性的敏感度分数。

敏感度 $I(w)$：定义为“权重与梯度的乘积的绝对值”，即 $I(w_{ij})=|w_{ij}\nabla_{w_{ij}}\mathcal{L}|$。

AdaLoRA 引入了指数移动平均 (EMA) 来平滑敏感度，并计算了不确定性项 $U^{(t)}$，来衡量近期敏感度的波动。

最终的单参数重要性是平滑后的敏感度与不确定性的乘积，这使得那些持续重要且学习尚不稳定的参数获得更高的分数。

$$
s^{(t)}(w_{ij})=\overline{I}^{(t)}(w_{ij})\cdot\overline{U}^{(t)}(w_{ij})
$$

在训练过程中，AdaLoRA 会周期性地根据当前预算 $b^{(t)}$  ，计算三元组重要性分数，保留分数最高的 $b^{(t)}$ 个三元组参数（保持它们的奇异值不变） ，并将其余三元组的奇异值设为零。

为了让整个训练过程更稳定、高效而设计的策略。 AdaLoRA 定义了总预算（即保留的奇异值总数）$b^{(t)}$ 如何随训练步数 $t$ 变化。

调度策略分为三个阶段：

1. **热身阶段** (Warm-up)：训练开始时，设置一个比最终目标预算 $b^{(T)}$ 更高的初始预算 $b^{(0)}$  。这允许模型在初期探索更广阔的参数空间，让更多的奇异值分量参与训练，从而更准确地评估它们的重要性。
2. **预算削减阶段** (Budget Pruning)：在热身结束后，采用一个三次函数将预算从 $b^{(0)}$ 平滑地降低到最终的目标预算 $b^{(T)}$。
3. **微调阶段** (Fine-tuning)：当预算达到 $b^{(T)}$ 后，保持预算不变，继续训练模型，直到收敛。

### DoRA 算法原理

**DoRA**，Weight-Decomposed Low-Rank：DoRA 将预训练的权重分解为量级和方向两部分进行微调，特别是采用 LoRA 进行方向部分的更新，以有效地减少可训练参数的数量，实现了与 FT（Full Fine-tuning）非常相似的学习能力。

![](images\05LoRA Variants01.png)

预训练的权重已经包含了适合下游任务的大量知识，所以仅仅对于量级和方向中的某一个进行较大幅度的更新，就已经足够了。

DoRA 限制 LoRA 只专注于方向 $V$ 适应，同时允许量级幅度 $m$ 可调，与原始方法相比简化了任务，在原始方法中，LoRA 需要学习两个幅度的调整。

$$
W^{\prime}=\underline{m}\frac{V+\Delta V}{||V+\Delta V||_c}=\underline{m}\frac{W_0+\underline{BA}}{||W_0+\underline{BA}||_c}
$$

对于两个微调维度量级变化量ΔM 以及方向变化量ΔD 进行深度的分析：

在中间步骤中，LoRA 表现出一致的正斜率趋势，这表明方向和量级的变化之间存在正比关系。

相比之下，全量微调（ Full Fine-tuning，FT）表现出更多样的学习模式，斜率相对为负。FT 和 LoRA 之间的这种区别反映了它们各自的学习能力。LoRA 倾向于按比例增加或减少量级和方向更新，但它缺乏进行更细微调整的细微能力。

而 DoRA 的行为更符合 FT 的学习模式，能够执行更轻微的方向变化和更显著的量级变化。

![](./images05LoRA Variants02.png)

###LoRA++ 算法原理

### MoE_LoRA 算法原理

## LoRA 微调应用场景

## 多模态应用

!!!!!!!
内容太浅了

现有多模态模型，依赖一个独立的、预训练的视觉模型（如 ViT）来提取图像特征。

推理时，ViT 和连接器是额外且独立的模块，这增加了计算成本和内存占用。

工作流程是串行的，LLM 必须等待 ViT 完全处理完图像才能开始工作。

!!!!!!!!!!!!!
注意 markdown 的格式，请参考其他 done 的文章
<img src="images\04LoRA02.png" style="zoom: 67%;" />

**VoRA**（Vision as LoRA） 提出了一种颠覆性的解决方案：不再依赖外部视觉专家，而是让 LLM 自己“学会”看图。

在 LLM 的每一层线性层中，都加入专门用于处理视觉信息的低秩适配器（LoRA）。

在训练时，冻结 LLM 的原始参数，只训练这些新加入的 LoRA 层和轻量级的图像嵌入层,从而将“视觉”能力，直接集成到 LLM 内部。

将语言能力（在冻结的 LLM 中）和新学习的视觉能力（在 LoRA 层中）解耦，从而避免了直接训练整个模型时可能出现的训练不稳定或“灾难性遗忘”问题。

<img src="images\04LoRA03.png" style="zoom: 67%;" />

## 总结与思考

!!!!!!!!!!补充

## 参考与引用

- AdaLoRA：Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning， https://arxiv.org/abs/2303.10512
- DoRA：Weight-Decomposed Low-Rank Adaptation， https://arxiv.org/abs/2402.09353
- LoRA：Low-Rank Adaptation of Large Language Models，https://arxiv.org/abs/2106.09685 
- VoRA：Vision as LoRA， https://arxiv.org/abs/2503.20680
