{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964e933a",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 03: Pipeline 并行实践\n",
    "\n",
    "> Author by: 许灿岷\n",
    "\n",
    "本实验旨在深入理解 Pipeline 并行原理。先实现 Gpipe 流水线并分析空泡率现象，后进阶实现 1F1B 和 Interleaved 1F1B 调度策略，优化空泡率现象，并实践混合并行策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a924ba",
   "metadata": {},
   "source": [
    "## 1. Pipeline 并行基础\n",
    "\n",
    "**Pipeline 并行（Pipeline Parallelism, PP）** 其核心思想是将一个庞大的神经网络模型，沿着层（Layer）的维度进行纵向切割，分割成多个连续的子模块（称为“阶段”，Stage），并将这些阶段部署到不同的计算设备（如 GPU）上。\n",
    "\n",
    "数学上，模型可表示为函数复合：$F(x) = f_n(f_{n-1}(...f_1(x)...))$，其中每个 $f_i$（模型层/层组）对应 Pipeline 的一个“阶段”，分配到不同设备上执行。数据以“批次”（batch）的形式，像工厂流水线一样，依次流经各个阶段。\n",
    "\n",
    "通过这种方式，每个设备只需加载和处理模型的一部分，从而突破**单卡显存的限制**。\n",
    "\n",
    "然而，这种拆分也引入了新的挑战：\n",
    "*   **通信开销：** 前向传播和反向传播过程中，相邻阶段之间需要频繁地传递中间结果（激活值和梯度），这会带来额外的通信延迟。\n",
    "*   **空泡现象（Bubble）：** 由于流水线的“填充”（Fill）和“排空”（Drain）过程，部分设备在某些时刻会处于等待数据的空闲状态，造成计算资源的浪费。\n",
    "\n",
    "**后续优化方向**：\n",
    "Gpipe、1F1B、Interleaved 1F1B 等调度策略，本质都是通过调整「前向」和「反向」的执行节奏，来**压缩空泡时间、降低通信影响、更高效利用显存** —— 这些我们将在代码实践中逐一实现和对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "54f0ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bb91bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_devices(max_devices=4):\n",
    "    \"\"\"自动获取可用设备\"\"\"\n",
    "    devices = []\n",
    "    num_cuda = torch.cuda.device_count()\n",
    "    if num_cuda > 0:\n",
    "        devices = [torch.device(f\"cuda:{i}\") for i in range(min(num_cuda, max_devices))]\n",
    "    else:\n",
    "        devices = [torch.device(\"cpu\")]\n",
    "    print(f\"可用设备列表: {[str(dev) for dev in devices]}\")\n",
    "    return devices\n",
    "\n",
    "\n",
    "def calculate_bubble_rate(strategy_name, num_stages, num_microbatches, interleaving_degree=2):\n",
    "    \"\"\"根据策略类型计算正确的空泡率\"\"\"\n",
    "    if num_stages == 1:\n",
    "        return 0.0\n",
    "\n",
    "    if strategy_name == \"Naive\":\n",
    "        # Naive 策略没有流水线并行，空泡率为 0\n",
    "        return 0.0\n",
    "    elif strategy_name == \"GPipe\":\n",
    "        # GPipe 的空泡率公式\n",
    "        return (num_stages - 1) / (num_microbatches + num_stages - 1)\n",
    "    elif strategy_name == \"1F1B\":\n",
    "        # 1F1B 的空泡率公式\n",
    "        return (num_stages - 1) / num_microbatches\n",
    "    elif strategy_name == \"Interleaved 1F1B\":\n",
    "        # Interleaved 1F1B 的空泡率公式\n",
    "        return (num_stages - 1) / (num_microbatches * interleaving_degree)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def create_model_parts(input_size=100, output_size=10):\n",
    "    \"\"\"创建更复杂的模型分段\"\"\"\n",
    "    layers = [\n",
    "        nn.Sequential(\n",
    "            nn.Linear(100, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "    ]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637649d",
   "metadata": {},
   "source": [
    "## 2. Native Pipeline Parallelism（传统流水线并行）\n",
    "\n",
    "首先，我们实现一个基础的流水线并行框架，只考虑了模型分割和流水线调度，将数据以 batch 为单位进行处理。\n",
    "\n",
    "![](./images/Code03Pipeline01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivePipelineParallel(nn.Module):\n",
    "    def __init__(self, module_list, device_ids):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"模块数量必须与设备数量相同\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_stages = len(device_ids)\n",
    "\n",
    "        # 将每个阶段移动到对应的设备\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediates = []\n",
    "        current_output = x.to(self.device_ids[0])\n",
    "\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            current_output = stage(current_output)\n",
    "            if i < len(self.stages) - 1:\n",
    "                # 移除 detach()，保留梯度\n",
    "                current_output_act = current_output.requires_grad_(True)\n",
    "                intermediates.append(current_output_act)\n",
    "                current_output = current_output_act.to(self.device_ids[i+1])\n",
    "\n",
    "        return current_output, intermediates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4a319",
   "metadata": {},
   "source": [
    "上面的代码实现了一个基础的流水线并行框架。它将模型分割为多个阶段，每个阶段放置在不同的设备上。在前向传播过程中，数据依次通过这些阶段，并在阶段间进行设备间的数据传输。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc12d09",
   "metadata": {},
   "source": [
    "## 3. Gpipe 流水线并行\n",
    "\n",
    "Gpipe(Gradient Pipeline) 是一种基于流水线并行的模型并行策略，它将一个大的训练批次（Batch）拆分成多个小的微批次（Micro-batch），依次流过 Pipeline 的各个阶段，每个阶段放置在不同的设备上。\n",
    "\n",
    "![](./images/Code03Pipeline02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1d3f0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GPipeParallel(nn.Module):\n",
    "    def __init__(self, module_list, device_ids, num_microbatches=4):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"模块数量必须与设备数量相同\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_stages = len(device_ids)\n",
    "        self.num_microbatches = num_microbatches\n",
    "\n",
    "        # 将每个阶段移动到对应的设备\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"GPipe 策略: 先所有微批次前向，再所有微批次反向\"\"\"\n",
    "        # 分割输入为微批次\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        activations = [[] for _ in range(self.num_stages)]\n",
    "\n",
    "        # 前向传播: 所有微批次通过所有阶段\n",
    "        for i, micro_batch in enumerate(micro_batches):\n",
    "            current = micro_batch.to(self.device_ids[0])\n",
    "            for stage_idx, stage in enumerate(self.stages):\n",
    "                current = stage(current)\n",
    "                if stage_idx < self.num_stages - 1:\n",
    "                    # 保存中间激活值，保留梯度计算\n",
    "                    current_act = current.detach().clone().requires_grad_(True)\n",
    "                    activations[stage_idx].append(current_act)\n",
    "                    current = current_act.to(self.device_ids[stage_idx + 1])\n",
    "                else:\n",
    "                    # 最后阶段直接保存输出\n",
    "                    activations[stage_idx].append(current)\n",
    "\n",
    "        # 拼接最终输出\n",
    "        output = torch.cat(activations[-1], dim=0)\n",
    "        return output, activations\n",
    "\n",
    "    def backward(self, loss, activations):\n",
    "        \"\"\"GPipe 反向传播 - 修复版本\"\"\"\n",
    "        # 计算最终损失梯度\n",
    "        loss.backward()\n",
    "\n",
    "        # 从最后阶段开始反向传播\n",
    "        for stage_idx in range(self.num_stages - 2, -1, -1):\n",
    "            # 获取当前阶段的激活值和下一阶段的梯度\n",
    "            stage_activations = activations[stage_idx]\n",
    "            next_gradients = []\n",
    "\n",
    "            # 收集下一阶段的梯度\n",
    "            for act in activations[stage_idx + 1]:\n",
    "                if act.grad is not None:\n",
    "                    # 确保梯度形状匹配\n",
    "                    grad = act.grad\n",
    "                    if grad.shape != stage_activations[0].shape:\n",
    "                        # 如果形状不匹配，尝试调整梯度形状\n",
    "                        try:\n",
    "                            grad = grad.view(stage_activations[0].shape)\n",
    "                        except:\n",
    "                            # 如果无法调整形状，跳过这个梯度\n",
    "                            continue\n",
    "                    next_gradients.append(grad.to(self.device_ids[stage_idx]))\n",
    "\n",
    "        # 反向传播通过当前阶段\n",
    "        for i in range(len(stage_activations) - 1, -1, -1):\n",
    "            if next_gradients and i < len(next_gradients):\n",
    "                stage_activations[i].backward(next_gradients[i], retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e102afd",
   "metadata": {},
   "source": [
    "## 4. 空泡率分析与计算\n",
    "\n",
    "**空泡率**是衡量流水线并行效率的重要指标，表示由于流水线填充和排空造成的计算资源浪费比例。空泡率的计算基于流水线填充和排空的时间开销。当微批次数量远大于流水线阶段数时，空泡率会降低，因为填充和排空时间相对于总计算时间的比例变小。\n",
    "\n",
    "我们在这里以**Gpipe 流水线并行**的空泡率计算为例，计算空泡率。\n",
    "\n",
    "在数学上，空泡率可以表示为：\n",
    "\n",
    "$$\n",
    "Bubble = (T_{fill} + T_{drain}) / (T_{total}) = (S - 1 + S - 1) / (2*(M + S - 1)) = (S - 1) / (M + S - 1)\n",
    "$$\n",
    "\n",
    "其中 $S$ 是流水线阶段数，$M$ 是微批次数量。$T_{fill}$ 表示流水线填充时间，$T_{drain}$ 表示流水线排空时间,$T_{total}$ 表示流水线总时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a4e7e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 不同配置下的空泡率计算结果 ===\n",
      "阶段数:   4, 微批次:   4, 空泡率: 0.429\n",
      "阶段数:   4, 微批次:   8, 空泡率: 0.273\n",
      "阶段数:   4, 微批次:  16, 空泡率: 0.158\n",
      "阶段数:   4, 微批次:  32, 空泡率: 0.086\n",
      "阶段数:   4, 微批次:  64, 空泡率: 0.045\n",
      "阶段数:   4, 微批次: 100, 空泡率: 0.029\n",
      "阶段数:   8, 微批次:  16, 空泡率: 0.304\n",
      "阶段数:  16, 微批次:  32, 空泡率: 0.319\n",
      "阶段数:  32, 微批次:  64, 空泡率: 0.326\n",
      "阶段数:   8, 微批次:  32, 空泡率: 0.179\n",
      "阶段数:  16, 微批次:  64, 空泡率: 0.190\n"
     ]
    }
   ],
   "source": [
    "def calculate_bubble_rate(strategy_name, num_stages, num_microbatches, interleaving_degree=2):\n",
    "    \"\"\"根据策略类型计算正确的空泡率\"\"\"\n",
    "    if num_stages == 1:\n",
    "        return 0.0\n",
    "\n",
    "    if strategy_name == \"Naive\":\n",
    "        # Naive 策略没有流水线并行，空泡率为 0\n",
    "        return 0.0\n",
    "    elif strategy_name == \"GPipe\":\n",
    "        # GPipe 的空泡率公式\n",
    "        return (num_stages - 1) / (num_microbatches + num_stages - 1)\n",
    "    elif strategy_name == \"1F1B\":\n",
    "        # 1F1B 的空泡率公式\n",
    "        return (num_stages - 1) / num_microbatches\n",
    "    elif strategy_name == \"Interleaved 1F1B\":\n",
    "        # Interleaved 1F1B 的空泡率公式\n",
    "        return (num_stages - 1) / (num_microbatches * interleaving_degree)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "configurations = [\n",
    "    # 【对比组 1】固定 S=4，观察 M 增大如何降低空泡率（展示收益递减）\n",
    "    (4, 4),   # M = S，空泡率较高，临界点\n",
    "    (4, 8),   # M = 2S\n",
    "    (4, 16),  # M = 4S（推荐工程起点）\n",
    "    (4, 32),  # M = 8S\n",
    "    (4, 64),  # M = 16S\n",
    "    (4, 100),  # M = 25S，接近理想\n",
    "\n",
    "    # 【对比组 2】固定 M=2S，观察 S 增大时空泡率如何上升（展示规模代价）\n",
    "    (8, 16),  # M = 2S\n",
    "    (16, 32), # M = 2S\n",
    "    (32, 64), # M = 2S（如资源允许）\n",
    "\n",
    "    # 【对比组 3】固定 M=4S，观察不同规模下的表现（推荐工程配置）\n",
    "    (8, 32),  # M = 4S\n",
    "    (16, 64), # M = 4S\n",
    "]\n",
    "\n",
    "print(\"=== 不同配置下的空泡率计算结果 ===\")\n",
    "for num_stages, num_microbatches in configurations:\n",
    "    rate = calculate_bubble_rate(\"GPipe\",num_stages, num_microbatches)\n",
    "    print(f\"阶段数: {num_stages:3d}, 微批次: {num_microbatches:3d}, 空泡率: {rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852011d3",
   "metadata": {},
   "source": [
    "从上面代码的运行结果我们可以看出：\n",
    "- **微批次的影响**：当 $M \\gg S$ 时，空泡率趋近于 0（如 $S=4, M=100$，空泡率≈0.029），因此增加微批次是降低空泡率的核心手段。\n",
    "- **阶段数的影响**：$S$ 越大，空泡率越高（相同 $M$ 下，$S=16$ 比 $S=4$ 空泡率高约 20%），因此 Pipeline 阶段数需与微批次数量匹配（建议 $M \\geq 4S$）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a66a8",
   "metadata": {},
   "source": [
    "## 5. 1F1B 调度策略实现\n",
    "\n",
    "1F1B(One-Forward-One-Backward) 调度是一种优化的流水线并行策略，它通过交替执行前向和反向传播来减少内存使用和空泡时间。\n",
    "\n",
    "![](./images/Code03Pipeline03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54329222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneFOneBPipeline(nn.Module):\n",
    "    def __init__(self, module_list, device_ids, num_microbatches=4):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"模块数量必须与设备数量相同\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_stages = len(device_ids)\n",
    "        self.num_microbatches = num_microbatches\n",
    "\n",
    "        # 将每个阶段移动到对应的设备\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"1F1B 策略: 交替执行前向和反向传播 - 重新实现\"\"\"\n",
    "        # 分割输入为微批次\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        activations = [[] for _ in range(self.num_stages)]\n",
    "        outputs = []\n",
    "\n",
    "        # 1. 前向填充阶段 (Warm-up)\n",
    "        for i in range(self.num_stages):\n",
    "            # 处理前 i+1 个微批次的前 i+1 个阶段\n",
    "            for j in range(i + 1):\n",
    "                if j >= len(micro_batches):\n",
    "                    break\n",
    "\n",
    "                current = micro_batches[j].to(self.device_ids[0])\n",
    "                for stage_idx in range(i + 1):\n",
    "                    if stage_idx >= self.num_stages:\n",
    "                        break\n",
    "\n",
    "                    current = self.stages[stage_idx](current)\n",
    "                    if stage_idx < self.num_stages - 1:\n",
    "                        current_act = current.detach().clone().requires_grad_(True)\n",
    "                        if stage_idx < len(activations):\n",
    "                            activations[stage_idx].append(current_act)\n",
    "                        current = current_act.to(self.device_ids[stage_idx + 1])\n",
    "\n",
    "                if i == self.num_stages - 1:\n",
    "                    outputs.append(current)\n",
    "\n",
    "        # 2. 1F1B 阶段 (Steady state)\n",
    "        for i in range(self.num_stages, self.num_microbatches):\n",
    "            # 前向传播\n",
    "            current = micro_batches[i].to(self.device_ids[0])\n",
    "            for stage_idx in range(self.num_stages):\n",
    "                current = self.stages[stage_idx](current)\n",
    "                if stage_idx < self.num_stages - 1:\n",
    "                    current_act = current.detach().clone().requires_grad_(True)\n",
    "                    activations[stage_idx].append(current_act)\n",
    "                    current = current_act.to(self.device_ids[stage_idx + 1])\n",
    "            outputs.append(current)\n",
    "\n",
    "        # 3. 反向排空阶段 (Cool-down)\n",
    "        for i in range(self.num_microbatches, self.num_microbatches + self.num_stages - 1):\n",
    "            # 这里只需要处理反向传播，前向已经完成\n",
    "            pass\n",
    "\n",
    "        # 确保输出批次大小正确\n",
    "        if outputs:\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "        else:\n",
    "            output = torch.tensor([])\n",
    "\n",
    "        return output, activations\n",
    "\n",
    "    def backward(self, loss, activations):\n",
    "        \"\"\"1F1B 反向传播 - 修复版本\"\"\"\n",
    "        # 计算最终损失梯度\n",
    "        loss.backward()\n",
    "\n",
    "        # 从最后阶段开始反向传播\n",
    "        for stage_idx in range(self.num_stages - 2, -1, -1):\n",
    "            stage_activations = activations[stage_idx]\n",
    "            next_gradients = []\n",
    "\n",
    "            for act in activations[stage_idx + 1]:\n",
    "                if act.grad is not None:\n",
    "                    # 确保梯度形状匹配\n",
    "                    grad = act.grad\n",
    "                    if grad.shape != stage_activations[0].shape:\n",
    "                        try:\n",
    "                            grad = grad.view(stage_activations[0].shape)\n",
    "                        except:\n",
    "                            continue\n",
    "                    next_gradients.append(grad.to(self.device_ids[stage_idx]))\n",
    "\n",
    "            for i in range(len(stage_activations) - 1, -1, -1):\n",
    "                if next_gradients and i < len(next_gradients):\n",
    "                    stage_activations[i].backward(next_gradients[i], retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5bdd3",
   "metadata": {},
   "source": [
    "1F1B 调度的核心思想是在流水线中交替执行前向传播和反向传播，而不是先完成所有前向传播再进行反向传播。这种策略有两个主要优势：\n",
    "\n",
    "1. **减少内存使用**：不需要存储所有微批次的前向传播中间结果\n",
    "2. **降低空泡率**：通过更早开始反向传播，减少设备空闲时间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9052f70",
   "metadata": {},
   "source": [
    "## 6. Interleaved 1F1B 调度策略实现\n",
    "\n",
    "Interleaved 1F1B 调度是一种改进的 1F1B 调度策略，它通过交替执行前向和反向传播，并引入额外的填充和排空步骤来减少空泡率。\n",
    "\n",
    "![](./images/Code03Pipeline04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b95a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterleavedOneFOneBPipeline(nn.Module):\n",
    "    def __init__(self, module_list, device_ids, num_microbatches=4, interleaving_degree=2):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"模块数量必须与设备数量相同\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_stages = len(device_ids)\n",
    "        self.num_microbatches = num_microbatches\n",
    "        self.interleaving_degree = interleaving_degree\n",
    "\n",
    "        # 将每个阶段移动到对应的设备\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Interleaved 1F1B 策略: 改进的 1F1B，更细粒度的流水线\"\"\"\n",
    "        # 分割输入为微批次\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        activations = [[] for _ in range(self.num_stages)]\n",
    "        outputs = []\n",
    "\n",
    "        # 简化的 Interleaved 实现 - 使用分组处理\n",
    "        group_size = self.interleaving_degree\n",
    "\n",
    "        # 处理每个微批次组\n",
    "        for group_start in range(0, self.num_microbatches, group_size):\n",
    "            group_end = min(group_start + group_size, self.num_microbatches)\n",
    "\n",
    "            # 对组内每个微批次进行处理\n",
    "            for i in range(group_start, group_end):\n",
    "                current = micro_batches[i].to(self.device_ids[0])\n",
    "                for stage_idx in range(self.num_stages):\n",
    "                    current = self.stages[stage_idx](current)\n",
    "                    if stage_idx < self.num_stages - 1:\n",
    "                        current_act = current.detach().clone().requires_grad_(True)\n",
    "                        activations[stage_idx].append(current_act)\n",
    "                        current = current_act.to(self.device_ids[stage_idx + 1])\n",
    "                outputs.append(current)\n",
    "\n",
    "        output = torch.cat(outputs, dim=0)\n",
    "        return output, activations\n",
    "\n",
    "    def backward(self, loss, activations):\n",
    "        \"\"\"Interleaved 1F1B 反向传播 - 修复版本\"\"\"\n",
    "        # 计算最终损失梯度\n",
    "        loss.backward()\n",
    "\n",
    "        # 从最后阶段开始反向传播\n",
    "        for stage_idx in range(self.num_stages - 2, -1, -1):\n",
    "            stage_activations = activations[stage_idx]\n",
    "            next_gradients = []\n",
    "\n",
    "            for act in activations[stage_idx + 1]:\n",
    "                if act.grad is not None:\n",
    "                    # 确保梯度形状匹配\n",
    "                    grad = act.grad\n",
    "                    if grad.shape != stage_activations[0].shape:\n",
    "                        try:\n",
    "                            grad = grad.view(stage_activations[0].shape)\n",
    "                        except:\n",
    "                            continue\n",
    "                    next_gradients.append(grad.to(self.device_ids[stage_idx]))\n",
    "\n",
    "            for i in range(len(stage_activations) - 1, -1, -1):\n",
    "                if next_gradients and i < len(next_gradients):\n",
    "                    stage_activations[i].backward(next_gradients[i], retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8384d9",
   "metadata": {},
   "source": [
    "## 7. 混合并行策略\n",
    "\n",
    "混合并行结合了数据并行、流水线并行和张量并行，以充分利用多种并行策略的优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "06093bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可用设备: [0, 1, 2, 3]\n",
      "配置 → 数据并行路数: 2, Pipeline 阶段数: 2\n",
      "\n",
      "=== 混合并行测试结果 ===\n",
      "输入形状: torch.Size([32, 100]), 输出形状: torch.Size([32, 10])\n",
      "并行配置: 数据并行路数=2, Pipeline 阶段数=2\n",
      "Pipeline 阶段 1 用设备: [0, 1]\n",
      "Pipeline 阶段 2 用设备: [2, 3]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 辅助函数：获取可用 GPU 设备（模拟）\n",
    "def get_available_devices(max_devices=4):\n",
    "    devices = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        if len(devices) >= max_devices:\n",
    "            break\n",
    "        devices.append(torch.device(f'cuda:{i}'))\n",
    "    if len(devices) == 0:\n",
    "        devices = [torch.device('cpu')] * min(max_devices, 1)\n",
    "    return devices\n",
    "\n",
    "# 示例模型（复用原结构，确保兼容性）\n",
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 混合并行模型：Pipeline + DataParallel\n",
    "class HybridParallelModel(nn.Module):\n",
    "    def __init__(self, base_model, device_ids, dp_size=2, pp_size=2):\n",
    "        super().__init__()\n",
    "        self.dp_size = dp_size  # 数据并行路数（每个 Pipeline 阶段的复制份数）\n",
    "        self.pp_size = pp_size  # Pipeline 阶段数（模型分割后的段数）\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        # 验证设备数量：总设备数 = 数据并行路数 × Pipeline 阶段数\n",
    "        assert len(device_ids) == dp_size * pp_size, \\\n",
    "            f\"设备数需等于数据并行路数×Pipeline 阶段数（当前：{len(device_ids)} != {dp_size}×{pp_size}）\"\n",
    "\n",
    "        # 1. Pipeline 分割：将基础模型拆分为 pp_size 个阶段\n",
    "        self.pipeline_stages = self._split_model_for_pipeline(base_model, pp_size)\n",
    "\n",
    "        # 2. 数据并行：为每个 Pipeline 阶段创建 dp_size 份副本（使用 nn.DataParallel）\n",
    "        self.parallel_stages = nn.ModuleList()\n",
    "        current_devices = device_ids  # 待分配的设备列表\n",
    "        for stage in self.pipeline_stages:\n",
    "            # 为当前 Pipeline 阶段分配 dp_size 个设备（数据并行）\n",
    "            dp_devices = current_devices[:dp_size]\n",
    "            current_devices = current_devices[dp_size:]  # 剩余设备用于下一阶段\n",
    "\n",
    "            # 🔥 修复关键：将 stage 移动到第一个设备（DataParallel 要求）\n",
    "            stage = stage.to(f'cuda:{dp_devices[0]}')\n",
    "\n",
    "            # 包装为数据并行模块\n",
    "            dp_stage = nn.DataParallel(stage, device_ids=dp_devices)\n",
    "            self.parallel_stages.append(dp_stage)\n",
    "\n",
    "    def _split_model_for_pipeline(self, model, pp_size):\n",
    "        \"\"\"\n",
    "        辅助函数：将 ExampleModel 按 Pipeline 逻辑分割为 pp_size 个阶段\n",
    "        分割规则：根据线性层拆分，确保每个阶段计算量均衡\n",
    "        \"\"\"\n",
    "        stages = []\n",
    "        if pp_size == 2:\n",
    "            # 2 阶段分割：[fc1+relu, fc2+relu+fc3]\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc2, model.relu, model.fc3))\n",
    "        elif pp_size == 3:\n",
    "            # 3 阶段分割：[fc1+relu, fc2+relu, fc3]\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc2, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc3))\n",
    "        else:\n",
    "            # 默认不分割（pp_size=1，仅数据并行）\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu, model.fc2, model.relu, model.fc3))\n",
    "        return stages\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        混合并行前向传播流程：\n",
    "        输入 → Pipeline 阶段 1（数据并行）→ Pipeline 阶段 2（数据并行）→ 输出\n",
    "        \"\"\"\n",
    "        if len(self.parallel_stages) == 0:\n",
    "            return x\n",
    "\n",
    "        # 确保输入在第一个 stage 的第一个设备上\n",
    "        first_device = self.parallel_stages[0].device_ids[0]\n",
    "        current_x = x.to(f'cuda:{first_device}')\n",
    "\n",
    "        for stage in self.parallel_stages:\n",
    "            current_x = stage(current_x)  # 每个阶段内部数据并行计算\n",
    "        return current_x\n",
    "\n",
    "\n",
    "# ========== 主程序：配置与测试 ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 模型参数配置\n",
    "    input_size, hidden_size, output_size = 100, 200, 10\n",
    "    base_model = ExampleModel(input_size, hidden_size, output_size)\n",
    "\n",
    "    # 2. 自动获取设备（模拟）\n",
    "    available_devices = get_available_devices(max_devices=4)\n",
    "    device_ids = [dev.index for dev in available_devices if dev.type == 'cuda']\n",
    "    if len(device_ids) == 0:\n",
    "        print(\"⚠️  未检测到 CUDA 设备，回退到 CPU 模式（不支持 DataParallel）\")\n",
    "        device_ids = [0]  # 模拟 CPU index，但 DataParallel 不支持纯 CPU，需特殊处理\n",
    "        # 为演示，我们强制至少 2 个设备，若无 GPU 则跳过并行\n",
    "        print(\"⚠️  跳过并行测试（无 GPU）\")\n",
    "        exit(0)\n",
    "\n",
    "    # 3. 调整并行配置以匹配设备数\n",
    "    dp_size = 2 if len(device_ids) >= 4 else 1\n",
    "    pp_size = len(device_ids) // dp_size\n",
    "\n",
    "    print(f\"可用设备: {device_ids}\")\n",
    "    print(f\"配置 → 数据并行路数: {dp_size}, Pipeline 阶段数: {pp_size}\")\n",
    "\n",
    "    # 4. 创建混合并行模型\n",
    "    hybrid_model = HybridParallelModel(\n",
    "        base_model,\n",
    "        device_ids=device_ids,\n",
    "        dp_size=dp_size,\n",
    "        pp_size=pp_size\n",
    "    )\n",
    "\n",
    "    # 5. 测试输入与输出\n",
    "    x = torch.randn(32, input_size)  # 输入：批量 32，维度 100\n",
    "    output = hybrid_model(x)\n",
    "\n",
    "    # 6. 打印测试结果\n",
    "    print(f\"\\n=== 混合并行测试结果 ===\")\n",
    "    print(f\"输入形状: {x.shape}, 输出形状: {output.shape}\")\n",
    "    print(f\"并行配置: 数据并行路数={dp_size}, Pipeline 阶段数={pp_size}\")\n",
    "    current_devices = device_ids\n",
    "    for i in range(pp_size):\n",
    "        dp_devices = current_devices[:dp_size]\n",
    "        current_devices = current_devices[dp_size:]\n",
    "        print(f\"Pipeline 阶段 {i+1} 用设备: {dp_devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b3468",
   "metadata": {},
   "source": [
    "## 8. 完整实验与性能分析\n",
    "\n",
    "下面是一个完整的流水线并行实验，包括训练循环和性能分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba526e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage(device_ids):\n",
    "    \"\"\"获取所有 GPU 的显存使用情况\"\"\"\n",
    "    memory_usage = {}\n",
    "    for device in device_ids:\n",
    "        if device.type == 'cuda':\n",
    "            memory_allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)  # 转换为 GB\n",
    "            memory_cached = torch.cuda.memory_reserved(device) / (1024 ** 3)  # 转换为 GB\n",
    "            memory_usage[str(device)] = {\n",
    "                'allocated': memory_allocated,\n",
    "                'cached': memory_cached\n",
    "            }\n",
    "    return memory_usage\n",
    "\n",
    "def track_memory_usage(device_ids, memory_history):\n",
    "    \"\"\"跟踪显存使用情况并记录到历史\"\"\"\n",
    "    current_memory = get_gpu_memory_usage(device_ids)\n",
    "    memory_history.append(current_memory)\n",
    "    return memory_history\n",
    "\n",
    "def calculate_avg_memory_usage(memory_history):\n",
    "    \"\"\"计算平均显存使用量\"\"\"\n",
    "    if not memory_history:\n",
    "        return 0.0\n",
    "\n",
    "    total_allocated = 0.0\n",
    "    total_cached = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for memory_snapshot in memory_history:\n",
    "        for device, usage in memory_snapshot.items():\n",
    "            total_allocated += usage['allocated']\n",
    "            total_cached += usage['cached']\n",
    "            count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    return total_allocated / count, total_cached / count\n",
    "\n",
    "# 修改实验运行函数\n",
    "def run_pipeline_experiment(pipeline_class, strategy_name, num_epochs=50, batch_size=256, num_microbatches=32):\n",
    "    \"\"\"运行指定流水线策略的实验 - 添加显存跟踪\"\"\"\n",
    "    # 1. 自动获取设备与配置\n",
    "    device_ids = get_available_devices(max_devices=4)\n",
    "    num_stages = len(device_ids)\n",
    "    input_size, output_size = 100, 10\n",
    "\n",
    "    # 清空显存缓存\n",
    "    for device in device_ids:\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 2. 构建 Pipeline 模型\n",
    "    model_parts = create_model_parts(input_size=input_size, output_size=output_size)\n",
    "    model_parts = model_parts[:num_stages]\n",
    "\n",
    "    # 根据策略名称选择不同的初始化参数\n",
    "    if strategy_name == \"Naive\":\n",
    "        pipeline_model = pipeline_class(model_parts, device_ids)\n",
    "    elif strategy_name == \"GPipe\":\n",
    "        pipeline_model = pipeline_class(model_parts, device_ids, num_microbatches=num_microbatches)\n",
    "    elif strategy_name == \"1F1B\":\n",
    "        pipeline_model = pipeline_class(model_parts, device_ids, num_microbatches=num_microbatches)\n",
    "    elif strategy_name == \"Interleaved 1F1B\":\n",
    "        pipeline_model = pipeline_class(model_parts, device_ids, num_microbatches=num_microbatches, interleaving_degree=2)\n",
    "    else:\n",
    "        raise ValueError(f\"未知策略: {strategy_name}\")\n",
    "\n",
    "    # 3. 优化器与训练配置\n",
    "    optimizer = torch.optim.Adam(pipeline_model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    losses = []\n",
    "    times = []\n",
    "    memory_history = []  # 存储显存使用历史\n",
    "\n",
    "    # 4. 训练循环\n",
    "    print(f\"\\n=== 开始 {strategy_name} Pipeline 训练（共{num_epochs}轮）===\")\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 记录训练前的显存使用\n",
    "        memory_history = track_memory_usage(device_ids, memory_history)\n",
    "\n",
    "        # 模拟训练数据\n",
    "        x = torch.randn(batch_size, input_size)\n",
    "        y = torch.randint(0, output_size, (batch_size,))\n",
    "\n",
    "        # 前向传播\n",
    "        outputs, activations = pipeline_model(x)\n",
    "\n",
    "        # 处理输出批次大小不匹配的问题\n",
    "        if outputs.shape[0] != batch_size:\n",
    "            y_adjusted = y[:outputs.shape[0]].to(device_ids[-1])\n",
    "        else:\n",
    "            y_adjusted = y.to(device_ids[-1])\n",
    "\n",
    "        loss = F.cross_entropy(outputs, y_adjusted)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        if hasattr(pipeline_model, 'backward'):\n",
    "            pipeline_model.backward(loss, activations)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(pipeline_model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        losses.append(loss.item())\n",
    "        times.append(epoch_time)\n",
    "\n",
    "        # 记录训练后的显存使用\n",
    "        memory_history = track_memory_usage(device_ids, memory_history)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            # 计算当前平均显存使用\n",
    "            avg_allocated, avg_cached = calculate_avg_memory_usage(memory_history)\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs}, 损失: {loss.item():.4f}, 时间: {epoch_time:.4f}s, \"\n",
    "                  f\"显存: {avg_allocated:.2f}GB/{avg_cached:.2f}GB, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    # 5. 性能分析\n",
    "    bubble_rate = calculate_bubble_rate(strategy_name, num_stages, num_microbatches)\n",
    "    avg_time = sum(times) / len(times)\n",
    "    avg_allocated, avg_cached = calculate_avg_memory_usage(memory_history)\n",
    "\n",
    "    print(f\"\\n=== {strategy_name} 实验结果 ===\")\n",
    "    print(f\"设备配置: {[str(dev) for dev in device_ids]}\")\n",
    "    print(f\"流水线阶段: {num_stages}, 微批次: {num_microbatches}\")\n",
    "    print(f\"空泡率: {bubble_rate:.3f} ({bubble_rate*100:.1f}%)\")\n",
    "    print(f\"平均每轮时间: {avg_time:.4f}s\")\n",
    "    print(f\"平均显存使用: {avg_allocated:.2f}GB (分配) / {avg_cached:.2f}GB (缓存)\")\n",
    "    print(f\"最终损失: {losses[-1]:.4f}\")\n",
    "\n",
    "    # 收敛判断\n",
    "    if losses[-1] < 1.0 and losses[-1] < losses[0]:\n",
    "        print(\"训练结论: 成功收敛\")\n",
    "    elif losses[-1] < losses[0]:\n",
    "        print(\"训练结论: 部分收敛\")\n",
    "    else:\n",
    "        print(\"训练结论: 可能未收敛\")\n",
    "\n",
    "    return losses, bubble_rate, avg_time, avg_allocated, avg_cached\n",
    "\n",
    "# 更新结果展示函数\n",
    "def print_results_table(results):\n",
    "    \"\"\"打印结果表格 - 添加显存使用列\"\"\"\n",
    "    if not results:\n",
    "        print(\"没有成功运行的策略\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== 所有策略综合比较 ===\")\n",
    "    # 表头\n",
    "    print(f\"+{'-'*20}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+\")\n",
    "    print(f\"| {'策略名称':<18} | {'平均时间':<10} | {'最终损失':<10} | {'空泡率':<10} | {'显存(GB)':<10} | {'缓存(GB)':<10} |\")\n",
    "    print(f\"+{'-'*20}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+\")\n",
    "\n",
    "    # 获取 Naive 策略的结果作为基准\n",
    "    naive_time = results[\"Naive\"][\"avg_time\"] if \"Naive\" in results else 1.0\n",
    "    num_devices = len(get_available_devices(max_devices=4))\n",
    "\n",
    "    # 数据行\n",
    "    for strategy, data in results.items():\n",
    "        # speedup = calculate_speedup(naive_time, data[\"avg_time\"])\n",
    "        # efficiency = calculate_efficiency(speedup, num_devices)\n",
    "        print(f\"| {strategy:<18} | {data['avg_time']:>10.4f}s | {data['losses'][-1]:>10.4f} | \"\n",
    "              f\"{data['bubble_rate']:>10.3f} | \"\n",
    "              f\"{data['avg_allocated']:>10.2f} | {data['avg_cached']:>10.2f} |\")\n",
    "\n",
    "    print(f\"+{'-'*20}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "正在运行 Naive 策略...\n",
      "============================================================\n",
      "\n",
      "=== 开始 Naive Pipeline 训练（共 50 轮）===\n",
      "Epoch  10/50, 损失: 2.3016, 时间: 0.0090s, 显存: 0.04GB/0.08GB, LR: 0.001000\n",
      "Epoch  20/50, 损失: 2.3015, 时间: 0.0084s, 显存: 0.04GB/0.08GB, LR: 0.000500\n",
      "Epoch  30/50, 损失: 2.3061, 时间: 0.0083s, 显存: 0.04GB/0.08GB, LR: 0.000500\n",
      "Epoch  40/50, 损失: 2.3025, 时间: 0.0080s, 显存: 0.04GB/0.08GB, LR: 0.000250\n",
      "Epoch  50/50, 损失: 2.3019, 时间: 0.0078s, 显存: 0.04GB/0.08GB, LR: 0.000250\n",
      "\n",
      "=== Naive 实验结果 ===\n",
      "设备配置: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n",
      "流水线阶段: 4, 微批次: 32\n",
      "空泡率: 0.000 (0.0%)\n",
      "平均每轮时间: 0.0088s\n",
      "平均显存使用: 0.04GB (分配) / 0.08GB (缓存)\n",
      "最终损失: 2.3019\n",
      "训练结论: 部分收敛\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "正在运行 GPipe 策略...\n",
      "============================================================\n",
      "\n",
      "=== 开始 GPipe Pipeline 训练（共 50 轮）===\n",
      "Epoch  10/50, 损失: 2.3045, 时间: 0.0510s, 显存: 0.01GB/0.03GB, LR: 0.001000\n",
      "Epoch  20/50, 损失: 2.3078, 时间: 0.0513s, 显存: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  30/50, 损失: 2.3016, 时间: 0.0511s, 显存: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  40/50, 损失: 2.3064, 时间: 0.0512s, 显存: 0.01GB/0.03GB, LR: 0.000250\n",
      "Epoch  50/50, 损失: 2.3032, 时间: 0.0515s, 显存: 0.01GB/0.03GB, LR: 0.000250\n",
      "\n",
      "=== GPipe 实验结果 ===\n",
      "设备配置: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n",
      "流水线阶段: 4, 微批次: 32\n",
      "空泡率: 0.086 (8.6%)\n",
      "平均每轮时间: 0.0514s\n",
      "平均显存使用: 0.01GB (分配) / 0.03GB (缓存)\n",
      "最终损失: 2.3032\n",
      "训练结论: 部分收敛\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "正在运行 1F1B 策略...\n",
      "============================================================\n",
      "\n",
      "=== 开始 1F1B Pipeline 训练（共 50 轮）===\n",
      "Epoch  10/50, 损失: 2.3094, 时间: 0.0570s, 显存: 0.01GB/0.03GB, LR: 0.001000\n",
      "Epoch  20/50, 损失: 2.3015, 时间: 0.0568s, 显存: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  30/50, 损失: 2.3067, 时间: 0.0567s, 显存: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  40/50, 损失: 2.3056, 时间: 0.0572s, 显存: 0.01GB/0.03GB, LR: 0.000250\n",
      "Epoch  50/50, 损失: 2.3039, 时间: 0.0569s, 显存: 0.01GB/0.03GB, LR: 0.000250\n",
      "\n",
      "=== 1F1B 实验结果 ===\n",
      "设备配置: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n",
      "流水线阶段: 4, 微批次: 32\n",
      "空泡率: 0.094 (9.4%)\n",
      "平均每轮时间: 0.0572s\n",
      "平均显存使用: 0.01GB (分配) / 0.03GB (缓存)\n",
      "最终损失: 2.3039\n",
      "训练结论: 可能未收敛\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "正在运行 Interleaved 1F1B 策略...\n",
      "============================================================\n",
      "\n",
      "=== 开始 Interleaved 1F1B Pipeline 训练（共 50 轮）===\n",
      "Epoch  10/50, 损失: 2.3026, 时间: 0.0515s, 显存: 0.01GB/0.03GB, LR: 0.001000\n",
      "Epoch  20/50, 损失: 2.2959, 时间: 0.0517s, 显存: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  30/50, 损失: 2.3065, 时间: 0.0519s, 显存: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  40/50, 损失: 2.3047, 时间: 0.0519s, 显存: 0.01GB/0.03GB, LR: 0.000250\n",
      "Epoch  50/50, 损失: 2.3014, 时间: 0.0516s, 显存: 0.01GB/0.03GB, LR: 0.000250\n",
      "\n",
      "=== Interleaved 1F1B 实验结果 ===\n",
      "设备配置: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n",
      "流水线阶段: 4, 微批次: 32\n",
      "空泡率: 0.047 (4.7%)\n",
      "平均每轮时间: 0.0521s\n",
      "平均显存使用: 0.01GB (分配) / 0.03GB (缓存)\n",
      "最终损失: 2.3014\n",
      "训练结论: 部分收敛\n",
      "============================================================\n",
      "\n",
      "\n",
      "=== 所有策略综合比较 ===\n",
      "+--------------------+------------+------------+------------+------------+------------+\n",
      "| 策略名称               | 平均时间       | 最终损失       | 空泡率        | 显存(GB)     | 缓存(GB)     |\n",
      "+--------------------+------------+------------+------------+------------+------------+\n",
      "| Naive              |     0.0088s |     2.3019 |      0.000 |       0.04 |       0.08 |\n",
      "| GPipe              |     0.0514s |     2.3032 |      0.086 |       0.01 |       0.03 |\n",
      "| 1F1B               |     0.0572s |     2.3039 |      0.094 |       0.01 |       0.03 |\n",
      "| Interleaved 1F1B   |     0.0521s |     2.3014 |      0.047 |       0.01 |       0.03 |\n",
      "+--------------------+------------+------------+------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# 策略类映射\n",
    "strategy_classes = {\n",
    "    \"Naive\": NaivePipelineParallel,\n",
    "    \"GPipe\": GPipeParallel,\n",
    "    \"1F1B\": OneFOneBPipeline,\n",
    "    \"Interleaved 1F1B\": InterleavedOneFOneBPipeline\n",
    "}\n",
    "\n",
    "# 运行所有四种流水线策略\n",
    "results = {}\n",
    "\n",
    "for strategy_name, strategy_class in strategy_classes.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"正在运行 {strategy_name} 策略...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    try:\n",
    "        losses, bubble_rate, avg_time, avg_allocated, avg_cached = run_pipeline_experiment(\n",
    "            strategy_class,\n",
    "            strategy_name,\n",
    "            num_epochs=50,\n",
    "            batch_size=256,\n",
    "            num_microbatches=32\n",
    "        )\n",
    "        results[strategy_name] = {\n",
    "            \"losses\": losses,\n",
    "            \"bubble_rate\": bubble_rate,\n",
    "            \"avg_time\": avg_time,\n",
    "            \"avg_allocated\": avg_allocated,\n",
    "            \"avg_cached\": avg_cached\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"策略 {strategy_name} 执行失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 打印综合比较结果\n",
    "print_results_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69de3df",
   "metadata": {},
   "source": [
    "这个完整实验展示了流水线并行的实际应用，包括模型分割、训练循环和空泡率分析。在实际应用中，还需要考虑梯度同步、设备间通信优化等复杂问题。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b27fb7",
   "metadata": {},
   "source": [
    "## 总结与思考\n",
    "\n",
    "通过补充 Interleaved 1F1B 实现，我们完成了 Pipeline 并行三大核心调度策略的覆盖：\n",
    "\n",
    "1. **Gpipe (Native PP)**：简单直观，空泡率高，显存占用大。\n",
    "\n",
    "2. **1F1B**：通过前向/反向交替，降低显存占用，压缩部分空泡。\n",
    "\n",
    "3. **Interleaved 1F1B**：引入虚拟阶段，在同一设备上交织执行多个微批次，进一步压缩空泡，尤其适合大微批次场景。\n",
    "\n",
    "工程建议：\n",
    "\n",
    "- 微批次数量 M 应远大于阶段数 S（推荐 M >= 4S）。\n",
    "- Interleaved 1F1B 在 M >> S 时优势明显，但实现复杂度高。\n",
    "- 混合并行（DP+PP+TP）是大模型训练标配，需配合梯度检查点、通信优化等技术.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
