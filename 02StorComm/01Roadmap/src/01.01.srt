1
00:00:03,066 --> 00:00:03,866
hello 大家好

2
00:00:03,866 --> 00:00:05,633
我是那个动不动就嘴馋

3
00:00:05,633 --> 00:00:07,433
一上班就想睡的 ZOMI

4
00:00:10,066 --> 00:00:11,866
今天呢来到了一个新的内容

5
00:00:11,866 --> 00:00:15,266
看一下整个 AI 集群的网络的基本知识

6
00:00:15,266 --> 00:00:17,700
现在已经来到了网络这一部分

7
00:00:17,700 --> 00:00:19,833
那在整一个 AI Infra

8
00:00:19,833 --> 00:00:21,200
全栈的架构当中

9
00:00:21,200 --> 00:00:22,933
现在来到了一个新的内容

10
00:00:22,966 --> 00:00:24,400
看一下整个 AI 集群

11
00:00:24,400 --> 00:00:26,500
跟网络之间的一个互联

12
00:00:26,500 --> 00:00:28,200
相关的内容

13
00:00:28,600 --> 00:00:30,233
那在整体的知识栈

14
00:00:30,233 --> 00:00:33,733
是在通信跟存储这么一个部分

15
00:00:33,833 --> 00:00:35,900
今天呢或者这一期视频

16
00:00:35,900 --> 00:00:37,433
主要是分享 L2

17
00:00:37,433 --> 00:00:38,600
算力底座

18
00:00:38,600 --> 00:00:41,200
里面的网络所相关的部分

19
00:00:41,200 --> 00:00:43,066
那这里面你会发现整体

20
00:00:43,066 --> 00:00:47,000
会围绕着这个图进行详细的展开

21
00:00:47,000 --> 00:00:49,866
里面的每一个技术的细节点

22
00:00:50,100 --> 00:00:50,533
那首先

23
00:00:50,533 --> 00:00:52,533
每一期视频还是来到一个惯例

24
00:00:52,533 --> 00:00:53,900
看一下这一期视频

25
00:00:53,900 --> 00:00:55,833
想要解决哪些问题

26
00:00:56,033 --> 00:00:58,100
首先呢印度阿三也会过来问

27
00:00:58,100 --> 00:01:00,333
万卡集群感觉很难

28
00:01:00,333 --> 00:01:02,500
但不就是把 GPU 跟 NPU 都放在一起

29
00:01:02,500 --> 00:01:04,333
堆掉吗对

30
00:01:04,400 --> 00:01:06,666
万卡集群确实看上去很难

31
00:01:06,666 --> 00:01:08,233
但看上去也很简单

32
00:01:08,300 --> 00:01:10,733
但是啊要把这么多 GPU 跟 NPU

33
00:01:10,733 --> 00:01:11,533
堆在一起

34
00:01:11,533 --> 00:01:13,866
这个时候呢就离不开互联

35
00:01:13,866 --> 00:01:15,066
但是你想过没有

36
00:01:15,066 --> 00:01:16,300
互联里面

37
00:01:16,300 --> 00:01:17,733
有很多的知识点

38
00:01:17,733 --> 00:01:18,233
例如

39
00:01:18,233 --> 00:01:20,466
单层跟多层的组网到底是怎么样

40
00:01:20,466 --> 00:01:23,300
带宽收敛比应该取多少的值

41
00:01:23,300 --> 00:01:25,900
网络的拓扑应该用什么架构

42
00:01:25,900 --> 00:01:29,100
还有多平面的网络到底有多少个平面

43
00:01:29,100 --> 00:01:31,933
对应的交换机的交换槽位和端口

44
00:01:31,933 --> 00:01:33,633
那到底使用铜互联还是光互联

45
00:01:33,633 --> 00:01:35,333
单轨还是多轨通信方

46
00:01:35,333 --> 00:01:37,333
式的还是合适的交换机

47
00:01:37,333 --> 00:01:38,333
到底有什么区别

48
00:01:38,333 --> 00:01:40,066
今天的这期视频

49
00:01:40,066 --> 00:01:42,666
先跟大家简单的过一过

50
00:01:42,666 --> 00:01:44,733
网络的基本知识

51
00:01:44,733 --> 00:01:47,266
去了解整个 AI 集群

52
00:01:47,266 --> 00:01:49,433
相关的前置的知识

53
00:01:49,433 --> 00:01:50,900
那在这一期视频里面

54
00:01:50,900 --> 00:01:52,666
会跟大家一起分享

55
00:01:52,700 --> 00:01:55,433
整个 AI 集群的组网之路

56
00:01:55,433 --> 00:01:57,266
分开可能五六个视频

57
00:01:57,266 --> 00:01:58,666
跟大家一起去分享

58
00:01:58,666 --> 00:01:59,500
那今天

59
00:01:59,500 --> 00:02:01,433
就是在第一个网络的基本知识

60
00:02:01,433 --> 00:02:03,100
接下来将会去看一下

61
00:02:03,100 --> 00:02:05,733
HPC 的一个超算的洞察

62
00:02:05,733 --> 00:02:06,833
了解完超算之后

63
00:02:06,833 --> 00:02:09,066
看一下 AI 的集群的洞察

64
00:02:09,066 --> 00:02:10,666
包括国内外

65
00:02:10,666 --> 00:02:11,300
那第四个

66
00:02:11,300 --> 00:02:14,033
就正式的来到了最核心的内容

67
00:02:14,033 --> 00:02:17,300
看一下整个 AI 进行组网的基本的原理

68
00:02:17,300 --> 00:02:18,500
需求原则

69
00:02:18,500 --> 00:02:19,666
相关的挑战

70
00:02:19,666 --> 00:02:20,233
第五个

71
00:02:20,233 --> 00:02:23,233
就是正式的去看一下大家比较关心

72
00:02:23,233 --> 00:02:25,833
整个网络平面是怎么去划分

73
00:02:25,833 --> 00:02:27,633
那了解完所有的大概的知识

74
00:02:27,633 --> 00:02:28,666
知识之后

75
00:02:28,700 --> 00:02:31,933
来一个非常详细的总结跟思考

76
00:02:31,933 --> 00:02:34,033
那这一期或者这一系列的视频

77
00:02:34,033 --> 00:02:35,333
主要是围绕这些内容

78
00:02:35,333 --> 00:02:37,066
跟大家一起去分享

79
00:02:37,233 --> 00:02:39,000
在分享这期视频之前

80
00:02:39,000 --> 00:02:40,266
其实 ZOMI 去找

81
00:02:40,266 --> 00:02:40,900
在谷歌

82
00:02:40,900 --> 00:02:42,666
去搜了一下 GPU AI 集群

83
00:02:42,666 --> 00:02:44,433
还有一些通信相关内容

84
00:02:44,433 --> 00:02:46,033
发现大部分的内容

85
00:02:46,033 --> 00:02:47,300
或者有 1/3 的内容

86
00:02:47,300 --> 00:02:48,400
都是 ZOMI 贡献

87
00:02:48,400 --> 00:02:49,500
所以说这一期视频

88
00:02:49,500 --> 00:02:51,466
太少参考资料

89
00:02:51,466 --> 00:02:54,500
希望大家多多鼓励和多多转发

90
00:02:54,700 --> 00:02:55,333
今天的视频

91
00:02:55,333 --> 00:02:56,400
主要是分开

92
00:02:56,400 --> 00:02:59,000
scale up 跟 scale out 去展开

93
00:02:59,000 --> 00:03:01,233
不管现在是做超节点

94
00:03:01,233 --> 00:03:02,500
scale up 也好

95
00:03:02,500 --> 00:03:05,233
还是做万卡集群的 scale out 也好

96
00:03:05,233 --> 00:03:06,666
都离不开

97
00:03:06,666 --> 00:03:09,100
扩展那既然要扩展

98
00:03:09,100 --> 00:03:10,033
单芯片

99
00:03:10,033 --> 00:03:12,233
或者单芯粒的能力是有限

100
00:03:12,233 --> 00:03:14,066
于是呢需要进行组网

101
00:03:14,066 --> 00:03:14,433
因此

102
00:03:14,433 --> 00:03:17,300
今天会围绕着这一系列的内容

103
00:03:17,300 --> 00:03:20,833
去跟大家每一个知识点呢进行展开

104
00:03:20,833 --> 00:03:22,633
将会在总结这个环节

105
00:03:22,633 --> 00:03:25,200
再回到这个内容啊或者这个表

106
00:03:25,200 --> 00:03:27,233
跟大家娓娓道来

107
00:03:27,266 --> 00:03:29,666
呢首先呢来到整体的目录

108
00:03:29,666 --> 00:03:32,300
可能会分开四个内容跟大家去分享

109
00:03:32,300 --> 00:03:32,633
第一个

110
00:03:32,633 --> 00:03:35,266
就是看一下网络架构的基础

111
00:03:35,266 --> 00:03:36,666
什么是网络架构

112
00:03:36,666 --> 00:03:36,900
接着

113
00:03:36,900 --> 00:03:40,633
看一下网络的效率的关键的点

114
00:03:40,833 --> 00:03:42,066
既然讲到网络效率

115
00:03:42,066 --> 00:03:43,200
肯定是跟带宽

116
00:03:43,200 --> 00:03:44,866
跟流量比较强相关

117
00:03:44,866 --> 00:03:47,666
然后看一下网络的硬件的基础

118
00:03:47,666 --> 00:03:48,133
那这个

119
00:03:48,133 --> 00:03:50,666
比较像交通的基础设施

120
00:03:50,833 --> 00:03:52,533
一条交通的时候

121
00:03:52,533 --> 00:03:54,800
应该马路铺什么路面

122
00:03:54,800 --> 00:03:56,300
应该用什么马路

123
00:03:56,300 --> 00:03:56,833
那最后

124
00:03:56,833 --> 00:03:58,433
看一下通信的机制

125
00:03:58,433 --> 00:04:01,066
也就是数据的传输的方式

126
00:04:01,066 --> 00:04:02,466
既然跟数据的传输方式

127
00:04:02,466 --> 00:04:04,000
就会跟马路的交规

128
00:04:04,000 --> 00:04:06,500
或者道路的标准呢比较强相关

129
00:04:06,500 --> 00:04:07,133
所以说

130
00:04:07,133 --> 00:04:09,800
大家如果不理解网络相关内容

131
00:04:09,800 --> 00:04:11,533
你就把它看成一个城市的规划

132
00:04:11,533 --> 00:04:14,433
里面的交通相关的系统就好

133
00:04:15,133 --> 00:04:16,066
因为这期视频

134
00:04:16,066 --> 00:04:17,033
可能会有点长

135
00:04:17,033 --> 00:04:17,933
知识点有点多

136
00:04:17,933 --> 00:04:18,466
那后面

137
00:04:18,466 --> 00:04:20,133
会拆分成两个视频

138
00:04:20,133 --> 00:04:21,466
跟大家去分享

139
00:04:21,466 --> 00:04:22,700
就尽可能一个视频

140
00:04:22,700 --> 00:04:24,833
保持在十几分钟以内

141
00:04:24,900 --> 00:04:25,700
首先第一个内容

142
00:04:25,700 --> 00:04:28,300
就是看一下网络架构的基础

143
00:04:28,400 --> 00:04:29,666
那所谓的网络架构基础

144
00:04:29,666 --> 00:04:32,733
其实比较像整个城市交通

145
00:04:32,733 --> 00:04:33,400
不管怎么样

146
00:04:33,400 --> 00:04:34,200
你听不懂没关系

147
00:04:34,200 --> 00:04:35,633
现在呢马上打开第一个

148
00:04:35,633 --> 00:04:37,233
内容就是这些内容

149
00:04:37,233 --> 00:04:39,066
可能你记就行

150
00:04:39,066 --> 00:04:39,666
后面

151
00:04:39,666 --> 00:04:41,533
会在分享或者洞察相关内容

152
00:04:41,600 --> 00:04:43,066
大量的引用这里面的知识

153
00:04:43,066 --> 00:04:45,400
所以大家可以穿插着来看

154
00:04:45,400 --> 00:04:45,866
那首先

155
00:04:45,866 --> 00:04:48,033
会跟大家一起去分享一下

156
00:04:48,033 --> 00:04:50,066
网络的层级的结构

157
00:04:50,066 --> 00:04:52,466
那比较明显的就是你既然有层级嘛

158
00:04:52,466 --> 00:04:53,400
那我可以有单层

159
00:04:53,400 --> 00:04:54,933
也可以有多层的网络

160
00:04:54,933 --> 00:04:55,866
组网方式

161
00:04:55,866 --> 00:04:56,900
那现在

162
00:04:57,100 --> 00:04:57,933
主要分两个

163
00:04:57,933 --> 00:04:59,433
第一个呢是单层的组网

164
00:04:59,433 --> 00:05:01,000
第二个呢就是多层的组网

165
00:05:01,000 --> 00:05:01,933
那单层的组网

166
00:05:01,933 --> 00:05:02,900
就所有

167
00:05:02,900 --> 00:05:03,933
就像地铁一样

168
00:05:03,933 --> 00:05:06,300
所有的站点呢互相的连接

169
00:05:06,300 --> 00:05:08,266
这个时候呢像英伟达

170
00:05:08,266 --> 00:05:10,300
就像 NV link 做一个全互联

171
00:05:10,300 --> 00:05:11,600
就 fullmesh 的方式

172
00:05:11,633 --> 00:05:12,933
这种呢就最适合

173
00:05:12,933 --> 00:05:16,866
就是 scale up 单节点内多 GPU 的通信

174
00:05:16,866 --> 00:05:18,233
那举个具体的例子

175
00:05:18,233 --> 00:05:19,900
就是现在呢就这种

176
00:05:19,900 --> 00:05:21,466
就是 scale up 单层

177
00:05:21,466 --> 00:05:24,400
GPU 之间呢做一个全互联的方式

178
00:05:24,400 --> 00:05:27,033
那这种就是所谓的单层的网络

179
00:05:27,033 --> 00:05:28,900
单层的架构

180
00:05:29,366 --> 00:05:30,300
那了解完单层之后

181
00:05:30,300 --> 00:05:31,500
看一下多层

182
00:05:31,500 --> 00:05:32,066
所谓的多层

183
00:05:32,066 --> 00:05:33,666
刚才其实只是单层

184
00:05:33,666 --> 00:05:36,100
那多层呢就有点像整个城市

185
00:05:36,100 --> 00:05:37,900
大家去幻想一下重庆

186
00:05:37,900 --> 00:05:38,600
它有地铁

187
00:05:38,600 --> 00:05:39,733
其实也叫轻轨

188
00:05:39,733 --> 00:05:41,000
有公交高架

189
00:05:41,000 --> 00:05:42,833
整体的立体的交换系统

190
00:05:42,833 --> 00:05:44,333
那在整个网络里面

191
00:05:44,333 --> 00:05:45,400
就分为核心层

192
00:05:45,400 --> 00:05:46,800
汇聚层和接入层

193
00:05:46,800 --> 00:05:48,000
特别适合

194
00:05:48,000 --> 00:05:51,466
scale out 就对应的万卡集群这种方式

195
00:05:51,466 --> 00:05:53,033
那所谓的多层组网

196
00:05:53,033 --> 00:05:55,400
现在呀基本上整个万卡集群

197
00:05:55,400 --> 00:05:56,300
或者 AI 集群

198
00:05:56,300 --> 00:05:57,900
都会用多层组网

199
00:05:57,900 --> 00:05:59,100
最简线一种架构

200
00:05:59,100 --> 00:06:01,800
就是用 spine leaf 这种多层

201
00:06:01,800 --> 00:06:03,000
胖树架构

202
00:06:03,000 --> 00:06:03,533
那 leaf

203
00:06:03,533 --> 00:06:05,800
就是对应的层级的交换机

204
00:06:05,800 --> 00:06:07,200
那 spine 层的交换机

205
00:06:07,200 --> 00:06:09,433
就是核心的交换机

206
00:06:09,433 --> 00:06:12,833
那所谓的 leaf 呢就是直连对应的 GPU

207
00:06:12,833 --> 00:06:16,000
那 spine 呢就是高速互联到 leaf 交换机

208
00:06:16,000 --> 00:06:16,833
那现在

209
00:06:16,833 --> 00:06:19,300
来看一下所谓的这个图案

210
00:06:19,300 --> 00:06:19,900
那可以看到

211
00:06:19,900 --> 00:06:22,500
下面就是一个计算节点

212
00:06:22,500 --> 00:06:24,533
那一个节点里面呢有八张卡

213
00:06:24,533 --> 00:06:26,666
所以 ZOMI 就画了八个卡

214
00:06:26,666 --> 00:06:30,633
12345678 这是一个节点

215
00:06:30,633 --> 00:06:32,933
那你会发现这里面有很多个节点

216
00:06:32,933 --> 00:06:34,733
但是呢节点跟节点之间

217
00:06:34,733 --> 00:06:36,000
要进行交互的时候

218
00:06:36,000 --> 00:06:39,133
就会通过 leaf 交换机进行交换

219
00:06:39,133 --> 00:06:40,133
那 leaf 交换机

220
00:06:40,133 --> 00:06:41,700
里面后面的网口

221
00:06:41,700 --> 00:06:43,933
或者所谓的端口呢是有限

222
00:06:43,933 --> 00:06:47,433
所以它只能连 n 台计算节点

223
00:06:47,433 --> 00:06:48,866
那另外 n 台

224
00:06:48,866 --> 00:06:52,266
跟 2N 台啊或者 DK 台进行交互的时候

225
00:06:52,266 --> 00:06:53,800
那有可能涉及到 leaf 交换机

226
00:06:53,800 --> 00:06:55,500
跟 leaf 交换机进行交互

227
00:06:55,500 --> 00:06:56,300
这个时候

228
00:06:56,300 --> 00:06:58,600
就需要用到多层的网络

229
00:06:58,600 --> 00:07:01,866
到上一层的 spine 核心的交换机

230
00:07:01,866 --> 00:07:03,333
进行一个数据的交换

231
00:07:03,333 --> 00:07:05,833
所以 n 到 k 个节点里面

232
00:07:05,833 --> 00:07:07,433
有可能互相传数据的时候

233
00:07:07,433 --> 00:07:10,300
就需要进行多层的交互

234
00:07:10,300 --> 00:07:10,700
因此

235
00:07:10,700 --> 00:07:12,500
这里面呢简单跟大家看一下

236
00:07:12,500 --> 00:07:14,400
网络的层级结构分两种

237
00:07:14,400 --> 00:07:15,633
第一种呢就是单层的组网

238
00:07:15,633 --> 00:07:17,433
还有多层的组网

239
00:07:18,033 --> 00:07:18,933
来到了这个内容

240
00:07:18,933 --> 00:07:21,533
看一下网络的效率的关键的指标

241
00:07:21,533 --> 00:07:22,233
有两个

242
00:07:22,233 --> 00:07:22,600
第一个

243
00:07:22,600 --> 00:07:25,300
就是带宽跟对应的流量

244
00:07:25,300 --> 00:07:26,233
那带宽跟流量

245
00:07:26,233 --> 00:07:29,933
会很大程度去决定网络的效率的问题

246
00:07:29,933 --> 00:07:30,266
首先

247
00:07:30,266 --> 00:07:32,100
来看一下一个比较重要的指标

248
00:07:32,100 --> 00:07:33,800
就是带宽的收敛比

249
00:07:33,800 --> 00:07:34,933
在后面的内容里面

250
00:07:34,933 --> 00:07:38,866
经常会谈到 1:1 的无收敛的网络

251
00:07:38,866 --> 00:07:40,400
还有一个 n 比 1

252
00:07:40,400 --> 00:07:41,800
就 3:1 啊 15:1

253
00:07:41,800 --> 00:07:44,200
7:1 各种各样的收敛的网络

254
00:07:44,200 --> 00:07:44,833
那这里面

255
00:07:44,833 --> 00:07:46,433
所谓的收敛比

256
00:07:46,433 --> 00:07:48,033
主要是去用于衡量

257
00:07:48,033 --> 00:07:49,033
接入层的设备

258
00:07:49,033 --> 00:07:50,900
下行的总带宽

259
00:07:50,900 --> 00:07:53,000
也是下行的总带宽

260
00:07:53,033 --> 00:07:56,300
跟上行总带宽的一个比例

261
00:07:56,300 --> 00:07:57,500
那这里面怎么去理解

262
00:07:57,500 --> 00:07:59,733
简单的打开这个图

263
00:07:59,733 --> 00:08:01,000
那所谓的收敛比

264
00:08:01,000 --> 00:08:02,900
更多的是指接入层

265
00:08:02,900 --> 00:08:04,700
那这一层呢就是接入层

266
00:08:04,733 --> 00:08:06,066
接入到设备里面

267
00:08:06,066 --> 00:08:08,866
或者对应的 GPU 的服务器

268
00:08:09,000 --> 00:08:10,800
那这里面的下行的总带宽

269
00:08:10,800 --> 00:08:13,233
就是指一个交换机接入层

270
00:08:13,233 --> 00:08:15,300
对所有的 it 的设备

271
00:08:15,300 --> 00:08:18,233
下行的总带宽跟上行的总带宽

272
00:08:18,233 --> 00:08:19,933
接入到一个核心的交换机

273
00:08:19,933 --> 00:08:23,433
或者是 spine 交换机的一个总带宽的比例

274
00:08:23,566 --> 00:08:25,533
因此呢整个收敛比的公式

275
00:08:25,533 --> 00:08:28,500
就等于下行的端口的带宽的总和

276
00:08:28,500 --> 00:08:30,733
除以上行的带宽的总和

277
00:08:30,733 --> 00:08:31,500
那经常

278
00:08:31,500 --> 00:08:34,466
会在后面看到 1:1 的无收敛

279
00:08:34,466 --> 00:08:35,800
3:1 的收敛

280
00:08:35,800 --> 00:08:36,533
这种方式

281
00:08:36,533 --> 00:08:38,433
更多的就是指

282
00:08:38,433 --> 00:08:40,900
那简单的就是说现在有一个入口

283
00:08:40,900 --> 00:08:42,800
这个入口呢就是对应的交换机

284
00:08:42,800 --> 00:08:43,400
那这里面

285
00:08:43,400 --> 00:08:46,700
十条入口的出车道跟十条出口的车道

286
00:08:46,700 --> 00:08:47,400
那这个时候

287
00:08:47,400 --> 00:08:49,033
是非常的畅通

288
00:08:49,133 --> 00:08:50,933
所以在 GPU 的 AI 集群里面

289
00:08:50,933 --> 00:08:53,100
更希望是做到无收敛

290
00:08:53,100 --> 00:08:54,700
特别是在大模型训练

291
00:08:54,700 --> 00:08:57,066
但是无收敛会带来很多的一个代价

292
00:08:57,066 --> 00:08:59,100
那另外的话看一下 3:1 的收敛

293
00:08:59,100 --> 00:08:59,800
就是现

294
00:08:59,800 --> 00:09:02,333
在有三条车道要挤进来

295
00:09:02,333 --> 00:09:04,233
做一条车道进行出口

296
00:09:04,233 --> 00:09:04,800
这个时候

297
00:09:04,800 --> 00:09:07,300
有可能会导致网络拥塞

298
00:09:07,300 --> 00:09:09,266
或者所谓的导致堵车

299
00:09:09,266 --> 00:09:11,633
现在啊 90%的 AI 集群

300
00:09:11,633 --> 00:09:13,200
现在万卡的集群

301
00:09:13,200 --> 00:09:14,900
最大的卡顿的原因

302
00:09:14,900 --> 00:09:17,333
就是来源于网络的收敛比过高

303
00:09:17,333 --> 00:09:18,333
所谓的收敛比过高

304
00:09:18,333 --> 00:09:19,700
就是 2:1 的收敛

305
00:09:19,700 --> 00:09:20,500
3:1 的收敛

306
00:09:20,500 --> 00:09:21,700
15:1 的收敛

307
00:09:21,800 --> 00:09:23,200
只要 n

308
00:09:23,200 --> 00:09:25,866
就是 n 现在是大于一的时候

309
00:09:25,866 --> 00:09:27,800
就会导致收敛比

310
00:09:27,800 --> 00:09:29,100
慢慢的增加

311
00:09:29,100 --> 00:09:29,933
那这个时候

312
00:09:29,933 --> 00:09:31,800
就很多条车道

313
00:09:31,800 --> 00:09:34,066
挤进去一个收费路口

314
00:09:34,066 --> 00:09:35,400
这么一个简单的概念

315
00:09:35,400 --> 00:09:35,933
所以说

316
00:09:35,933 --> 00:09:38,266
现在在整个 AI 集群里面

317
00:09:38,266 --> 00:09:39,200
训练过程当中

318
00:09:39,200 --> 00:09:41,100
主要是训大模型嘛

319
00:09:41,100 --> 00:09:42,266
那训大模型的过程当中

320
00:09:42,266 --> 00:09:44,266
GPU 就需要频繁的去交互

321
00:09:44,266 --> 00:09:45,400
各种各样的数据

322
00:09:45,400 --> 00:09:47,200
因为可能会用 ring all reduce

323
00:09:47,200 --> 00:09:48,633
half doubling 的 all reduce

324
00:09:48,633 --> 00:09:51,833
all2all 啊各种各样的集合通信的算法

325
00:09:51,833 --> 00:09:53,466
不管你用哪种算法也好

326
00:09:53,466 --> 00:09:54,500
对应的数据

327
00:09:54,500 --> 00:09:56,266
也就是对应大模型的参数

328
00:09:56,266 --> 00:09:57,633
梯度啊权重

329
00:09:57,633 --> 00:10:01,333
会大量的在卡跟卡之间的频繁的交互

330
00:10:01,333 --> 00:10:02,333
那这个时候

331
00:10:02,333 --> 00:10:03,633
因为涉及到交互

332
00:10:03,633 --> 00:10:05,266
就必须要通过网络

333
00:10:05,266 --> 00:10:06,266
那这个时候

334
00:10:06,266 --> 00:10:07,033
高收敛比

335
00:10:07,033 --> 00:10:09,233
就有可能会导致整体的数据

336
00:10:09,233 --> 00:10:11,033
堵车啊网络拥塞

337
00:10:11,033 --> 00:10:11,933
那这个时候

338
00:10:11,933 --> 00:10:13,533
因为大部分的 GPU

339
00:10:13,533 --> 00:10:16,000
你会发现现在的 MFU 的利用率

340
00:10:16,000 --> 00:10:18,900
只有 5% 12-60 左右

341
00:10:18,900 --> 00:10:20,400
那这个时候大部分的时间

342
00:10:20,400 --> 00:10:22,600
GPU 的时间都在等待数据

343
00:10:22,600 --> 00:10:24,166
所以说希望

344
00:10:24,166 --> 00:10:25,900
从传统的 n 比 1

345
00:10:25,900 --> 00:10:28,866
慢慢的向 1:1 这种无收敛的演进

346
00:10:28,900 --> 00:10:30,533
因此呢 400G 的高性能网卡

347
00:10:30,533 --> 00:10:32,600
现在要逐渐的成为各种各样

348
00:10:32,600 --> 00:10:33,933
主流的方式

349
00:10:33,933 --> 00:10:36,600
当然这里面主要是指 scale out

350
00:10:36,600 --> 00:10:39,600
那 scale up 呢可能是指超节点

351
00:10:39,600 --> 00:10:41,066
现在还不是这种方式

352
00:10:41,066 --> 00:10:42,933
做大规模的组网的时候

353
00:10:42,933 --> 00:10:46,700
非常的讲究网络带宽的收敛比

354
00:10:47,266 --> 00:10:47,733
那这里面

355
00:10:47,733 --> 00:10:49,933
还是做一个具体的例子

356
00:10:49,933 --> 00:10:53,833
现在呢我有 256 台 GPU leaf 交换机

357
00:10:53,833 --> 00:10:55,333
就是对应的柜顶的交换机

358
00:10:55,333 --> 00:10:57,066
接入层的交换机

359
00:10:57,066 --> 00:10:59,833
下行的端口的总带宽呢是 40G

360
00:10:59,833 --> 00:11:01,700
那等于 40G 乘以 256

361
00:11:01,700 --> 00:11:04,200
就是 10.24Tbps

362
00:11:04,200 --> 00:11:05,333
核心交换器

363
00:11:05,333 --> 00:11:07,400
整体上行的端口的带宽

364
00:11:07,400 --> 00:11:09,533
能大于 10.24Tbps

365
00:11:09,533 --> 00:11:10,133
那这个时候

366
00:11:10,133 --> 00:11:13,200
就说它是一个 1:1 的无收敛的网络

367
00:11:13,200 --> 00:11:13,733
那当然

368
00:11:13,733 --> 00:11:15,866
一个 n 比 1 的收敛的网络

369
00:11:15,866 --> 00:11:16,600
的时候

370
00:11:16,600 --> 00:11:20,033
假设现在有又同样的有 256 台 GPU

371
00:11:20,133 --> 00:11:23,200
整体的需求带宽呢就是 40G 乘以 256

372
00:11:23,200 --> 00:11:24,700
但是呢核心的交换机

373
00:11:24,700 --> 00:11:26,533
对应的 spine 交换机

374
00:11:26,533 --> 00:11:28,700
仅仅只有 2.56Tbps

375
00:11:28,700 --> 00:11:31,200
那这个时候你会发现就成为瓶颈

376
00:11:31,200 --> 00:11:33,866
这个就是 n 比一的网络的收敛比

377
00:11:33,866 --> 00:11:34,433
的时候

378
00:11:34,433 --> 00:11:37,533
就会导致上行跟下行不对等

379
00:11:37,800 --> 00:11:39,233
在传统的 it 集群

380
00:11:39,233 --> 00:11:41,066
可能这种方式是没太多问题

381
00:11:41,066 --> 00:11:43,033
不需要非常夸张的收敛比

382
00:11:43,033 --> 00:11:43,466
但是

383
00:11:43,466 --> 00:11:46,133
你会发现在 AI 大模型训练过程当中

384
00:11:46,133 --> 00:11:48,300
会经常遇到这么一个过程

385
00:11:48,300 --> 00:11:50,733
网络收敛比呢极度的影响

386
00:11:50,733 --> 00:11:52,666
那这个呢也举一个具体的例子

387
00:11:52,666 --> 00:11:54,200
也就是 ZOMI 现在的工作当中

388
00:11:54,200 --> 00:11:54,600
遇到一

389
00:11:54,600 --> 00:11:56,300
个算法部给提的一个问题

390
00:11:56,300 --> 00:11:58,800
为什么我用 16 卡训练的过程当中

391
00:11:58,800 --> 00:12:00,533
你会发现吞吐还可以

392
00:12:00,533 --> 00:12:02,400
去到 300 tokens 每秒

393
00:12:02,400 --> 00:12:04,800
但是呢我用了 32 卡之后

394
00:12:04,800 --> 00:12:06,133
整个网络呀

395
00:12:06,133 --> 00:12:08,700
就变成 250 tokens 每秒

396
00:12:08,700 --> 00:12:09,833
我用的卡越多

397
00:12:09,833 --> 00:12:11,400
不应该我的吞吐越大吗

398
00:12:11,400 --> 00:12:12,666
那这个时候其实

399
00:12:12,666 --> 00:12:13,533
ZOMI 定位完之后

400
00:12:13,533 --> 00:12:14,333
你就发现哎

401
00:12:14,333 --> 00:12:15,866
卡跟卡之间

402
00:12:15,866 --> 00:12:18,333
因为呢已经跨机器跨节点

403
00:12:18,333 --> 00:12:18,900
那这个时候

404
00:12:18,900 --> 00:12:19,733
扩充卡数

405
00:12:19,733 --> 00:12:20,83
看到网络收敛比

406
00:12:20,833 --> 00:12:23,433
是 7:1 的导致整体的网络的拥塞

407
00:12:23,433 --> 00:12:26,600
影响了整个大模型的训练的吞吐

408
00:12:26,800 --> 00:12:28,733
看一下网络的拓扑

409
00:12:28,733 --> 00:12:29,733
拓扑这个很重要

410
00:12:29,733 --> 00:12:32,400
是决定数据的传输的路径

411
00:12:32,400 --> 00:12:33,866
那这个比较形象的一个比喻

412
00:12:33,866 --> 00:12:36,733
就像对应的快递的分拣中心

413
00:12:36,733 --> 00:12:38,500
现在整个城市

414
00:12:38,500 --> 00:12:42,133
或者现在你的站点有非常多的快递

415
00:12:42,133 --> 00:12:42,900
那快递员

416
00:12:42,900 --> 00:12:44,400
应该怎么把包裹

417
00:12:44,400 --> 00:12:46,233
分发到每个人的手中

418
00:12:46,233 --> 00:12:47,066
那这个时候

419
00:12:47,066 --> 00:12:49,300
就决定数据的传输的路径

420
00:12:49,300 --> 00:12:51,433
特别是在万卡集群里面

421
00:12:51,466 --> 00:12:53,400
如果没有一个好的网络拓扑

422
00:12:53,400 --> 00:12:55,200
那整体的万卡集群

423
00:12:55,200 --> 00:12:56,933
就像没有导航的快递系统

424
00:12:56,933 --> 00:12:57,833
随便的分发

425
00:12:57,833 --> 00:12:58,866
随便的丢

426
00:13:07,700 --> 00:13:09,700
so 需要有一个好的网络的拓扑

427
00:13:09,700 --> 00:13:10,533
把快递

428
00:13:10,533 --> 00:13:12,900
把包裹分发到每个人的手中

429
00:13:12,900 --> 00:13:13,466
每个家庭

430
00:13:13,466 --> 00:13:15,200
每个户因此

431
00:13:15,200 --> 00:13:17,066
只要 AI 集群的大模型

432
00:13:17,066 --> 00:13:19,066
整个网络要做集群

433
00:13:19,066 --> 00:13:20,533
必然叫 scale out

434
00:13:20,533 --> 00:13:21,133
那这个时候

435
00:13:21,133 --> 00:13:24,300
必然要上一些高级的拓扑的架构

436
00:13:24,466 --> 00:13:26,633
那刚才讲到的其实也分两个嘛

437
00:13:26,633 --> 00:13:28,466
scale up 那大部分

438
00:13:28,466 --> 00:13:29,000
会用

439
00:13:29,000 --> 00:13:31,066
简单的全互联的拓扑

440
00:13:31,066 --> 00:13:33,033
那上到 scale out

441
00:13:33,033 --> 00:13:35,433
就必须使用高级的网络的拓扑

442
00:13:35,433 --> 00:13:36,900
那所谓的高级

443
00:13:36,900 --> 00:13:38,433
呃现在分开来看

444
00:13:38,433 --> 00:13:39,000
那第一个
 
445
00:13:39,000 --> 00:13:41,066
就是 scale up 的一个具体的趋势是

446
00:13:41,400 --> 00:13:42,400
那么你会发现

447
00:13:42,400 --> 00:13:45,533
最重要的就是任意的两点能够直连

448
00:13:45,533 --> 00:13:47,833
就保证延迟能做到最低

449
00:13:47,833 --> 00:13:49,500
但是呢这个会引起一个问题

450
00:13:49,500 --> 00:13:51,333
就是假设 GPU 数量

451
00:13:51,333 --> 00:13:52,933
是 n 个整体的连线

452
00:13:52,933 --> 00:13:55,700
就变成了 n 乘以 n 减 1/2

453
00:13:55,700 --> 00:13:56,600
那这个时候

454
00:13:56,600 --> 00:13:59,400
如果 8 台 GPU 就需要 28 条路线

455
00:13:59,400 --> 00:14:01,333
所以说做 scale up 的时候

456
00:14:01,333 --> 00:14:03,333
整体的一个网络的带宽

457
00:14:03,333 --> 00:14:04,733
或者网络拓扑

458
00:14:04,733 --> 00:14:05,433
比较简单

459
00:14:05,433 --> 00:14:07,200
采用了全互联的方式

460
00:14:07,200 --> 00:14:10,233
但是呢它会导致连线会很多

461
00:14:10,233 --> 00:14:10,666
那这个

462
00:14:10,666 --> 00:14:11,733
就是 scale up 的方式

463
00:14:11,733 --> 00:14:14,733
也是为什么现在看到的超节点

464
00:14:14,866 --> 00:14:16,533
或者英伟达的 NVL

465
00:14:16,533 --> 00:14:18,933
后面的线缆会非常非常多

466
00:14:18,933 --> 00:14:21,066
就是因为希望的做 scale up

467
00:14:21,066 --> 00:14:22,233
做超级点的时候

468
00:14:22,233 --> 00:14:25,100
全互联实现一个端到端

469
00:14:25,133 --> 00:14:26,866
或者我某一张 a 卡

470
00:14:26,866 --> 00:14:28,733
去到我另外一张 k 卡

471
00:14:28,733 --> 00:14:31,933
我之间能够有一条最简单的连线

472
00:14:31,933 --> 00:14:33,233
最直接的连线

473
00:14:33,233 --> 00:14:35,233
最短的连线路径

474
00:14:35,566 --> 00:14:37,466
但是呢要做 scale out 的时候

475
00:14:37,466 --> 00:14:39,800
需要做一个大规模的组网

476
00:14:39,800 --> 00:14:40,866
那这个时候你会发现

477
00:14:40,866 --> 00:14:42,200
组网的方式

478
00:14:42,200 --> 00:14:42,866
就高级

479
00:14:42,866 --> 00:14:45,533
一个网络模型的拓扑有很多种

480
00:14:45,533 --> 00:14:46,066
有 Fat-Tree

481
00:14:46,066 --> 00:14:46,900
有 dragonfly

482
00:14:46,900 --> 00:14:47,800
有 clos

483
00:14:47,800 --> 00:14:49,533
不同的网络的拓扑

484
00:14:49,533 --> 00:14:50,466
不同的网络拓扑

485
00:14:50,466 --> 00:14:53,333
你会发现它用在不同的集群上面

486
00:14:53,333 --> 00:14:54,066
那这个时候

487
00:14:54,066 --> 00:14:55,600
任意的两点

488
00:14:55,600 --> 00:14:56,500
因为做 scale out 嘛

489
00:14:56,500 --> 00:14:58,633
需要存在多条路径

490
00:14:58,633 --> 00:14:59,400
那这个时候

491
00:14:59,400 --> 00:15:00,866
还是回到这个图

492
00:15:00,866 --> 00:15:02,600
是我一个 a 节点

493
00:15:02,600 --> 00:15:04,866
跟这里面的 b 节点进行连接的时候

494
00:15:04,866 --> 00:15:07,033
我可以这么去走

495
00:15:07,133 --> 00:15:09,100
我也可以这么去走

496
00:15:09,100 --> 00:15:09,733
所以你会发现

497
00:15:09,733 --> 00:15:11,300
有多条路径

498
00:15:11,300 --> 00:15:13,900
去到达任意的另外两个点

499
00:15:14,066 --> 00:15:16,633
但是做超节点 scale up 的时候

500
00:15:16,633 --> 00:15:19,200
就希望 a 节点呢跟 b 节点之间

501
00:15:19,200 --> 00:15:21,333
有一个天然的最短的路径

502
00:15:21,333 --> 00:15:23,400
能够快速的最快的到达

503
00:15:23,400 --> 00:15:25,500
因此呢根据 scale up 跟 scale out

504
00:15:25,500 --> 00:15:27,800
采用不同的高级的网络的拓扑

505
00:15:27,800 --> 00:15:29,133
那网络拓扑的形态

506
00:15:29,133 --> 00:15:31,633
会决定数据的传输的对应路径

507
00:15:31,633 --> 00:15:33,300
到底短还是长

508
00:15:33,300 --> 00:15:33,633
这个时候

509
00:15:33,633 --> 00:15:36,533
就决定网络的传输的对应的路径

510
00:15:36,533 --> 00:15:37,633
到底有哪一条

511
00:15:37,633 --> 00:15:39,866
我的数据应该从 a 点到 b 点

512
00:15:39,866 --> 00:15:40,800
走哪几个点

513
00:15:40,800 --> 00:15:42,233
走哪几个交换机

514
00:15:42,933 --> 00:15:45,066
收敛比呢决定对应的带宽

515
00:15:45,066 --> 00:15:46,000
网络的拓扑

516
00:15:46,000 --> 00:15:47,400
会决定网络的路径

517
00:15:47,400 --> 00:15:48,733
和流量的管理

518
00:15:48,733 --> 00:15:49,733
一个好的收敛比

519
00:15:49,733 --> 00:15:52,100
和一个好的拓扑结构

520
00:15:52,100 --> 00:15:55,400
会有效的提升整体的网络的效率

521
00:15:55,400 --> 00:15:57,666
简单的分享完这两个内容之后

522
00:15:57,666 --> 00:15:59,533
后面第三个内容跟第四个内容

523
00:15:59,533 --> 00:16:02,400
留到下一个视频再跟大家去分享

524
00:16:02,400 --> 00:16:04,233
今天的内容呢就先到这里为止

525
00:16:04,233 --> 00:16:04,733
谢谢各位
