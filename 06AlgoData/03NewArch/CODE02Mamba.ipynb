{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cdd30c",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# Mamba 状态空间模型(DONE)\n",
    "\n",
    "这个实验带你探索超越 Transformer 的下一代架构[Mamba](https://arxiv.org/abs/2312.00752)。你将动手实现 Mamba 的核心组件——硬件感知的状态空间模型和选择性扫描机制(S6)。通过代码你将看到，Mamba 如何根据输入动态调整模型参数，实现数据依赖的隐藏状态演化，并利用并行算法实现高效的训练推理。\n",
    "\n",
    "通过该实验，你将直观感受到 Mamba 是如何通过结构化状态空间模型和硬件感知算法优化，在语言、基因等多个长序列建模领域展现出巨大潜力，成为当前最受关注的新架构之一。\n",
    "\n",
    "![](./images/Practice02Mamba01.png)\n",
    "\n",
    "## 1. 状态空间模型 SSM\n",
    "\n",
    "近年来，Transformer 架构在深度学习领域取得了巨大成功，但其二次方的计算复杂度限制了其在超长序列任务中的应用。2023 年底提出的 Mamba 模型基于状态空间模型(SSM)，通过选择性机制和硬件感知算法，实现了线性计算复杂度，成为处理长序列任务的新范式。\n",
    "\n",
    "状态空间模型源于控制理论，用于描述动态系统行为。其核心思想是通过隐藏状态来捕捉序列的历史信息，并根据当前输入更新状态。\n",
    "\n",
    "### 1.1 环境设置与导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af212d",
   "metadata": {},
   "source": [
    "### 1.2 连续时间 SSM\n",
    "\n",
    "连续时间状态空间模型用微分方程表示：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h'(t) &= A h(t) + B x(t) \\quad &\\text{(状态方程)} \\\\\n",
    "y(t) &= C h(t) + D x(t) \\quad &\\text{(输出方程)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $x(t)$: 输入信号\n",
    "- $h(t)$: 隐藏状态\n",
    "- $y(t)$: 输出信号\n",
    "- $A, B, C, D$: 可学习参数矩阵\n",
    "\n",
    "### 1.3 离散化过程\n",
    "\n",
    "为了在数字系统中使用，需要将连续方程离散化。Mamba 采用零阶保持(ZOH)方法：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\overline{A} &= e^{A\\Delta} \\\\\n",
    "\\overline{B} &= (e^{A\\Delta} - I) A^{-1} B\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "离散化后的方程：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h_k &= \\overline{A} h_{k-1} + \\overline{B} x_k \\\\\n",
    "y_k &= C h_k + D x_k\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中 $\\Delta$ 为步长参数，控制状态更新频率。\n",
    "\n",
    "### 1.4 基础 SSM 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1eed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSSM(nn.Module):\n",
    "    def __init__(self, state_dim, input_dim):\n",
    "        super().__init__()\n",
    "        # 初始化状态转移矩阵和输入/输出参数\n",
    "        self.A = nn.Parameter(torch.randn(state_dim, state_dim))\n",
    "        self.B = nn.Parameter(torch.randn(state_dim, input_dim))\n",
    "        self.C = nn.Parameter(torch.randn(input_dim, state_dim))\n",
    "        self.D = nn.Parameter(torch.randn(input_dim))\n",
    "        \n",
    "        # 控制状态更新频率的步长参数\n",
    "        self.delta = nn.Parameter(torch.tensor(0.1))\n",
    "        \n",
    "    def discretize(self):\n",
    "        \"\"\"实现零阶保持离散化，将连续参数转换为离散参数\"\"\"\n",
    "        A_bar = torch.matrix_exp(self.A * self.delta)\n",
    "        inv_A = torch.inverse(self.A)\n",
    "        A_exp_minus_I = torch.matrix_exp(self.A * self.delta) - torch.eye(self.A.size(0))\n",
    "        B_bar = A_exp_minus_I @ inv_A @ self.B\n",
    "        \n",
    "        return A_bar, B_bar\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"循环更新隐藏状态并计算输出\"\"\"\n",
    "        A_bar, B_bar = self.discretize()\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        state_dim = self.A.size(0)\n",
    "        \n",
    "        h = torch.zeros(batch_size, state_dim).to(x.device)\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            h = torch.tanh(A_bar @ h + B_bar @ x[:, i, :])\n",
    "            y = self.C @ h + self.D * x[:, i, :]\n",
    "            outputs.append(y)\n",
    "            \n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b91d94",
   "metadata": {},
   "source": [
    "## 2. Mamba 的核心创新\n",
    "\n",
    "传统 SSM 的主要限制是参数固定，无法根据输入内容动态调整。Mamba 通过选择性机制解决了这一问题。\n",
    "\n",
    "### 2.1 选择性状态空间模型\n",
    "\n",
    "Mamba 的核心创新是使 SSM 参数成为输入的函数：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "B_k &= B(x_k) \\\\\n",
    "C_k &= C(x_k) \\\\\n",
    "\\Delta_k &= \\tau_{\\Delta}(x_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中 $\\tau_{\\Delta}$ 是一个小型神经网络，根据当前输入 $x_k$ 预测步长 $\\Delta_k$。\n",
    "\n",
    "这种设计使模型能够对重要输入延长记忆（大 $\\Delta$），对无关输入缩短记忆（小 $\\Delta$）。\n",
    "\n",
    "![](./images/Practice02Mamba02.png)\n",
    "\n",
    "### 2.2 选择性 SSM 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d409d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveSSM(nn.Module):\n",
    "    def __init__(self, dim, state_dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim  # 输入/输出维度（D）\n",
    "        self.state_dim = state_dim  # 状态维度（N）\n",
    "        \n",
    "        # 投影层生成输入依赖的参数\n",
    "        self.proj = nn.Linear(dim, 3 * dim + state_dim)\n",
    "        self.A = nn.Parameter(torch.randn(dim, state_dim))\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        projected = self.proj(x)\n",
    "        # 分割为Δ、B、C 和门控信号\n",
    "        delta, B, C, gate = torch.split(\n",
    "            projected, [self.dim, self.dim, self.state_dim, self.dim], dim=-1\n",
    "        )\n",
    "        \n",
    "        delta = F.softplus(delta)  # 确保Δ为正数\n",
    "        gate = F.silu(gate)        # 门控激活\n",
    "        # 计算离散化参数\n",
    "        A_bar = torch.exp(delta.unsqueeze(-1) * self.A)\n",
    "        B_bar = delta.unsqueeze(-1) * B.unsqueeze(-1)\n",
    "        \n",
    "        # 循环更新状态并计算输出\n",
    "        h = torch.zeros(batch_size, self.dim, self.state_dim, device=x.device)\n",
    "        outputs = []\n",
    "        for i in range(seq_len):\n",
    "            h = A_bar[:, i] * h + B_bar[:, i] * x[:, i].unsqueeze(-1)\n",
    "            y = (h @ C[:, i].unsqueeze(-1)).squeeze(-1)\n",
    "            outputs.append(y)\n",
    "        \n",
    "        output = torch.stack(outputs, dim=1)\n",
    "        return self.out_proj(output) * gate  # 应用门控控制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a241274",
   "metadata": {},
   "source": [
    "## 3. 硬件感知算法\n",
    "\n",
    "Mamba 的另一个关键创新是硬件感知算法，通过优化内存访问模式在 GPU 上实现高效计算。传统 RNN 需要串行计算，而 Mamba 利用并行前缀和算法实现高效并行计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ebac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwareAwareScan(nn.Module):\n",
    "    def __init__(self, chunk_size=1024):\n",
    "        super().__init__()\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "    def forward(self, A_bar, B_bar, x):\n",
    "        batch_size, seq_len, dim, state_dim = A_bar.shape\n",
    "        \n",
    "        # 短序列直接使用串行扫描\n",
    "        if seq_len <= self.chunk_size:\n",
    "            return self.serial_scan(A_bar, B_bar, x)\n",
    "        \n",
    "        # 长序列分块并行处理\n",
    "        return self.parallel_scan(A_bar, B_bar, x)\n",
    "    \n",
    "    def serial_scan(self, A_bar, B_bar, x):\n",
    "        \"\"\"串行状态更新\"\"\"\n",
    "        batch_size, seq_len, dim, state_dim = A_bar.shape\n",
    "        \n",
    "        h = torch.zeros(batch_size, dim, state_dim).to(x.device)\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            h = A_bar[:, i] * h + B_bar[:, i] * x[:, i].unsqueeze(-1)\n",
    "            outputs.append(h)\n",
    "            \n",
    "        return torch.stack(outputs, dim=1)\n",
    "    \n",
    "    def parallel_scan(self, A_bar, B_bar, x):\n",
    "        \"\"\"分块并行扫描优化内存访问\"\"\"\n",
    "        batch_size, seq_len, dim, state_dim = A_bar.shape\n",
    "        num_chunks = math.ceil(seq_len / self.chunk_size)\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_state = torch.zeros(batch_size, dim, state_dim).to(x.device)\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start = i * self.chunk_size\n",
    "            end = min(start + self.chunk_size, seq_len)\n",
    "            \n",
    "            chunk_len = end - start\n",
    "            chunk_A = A_bar[:, start:end]\n",
    "            chunk_B = B_bar[:, start:end]\n",
    "            chunk_x = x[:, start:end]\n",
    "            \n",
    "            # 处理当前块\n",
    "            chunk_output = self.process_chunk(chunk_A, chunk_B, chunk_x, hidden_state)\n",
    "            outputs.append(chunk_output)\n",
    "            \n",
    "            # 更新隐藏状态传递给下一块\n",
    "            hidden_state = chunk_output[:, -1]\n",
    "            \n",
    "        return torch.cat(outputs, dim=1)\n",
    "    \n",
    "    def process_chunk(self, A, B, x, h0):\n",
    "        \"\"\"处理单个块内的状态更新\"\"\"\n",
    "        batch_size, chunk_len, dim, state_dim = A.shape\n",
    "        \n",
    "        h = h0.clone()\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(chunk_len):\n",
    "            h = A[:, i] * h + B[:, i] * x[:, i].unsqueeze(-1)\n",
    "            outputs.append(h)\n",
    "            \n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07862e9",
   "metadata": {},
   "source": [
    "![](./images/Practice02Mamba03.png)\n",
    "\n",
    "## 4. 执行实验与结果分析\n",
    "\n",
    "### 4.1 整体 Mamba 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, dim, state_dim=16):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        # 输入投影分离门控和特征路径\n",
    "        self.in_proj = nn.Linear(dim, 2 * dim)\n",
    "        \n",
    "        # 卷积层捕捉局部特征\n",
    "        self.conv = nn.Conv1d(dim, dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 选择性 SSM 处理全局依赖\n",
    "        self.ssm = SelectiveSSM(dim, state_dim)\n",
    "        \n",
    "        # 输出投影\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 分离门控和特征路径\n",
    "        x_proj = self.in_proj(x)\n",
    "        gate, x = torch.split(x_proj, [self.dim, self.dim], dim=-1)\n",
    "        gate = F.silu(gate)\n",
    "        \n",
    "        # 卷积处理局部特征\n",
    "        x_conv = self.conv(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x_conv = F.silu(x_conv)\n",
    "        \n",
    "        # 选择性 SSM 处理序列依赖\n",
    "        x_ssm = self.ssm(x_conv)\n",
    "        \n",
    "        # 门控融合和残差连接\n",
    "        output = self.out_proj(x_ssm * gate)\n",
    "        return output + x  # 残差连接稳定训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2830fd",
   "metadata": {},
   "source": [
    "### 4.2 简单复制任务测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_copy_task():\n",
    "    \"\"\"测试模型记忆和复制序列的能力\"\"\"\n",
    "    dim, state_dim, seq_len, batch_size = 32, 16, 100, 4\n",
    "    \n",
    "    model = MambaBlock(dim, state_dim)\n",
    "    input_seq = torch.randn(batch_size, seq_len, dim)\n",
    "    output = model(input_seq)\n",
    "    \n",
    "    print(f\"输入形状: {input_seq.shape}, 输出形状: {output.shape}\")\n",
    "    print(f\"参数数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # 估计计算量(FLOPs)\n",
    "    flops = estimate_flops(model, input_seq)\n",
    "    print(f\"估计 FLOPs: {flops / 1e6:.2f} M\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "def estimate_flops(model, input_seq):\n",
    "    \"\"\"计算模型前向传播的浮点运算量\"\"\"\n",
    "    batch_size, seq_len, dim = input_seq.shape\n",
    "    state_dim = model.ssm.state_dim\n",
    "    \n",
    "    # 各组件 FLOPs 计算\n",
    "    in_proj_flops = batch_size * seq_len * dim * (2 * dim)\n",
    "    conv_flops = batch_size * seq_len * dim * (dim * 3)\n",
    "    ssm_proj_flops = batch_size * seq_len * dim * (3 * dim + state_dim)\n",
    "    state_update_flops = batch_size * seq_len * 2 * dim * state_dim\n",
    "    ssm_out_flops = batch_size * seq_len * dim * dim\n",
    "    \n",
    "    return in_proj_flops + conv_flops + ssm_proj_flops + state_update_flops + ssm_out_flops\n",
    "\n",
    "output = test_copy_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f5ef2",
   "metadata": {},
   "source": [
    "### 4.3 与 Transformer 对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4efaf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_transformer():\n",
    "    \"\"\"对比 Mamba 和 Transformer 在长序列下的性能\"\"\"\n",
    "    dim, seq_len, batch_size = 32, 1000, 4\n",
    "    \n",
    "    mamba_model = MambaBlock(dim)\n",
    "    transformer_layer = nn.TransformerEncoderLayer(dim, nhead=4)\n",
    "    \n",
    "    input_seq = torch.randn(batch_size, seq_len, dim)\n",
    "    \n",
    "    # 测量内存使用\n",
    "    with torch.no_grad():\n",
    "        mamba_output = mamba_model(input_seq)\n",
    "        mamba_memory = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        transformer_output = transformer_layer(input_seq.transpose(0, 1))\n",
    "        transformer_memory = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0\n",
    "    \n",
    "    # 计算复杂度对比\n",
    "    mamba_flops = estimate_flops(mamba_model, input_seq)\n",
    "    transformer_flops = 4 * seq_len * seq_len * dim  # 自注意力二次方复杂度\n",
    "    \n",
    "    print(f\"序列长度: {seq_len}\")\n",
    "    print(f\"Mamba 内存: {mamba_memory/1024**2:.2f} MB, Transformer 内存: {transformer_memory/1024**2:.2f} MB\")\n",
    "    print(f\"Mamba FLOPs: {mamba_flops/1e6:.2f} M, Transformer FLOPs: {transformer_flops/1e6:.2f} M\")\n",
    "    print(f\"复杂度比率: {transformer_flops/mamba_flops:.2f}倍\")\n",
    "\n",
    "compare_with_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a60b7a",
   "metadata": {},
   "source": [
    "## 5. 总结与思考\n",
    "\n",
    "本实验实现并验证了 Mamba 状态空间模型的核心组件，其依托选择性机制与硬件感知算法，实现线性计算复杂度，适配长序列任务。\n",
    "\n",
    "Mamba 具备三大优势：线性复杂度适用于 DNA、高分辨率图像等长序列场景，动态记忆能依输入调整状态演化以提升语义理解，硬件友好性使算法与 GPU 内存特性深度协同；但也存在局限，如短文本效果略逊 Transformer、需专用 CUDA 内核实现最佳性能、训练数据需求比 Transformer 多。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
