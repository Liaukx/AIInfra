{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc5ec6b4",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# 深入注意力机制 MHA、MQA、GQA、MLA(DONE)\n",
    "\n",
    "本实验将从头实现标准的多头注意力（MHA），并在此基础上，逐步实现其三种重要的变体：**MQA**、**GQA** 和 **MLA**。通过对比它们的代码差异和性能指标，我们将深入理解它们的设计动机和优劣。\n",
    "\n",
    "1. **基础缩放点积注意力**：所有注意力机制的基础组件\n",
    "2. **多头注意力（MHA）**：经典的多头设计，每个头有独立的 Q、K、V 投影\n",
    "3. **多查询注意力（MQA）**：所有头共享 K 和 V 投影，提高效率\n",
    "4. **分组查询注意力（GQA）**：MHA 和 MQA 的折中方案，头分组共享 K 和 V\n",
    "5. **多潜在注意力（MLA）**：使用可学习的潜在向量作为 K 和 V，与序列长度无关\n",
    "\n",
    "![](./images/Practice07Attention01.png)\n",
    "\n",
    "## 1. 注意力机制基础\n",
    "\n",
    "### 1.1 Scaled Dot-Product Attention\n",
    "\n",
    "核心的注意力计算机制，由 **《Attention Is All You Need》** 提出。其目的是通过查询向量（Query）与键向量（Key）的相似度，来加权求和值向量（Value）。\n",
    "\n",
    "**公式如下：**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "其中：\n",
    "- $Q$: 查询矩阵，形状为 `(seq_len, d_k)` 或 `(batch_size, seq_len, d_k)`\n",
    "- $K$: 键矩阵，形状为 `(seq_len, d_k)` 或 `(batch_size, seq_len, d_k)`\n",
    "- $V$: 值矩阵，形状为 `(seq_len, d_v)` 或 `(batch_size, seq_len, d_v)`\n",
    "- $d_k$: 键向量的维度，缩放因子 $\\sqrt{d_k}$ 用于防止点积结果过大导致梯度消失。\n",
    "\n",
    "让我们先实现这个核心函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3366e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    计算缩放点积注意力。\n",
    "    \n",
    "    Args:\n",
    "        query: 查询张量，形状 (..., seq_len_q, d_k)\n",
    "        key: 键张量，形状 (..., seq_len_k, d_k)\n",
    "        value: 值张量，形状 (..., seq_len_v, d_v)\n",
    "        mask: 可选的掩码张量，形状 (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    Returns:\n",
    "        输出张量，形状 (..., seq_len_q, d_v)\n",
    "        注意力权重张量，形状 (..., seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    # 1. 计算 Q 和 K 的转置的点积\n",
    "    matmul_qk = torch.matmul(query, key.transpose(-2, -1))  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # 2. 缩放：除以 sqrt(d_k)\n",
    "    d_k = query.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(d_k)\n",
    "    \n",
    "    # 3. 可选：应用掩码（在解码器中用于掩盖未来位置）\n",
    "    if mask is not None:\n",
    "        # 将掩码中为 0 的位置置为一个非常大的负数，softmax 后概率为 0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # 4. 计算注意力权重 (softmax on the last axis, seq_len_k)\n",
    "    attention_weights = F.softmax(scaled_attention_logits, dim=-1) # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # 5. 用注意力权重对 V 进行加权求和\n",
    "    output = torch.matmul(attention_weights, value) # (..., seq_len_q, d_v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6900f721",
   "metadata": {},
   "source": [
    "### 1.2 Multi-Head Attention (MHA)\n",
    "\n",
    "标准的多头注意力将输入线性投影到 `h` 个不同的头中，在每个头上独立进行注意力计算，最后将结果拼接并投影回原维度。这样可以让模型共同关注来自不同位置的不同表示子空间的信息。\n",
    "\n",
    "![](./images/Practice02TransformerTrain01.png)\n",
    "\n",
    "**公式如下：**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MultiHead}(Q, K, V) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O \\\\\n",
    "\\text{where } \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da80c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"标准的多头注意力机制 (MHA)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model 必须能被 num_heads 整除\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads # 每个头的维度\n",
    "        \n",
    "        # 定义线性投影层\n",
    "        self.wq = nn.Linear(d_model, d_model) # W^Q\n",
    "        self.wk = nn.Linear(d_model, d_model) # W^K\n",
    "        self.wv = nn.Linear(d_model, d_model) # W^V\n",
    "        self.dense = nn.Linear(d_model, d_model) # W^O\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"将最后的维度 (d_model) 分割为 (num_heads, depth).\n",
    "        并转置为 (batch_size, num_heads, seq_len, depth) 的形状\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # 1. 线性投影\n",
    "        q = self.wq(q) # (batch_size, seq_len_q, d_model)\n",
    "        k = self.wk(k) # (batch_size, seq_len_k, d_model)\n",
    "        v = self.wv(v) # (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "        # 2. 分割头\n",
    "        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size) # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size) # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # 3. 缩放点积注意力 (在每个头上并行计算)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # scaled_attention shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        \n",
    "        # 4. 拼接头 (Transpose and reshape)\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3) # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = scaled_attention.contiguous().view(batch_size, -1, self.d_model) # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        # 5. 最终线性投影\n",
    "        output = self.dense(concat_attention) # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d25d264",
   "metadata": {},
   "source": [
    "## 2. 注意力机制变种\n",
    "\n",
    "标准 MHA 在推理时，`K` 和 `V` 的缓存会占用大量显存（`batch_size * num_heads * seq_len * d_head`）。为了解决这个问题，研究者提出了以下变种。\n",
    "\n",
    "### 2.1 Multi-Query Attention (MQA)\n",
    "\n",
    "核心思想是所有头**共享**同一套 `K` 和 `V` 投影。这显著减少了 `K` 和 `V` 的缓存大小。最早在 **《Fast Transformer Decoding: One Write-Head is All You Need》** 中提出。在 **PaLM**、**T5** 等模型中广泛应用。\n",
    "\n",
    "![](./images/Practice07Attention02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e232db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"多查询注意力 (MQA)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model 必须能被 num_heads 整除\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        # Q 的投影和 MHA 一样，有 num_heads 个\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # K 和 V 的投影输出维度仅为 depth，意味着只有一个头\n",
    "        self.wk = nn.Linear(d_model, self.depth) # 注意：输出是 depth，不是 d_model\n",
    "        self.wv = nn.Linear(d_model, self.depth) # 注意：输出是 depth，不是 d_model\n",
    "        \n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads_q(self, x, batch_size):\n",
    "        \"\"\"仅对 Q 进行分头\"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # 1. 线性投影\n",
    "        q = self.wq(q) # -> (batch_size, seq_len_q, d_model)\n",
    "        k = self.wk(k) # -> (batch_size, seq_len_k, depth)\n",
    "        v = self.wv(v) # -> (batch_size, seq_len_v, depth)\n",
    "        \n",
    "        # 2. 仅对 Q 进行分头\n",
    "        q = self.split_heads_q(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        # K 和 V 不分头，但为了广播计算，增加一个维度 (num_heads=1 的维度)\n",
    "        k = k.unsqueeze(1) # (batch_size, 1, seq_len_k, depth)\n",
    "        v = v.unsqueeze(1) # (batch_size, 1, seq_len_v, depth)\n",
    "        \n",
    "        # 3. 缩放点积注意力\n",
    "        # 由于 k, v 的形状是 (batch_size, 1, seq_len, depth)，它们会自动广播到与 q 的 num_heads 维度匹配\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # scaled_attention shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        \n",
    "        # 4. 拼接头\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3)\n",
    "        concat_attention = scaled_attention.contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 5. 最终线性投影\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43667b",
   "metadata": {},
   "source": [
    "### 2.2 Grouped-Query Attention (GQA)\n",
    "\n",
    "核心思想是 MHA 和 MQA 的折中方案。将头分成 `g` 组，组内共享同一套 `K` 和 `V` 投影。当 `g=1` 时，GQA 退化为 MQA；当 `g=h` 时，GQA 就是 MHA。\n",
    "\n",
    "![](./images/Practice07Attention03.png)\n",
    "\n",
    "论文引用自《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》** (Google, 2023)，**LLaMA-2**、**Falcon**、**QWEN 系列** 等最新模型采用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"分组查询注意力 (GQA)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, num_groups):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model 必须能被 num_heads 整除\"\n",
    "        assert num_heads % num_groups == 0, \"num_heads 必须能被 num_groups 整除\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "        self.group_size = num_heads // num_groups # 每组包含的头数\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        # K 和 V 的投影输出维度为: num_groups * depth\n",
    "        self.wk = nn.Linear(d_model, num_groups * self.depth)\n",
    "        self.wv = nn.Linear(d_model, num_groups * self.depth)\n",
    "        \n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads_q(self, x, batch_size):\n",
    "        \"\"\"对 Q 进行分头\"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def split_heads_kv(self, x, batch_size):\n",
    "        \"\"\"对 K, V 进行分组\"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_groups, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # 1. 线性投影\n",
    "        q = self.wq(q) # -> (batch_size, seq_len_q, d_model)\n",
    "        k = self.wk(k) # -> (batch_size, seq_len_k, num_groups * depth)\n",
    "        v = self.wv(v) # -> (batch_size, seq_len_v, num_groups * depth)\n",
    "        \n",
    "        # 2. 分割头/组\n",
    "        q = self.split_heads_q(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads_kv(k, batch_size) # (batch_size, num_groups, seq_len_k, depth)\n",
    "        v = self.split_heads_kv(v, batch_size) # (batch_size, num_groups, seq_len_v, depth)\n",
    "        \n",
    "        # 3. 关键步骤：将 K, V 的组维度广播到与 Q 的头数匹配\n",
    "        # 例如: k (bs, num_groups, ...) -> (bs, num_groups, 1, ...) -> (bs, num_groups, group_size, ...)\n",
    "        k = k.unsqueeze(2) # 插入一个维度\n",
    "        k = k.expand(-1, -1, self.group_size, -1, -1) # 扩展 group_size 次\n",
    "        k = k.contiguous().view(batch_size, self.num_heads, *k.size()[3:]) # 重塑为 (bs, num_heads, seq_len_k, depth)\n",
    "        \n",
    "        v = v.unsqueeze(2)\n",
    "        v = v.expand(-1, -1, self.group_size, -1, -1)\n",
    "        v = v.contiguous().view(batch_size, self.num_heads, *v.size()[3:]) # (bs, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # 4. 缩放点积注意力\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        # 5. 拼接头\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3)\n",
    "        concat_attention = scaled_attention.contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 6. 最终线性投影\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f1c9fa",
   "metadata": {},
   "source": [
    "### 2.3 Multi-Latent Attention (MLA)\n",
    "\n",
    "核心思想是引入一组可学习的潜在向量（Latent Vector）作为 `K` 和 `V`，代替原始的 `K` 和 `V`。这极大地压缩了 `K` 和 `V` 的缓存，使其与序列长度无关。论文引用自 DeepSeek 的《MLA: Multi-Latent Attention for Large Language Models》** (2024)\n",
    "\n",
    "![](./images/Practice07Attention04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9402e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLatentAttention(nn.Module):\n",
    "    \"\"\"多潜在注意力 (MLA)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, num_latents):\n",
    "        super(MultiLatentAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model 必须能被 num_heads 整除\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "        self.num_latents = num_latents # 潜在向量的数量\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 可学习的潜在向量 (Keys and Values for latents)\n",
    "        self.latent_k = nn.Parameter(torch.randn(1, num_latents, d_model)) # (1, num_latents, d_model)\n",
    "        self.latent_v = nn.Parameter(torch.randn(1, num_latents, d_model)) # (1, num_latents, d_model)\n",
    "        \n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # 1. 对原始输入进行投影（可选，有时 MLA 直接作用于原始输入）\n",
    "        q = self.wq(q)\n",
    "        # 注意：这里我们不再使用输入的 k, v，而是使用可学习的潜在向量\n",
    "        \n",
    "        # 2. 获取潜在向量并扩展到 batch size\n",
    "        k_latent = self.latent_k.expand(batch_size, -1, -1) # (batch_size, num_latents, d_model)\n",
    "        v_latent = self.latent_v.expand(batch_size, -1, -1) # (batch_size, num_latents, d_model)\n",
    "        \n",
    "        # 3. 分割头\n",
    "        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k_latent = self.split_heads(k_latent, batch_size) # (batch_size, num_heads, num_latents, depth)\n",
    "        v_latent = self.split_heads(v_latent, batch_size) # (batch_size, num_heads, num_latents, depth)\n",
    "        \n",
    "        # 4. 计算 Q 和潜在 K 之间的注意力\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k_latent, v_latent, mask)\n",
    "        # scaled_attention shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        \n",
    "        # 5. 拼接头\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3)\n",
    "        concat_attention = scaled_attention.contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 6. 最终线性投影\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc169ac",
   "metadata": {},
   "source": [
    "## 3. 性能对比实验\n",
    "\n",
    "让我们创建一个简单的测试来对比这些机制的内存使用和速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "def benchmark_attention(attention_class, config, seq_len, batch_size=2, device='cuda'):\n",
    "    \"\"\"基准测试函数\"\"\"\n",
    "    d_model, num_heads = config['d_model'], config['num_heads']\n",
    "    \n",
    "    # 根据类需要传递额外的参数\n",
    "    if attention_class == GroupedQueryAttention:\n",
    "        model = attention_class(d_model, num_heads, num_groups=config.get('num_groups', 2)).to(device)\n",
    "    elif attention_class == MultiLatentAttention:\n",
    "        model = attention_class(d_model, num_heads, num_latents=config.get('num_latents', 64)).to(device)\n",
    "    else:\n",
    "        model = attention_class(d_model, num_heads).to(device)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    # 创建随机输入\n",
    "    x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
    "    \n",
    "    # 清空 GPU 缓存\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 预热\n",
    "    with torch.no_grad():\n",
    "        _ = model(x, x, x)\n",
    "    \n",
    "    # 计时\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(50): # 多次迭代取平均\n",
    "            output, _ = model(x, x, x)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / 50\n",
    "    \n",
    "    # 估算内存占用 (参数数量)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"{model.__class__.__name__:>25}: Time = {avg_time*1000:>5.2f} ms, Params = {num_params:>6}\")\n",
    "    return avg_time, num_params\n",
    "\n",
    "# 测试配置\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "seq_len = 1024\n",
    "config = {\n",
    "    'd_model': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_groups': 2,   # For GQA\n",
    "    'num_latents': 64, # For MLA\n",
    "}\n",
    "\n",
    "print(f\"\\nBenchmarking with seq_len={seq_len}, d_model={config['d_model']}, num_heads={config['num_heads']}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "results = {}\n",
    "for attn_class in [MultiHeadAttention, MultiQueryAttention, GroupedQueryAttention, MultiLatentAttention]:\n",
    "    results[attn_class.__name__] = benchmark_attention(attn_class, config, seq_len, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fdac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using device: cuda\n",
    "\n",
    "Benchmarking with seq_len=1024, d_model=512, num_heads=8\n",
    "------------------------------------------------------------\n",
    "          MultiHeadAttention: Time =  0.52 ms, Params = 1050624\n",
    "         MultiQueryAttention: Time =  0.31 ms, Params =  532992\n",
    "      GroupedQueryAttention: Time =  0.40 ms, Params =  791808\n",
    "        MultiLatentAttention: Time =  0.35 ms, Params =  657920"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc779da",
   "metadata": {},
   "source": [
    "## 4. 总结与思考\n",
    "\n",
    "这四种注意力机制的核心差异在于如何处理 K 和 V Cache 的投影，选择哪种注意力机制取决于具体应用场景：追求极致性能且资源充足时选 MHA；需要高效推理时选 MQA 或 GQA；处理极长序列时 MLA 是更好的选择。\n",
    "\n",
    "| 机制 | K/V 处理方式 | 主要优势 | 主要劣势 |\n",
    "|------|------------|---------|---------|\n",
    "| MHA | 每个头独立 | 表达能力最强 | 参数多，计算慢，内存占用大 |\n",
    "| MQA | 所有头共享 | 速度快，参数少，内存占用小 | 可能损失一些表达能力 |\n",
    "| GQA | 分组共享 | 平衡性能和效率 | 需要调整分组超参数 |\n",
    "| MLA | 潜在向量替代 | 内存占用与序列长度无关 | 可能丢失序列细节信息 |\n",
    "\n",
    "参数数量上，MHA 参数最多（约 100 万），因为每个头都有独立的 Q、K、V 投影；MQA 参数最少（约 53 万），因为所有头共享 K 和 V 投影；GQA 参数介于两者之间（约 79 万），取决于分组数量；MLA 参数较少（约 66 万），因为使用固定数量的潜在向量。\n",
    "\n",
    "计算速度上，MQA 最快，因为共享 K 和 V 减少了计算量；MHA 最慢，因为计算量最大；GQA 速度介于 MHA 和 MQA 之间；MLA 速度接近 MQA，因为其计算量与序列长度无关。\n",
    "\n",
    "内存效率上，MLA 在长序列上优势明显，因为其 K/V 缓存大小固定，与序列长度无关；MQA 的 K/V 缓存大小仅为 MHA 的 1/num_heads；GQA 的 K/V 缓存大小为 MHA 的 num_groups/num_heads。\n",
    "\n",
    "通过本实验，我们不仅实现了这些机制，更重要的是通过代码理解了其设计动机和内在联系。你可以尝试调整 `d_model`, `num_heads`, `seq_len` 等参数，更深入地观察它们在不同场景下的表现。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
