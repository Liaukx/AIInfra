{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb02e00",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# 实战 Transformer 机器翻译\n",
    "\n",
    "author by: ZOMI\n",
    "\n",
    "本次实验将把之前实现的 Transformer 模型应用于真实的机器翻译任务，使用 [Multi30k 数据集](https://github.com/multi30k/dataset)。该数据集包含英德双语平行语料，句子长度适中（多为日常对话或短文本），适合验证 Transformer 在中低资源翻译任务中的效果。\n",
    "\n",
    "我们将引入训练过程的最佳实践，包括学习率调度、标签平滑、梯度裁剪等优化技巧，并实现贪婪搜索和束搜索算法进行推理解码，最后使用 BLEU 分数评估翻译质量。\n",
    "\n",
    "## 1. 环境准备与数据加载\n",
    "\n",
    "首先，导入必要的库并设置环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac2e3114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "from torchtext.datasets import Multi30k\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "# 设置随机种子以确保结果可重现\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e35117",
   "metadata": {},
   "source": [
    "### 1.1 加载和预处理数据\n",
    "\n",
    "使用 torchtext 库加载 Multi30k 数据集并进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fda7e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载 Multi30k 数据集...\n",
      "构建词汇表...\n",
      "源语言词汇表大小: 7853\n",
      "目标语言词汇表大小: 5893\n",
      "数据加载完成!\n"
     ]
    }
   ],
   "source": [
    "# 加载英语和德语的 spacy 模型用于分词\n",
    "try:\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "except OSError:\n",
    "    print(\"正在下载 spacy 模型...\")\n",
    "    try:\n",
    "        import urllib.request\n",
    "        urllib.request.urlopen(\"https://github.com\", timeout=3)\n",
    "        import spacy.cli as spacy_cli\n",
    "        spacy_cli.download('en_core_web_sm')\n",
    "        spacy_cli.download('de_core_news_sm')\n",
    "        spacy_en = spacy.load('en_core_web_sm')\n",
    "        spacy_de = spacy.load('de_core_news_sm')\n",
    "    except Exception as e:\n",
    "        print(f\"spacy 模型下载失败，改用简易分词器: {e}\")\n",
    "        spacy_en = spacy.blank('en')\n",
    "        spacy_de = spacy.blank('de')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]\n",
    "\n",
    "# 定义 Field 对象处理文本\n",
    "SRC = Field(tokenize=tokenize_de, \n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True,\n",
    "            batch_first=True)\n",
    "\n",
    "TRG = Field(tokenize=tokenize_en, \n",
    "            init_token='<sos>', \n",
    "            eos_token='<eos>', \n",
    "            lower=True,\n",
    "            batch_first=True)\n",
    "\n",
    "# 加载 Multi30k 数据集\n",
    "print(\"加载 Multi30k 数据集...\")\n",
    "DATA_ROOT = '/workspace/AIInfra/06AlgoData/01Basic/data'\n",
    "DATA_PATH = os.path.join(DATA_ROOT, 'multi30k')\n",
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "    exts=('.de', '.en'),\n",
    "    fields=(SRC, TRG),\n",
    "    path=DATA_PATH,\n",
    "    train='train',\n",
    "    validation='val',\n",
    "    test='test_2016_flickr'\n",
    ")\n",
    "\n",
    "# 构建词汇表\n",
    "print(\"构建词汇表...\")\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "print(f\"源语言词汇表大小: {len(SRC.vocab)}\")\n",
    "print(f\"目标语言词汇表大小: {len(TRG.vocab)}\")\n",
    "\n",
    "# 创建数据迭代器\n",
    "BATCH_SIZE = 128\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n",
    "\n",
    "print(\"数据加载完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522f33c",
   "metadata": {},
   "source": [
    "## 2. 模型构建与优化技术\n",
    "\n",
    "### 2.1 构建 Transformer 模型\n",
    "\n",
    "使用之前实现的 Transformer 模型，并针对机器翻译任务进行调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "452d3bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数量: 8,198,661\n"
     ]
    }
   ],
   "source": [
    "from transformer_components import Embedding, PositionalEncoding, MultiHeadAttention\n",
    "from transformer_components import FeedForward, SublayerConnection, EncoderLayer\n",
    "from transformer_components import DecoderLayer, Encoder, Decoder, Transformer, Generator\n",
    "from copy import deepcopy\n",
    "\n",
    "def make_model(src_vocab_size, trg_vocab_size, d_model=512, N=6, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    构建完整的 Transformer 模型\n",
    "    \"\"\"\n",
    "    attn = MultiHeadAttention(d_model, h, dropout)\n",
    "    ff = FeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    model = Transformer(\n",
    "        Encoder(EncoderLayer(d_model, attn, ff, dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, attn, attn, ff, dropout), N),\n",
    "        nn.Sequential(Embedding(src_vocab_size, d_model), deepcopy(position)),\n",
    "        nn.Sequential(Embedding(trg_vocab_size, d_model), deepcopy(position)),\n",
    "        Generator(d_model, trg_vocab_size)\n",
    "    )\n",
    "    \n",
    "    # Xavier 均匀初始化\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "            \n",
    "    return model\n",
    "\n",
    "# 创建模型\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "D_MODEL = 256\n",
    "N_LAYERS = 3\n",
    "HID_DIM = 512\n",
    "N_HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = make_model(INPUT_DIM, OUTPUT_DIM, D_MODEL, N_LAYERS, HID_DIM, N_HEADS, DROPOUT).to(device)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3f666",
   "metadata": {},
   "source": [
    "### 2.2 标签平滑 (Label Smoothing)\n",
    "\n",
    "标签平滑是一种正则化技术，通过软化硬标签来防止模型过度自信，提高泛化能力。\n",
    "\n",
    "原理公式：\n",
    "$$\n",
    "y_{\\text{smooth}} = (1 - \\epsilon) \\cdot y + \\frac{\\epsilon}{K}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66a46f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        assert 0.0 <= smoothing < 1.0\n",
    "        self.smoothing = smoothing\n",
    "        self.pad_idx = pad_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "\n",
    "    def forward(self, pred_log_probs, target):\n",
    "        pred = pred_log_probs.view(-1, pred_log_probs.size(-1))\n",
    "        tgt = target.view(-1)\n",
    "\n",
    "        non_pad_mask = tgt != self.pad_idx\n",
    "        valid_cnt = non_pad_mask.sum()\n",
    "        if valid_cnt == 0:\n",
    "            return torch.tensor(0.0, device=pred.device)\n",
    "\n",
    "        smooth_dist = pred.detach().clone()\n",
    "        smooth_dist.fill_(self.smoothing / (pred.size(1) - 2))\n",
    "        smooth_dist.scatter_(1, tgt.unsqueeze(1), self.confidence)\n",
    "        smooth_dist[:, self.pad_idx] = 0.0\n",
    "        smooth_dist[~non_pad_mask] = 0.0\n",
    "\n",
    "        loss = self.criterion(pred, smooth_dist)\n",
    "        return loss / valid_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9007ae",
   "metadata": {},
   "source": [
    "### 2.3 学习率调度 (Learning Rate Scheduling)\n",
    "\n",
    "Transformer 使用带 warmup 的学习率调度策略，先线性增加学习率，然后按步数的平方根反比衰减。\n",
    "\n",
    "原理公式：\n",
    "$$\n",
    "\\text{lrate} = d_{\\text{model}}^{-0.5} \\cdot \\min(\\text{step\\_num}^{-0.5}, \\text{step\\_num} \\cdot \\text{warmup\\_steps}^{-1.5})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "548e3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerOptimizer:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000, factor=1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.factor = factor\n",
    "        self.step_num = 0\n",
    "        self.lr = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self._get_lr()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "    def _get_lr(self):\n",
    "        lr = self.factor * (self.d_model ** -0.5) * \\\n",
    "             min(self.step_num ** -0.5, self.step_num * self.warmup_steps ** -1.5)\n",
    "        self.lr = lr\n",
    "        return lr\n",
    "\n",
    "# 创建优化器和学习率调度器\n",
    "optimizer = TransformerOptimizer(\n",
    "    optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9),\n",
    "    d_model=D_MODEL,\n",
    "    warmup_steps=2000,\n",
    "    factor=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b184264",
   "metadata": {},
   "source": [
    "### 2.4 梯度裁剪 (Gradient Clipping)\n",
    "\n",
    "梯度裁剪可以防止训练过程中梯度爆炸问题，提高训练稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "340305a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    \n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.data.mul_(clip_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2938a57",
   "metadata": {},
   "source": [
    "## 3. 训练与验证\n",
    "\n",
    "### 3.1 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "afd3e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        # 创建掩码\n",
    "        src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(1)\n",
    "        \n",
    "        tgt_in = trg[:, :-1]\n",
    "        B, L = tgt_in.size()\n",
    "        pad_mask = (tgt_in != TRG.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(1).repeat(1, 1, L, 1)\n",
    "        look_ahead = torch.triu(torch.ones((1, 1, L, L), device=device), diagonal=1) == 0\n",
    "        tgt_mask = pad_mask & look_ahead\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, tgt_in, src_mask, tgt_mask)\n",
    "        output = model.generator(output)\n",
    "        \n",
    "        output_flat = output.contiguous().view(-1, output.size(-1))\n",
    "        target_flat = trg[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output_flat, target_flat)\n",
    "        \n",
    "        loss.backward()\n",
    "        clip_gradients(model, clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"批次 {i}, 损失: {loss.item():.4f}, 学习率: {optimizer.lr:.6f}\")\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02560219",
   "metadata": {},
   "source": [
    "### 3.2 验证循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3966041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(1)\n",
    "            tgt_in = trg[:, :-1]\n",
    "            B, L = tgt_in.size()\n",
    "            pad_mask = (tgt_in != TRG.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(1).repeat(1, 1, L, 1)\n",
    "            look_ahead = torch.triu(torch.ones((1, 1, L, L), device=device), diagonal=1) == 0\n",
    "            tgt_mask = pad_mask & look_ahead\n",
    "\n",
    "            output = model(src, tgt_in, src_mask, tgt_mask)\n",
    "            output = model.generator(output)\n",
    "            \n",
    "            output_flat = output.contiguous().view(-1, output.size(-1))\n",
    "            target_flat = trg[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output_flat, target_flat)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a65f3b9",
   "metadata": {},
   "source": [
    "### 3.3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34e00774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD_IDX: 1\n",
      "SOS_IDX: 2\n",
      "EOS_IDX: 3\n"
     ]
    }
   ],
   "source": [
    "# 定义特殊 token 索引\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "SOS_IDX = TRG.vocab.stoi['<sos>']\n",
    "EOS_IDX = TRG.vocab.stoi['<eos>']\n",
    "\n",
    "criterion = LabelSmoothing(smoothing=0.1, pad_idx=PAD_IDX)\n",
    "\n",
    "print(f\"PAD_IDX: {PAD_IDX}\")\n",
    "print(f\"SOS_IDX: {SOS_IDX}\")\n",
    "print(f\"EOS_IDX: {EOS_IDX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c520aef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型...\n",
      "总训练步数（约）: 11350\n",
      "Warmup 步数: 2000\n",
      "------------------------------------------------------------\n",
      "批次 0, 损失: 7.4962, 学习率: 0.000001\n",
      "批次 100, 损失: 5.9606, 学习率: 0.000071\n",
      "批次 200, 损失: 4.4937, 学习率: 0.000140\n",
      "Epoch: 01 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 5.7926 | Train PPL: 327.873\n",
      "\t Val  Loss: 4.1621 |  Val PPL:  64.205\n",
      "\t当前学习率: 0.000159\n",
      "======================================================================\n",
      "批次 0, 损失: 4.2669, 学习率: 0.000159\n",
      "批次 100, 损失: 3.7943, 学习率: 0.000229\n",
      "批次 200, 损失: 3.3209, 学习率: 0.000299\n",
      "Epoch: 02 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 3.7030 | Train PPL:  40.570\n",
      "\t Val  Loss: 3.1395 |  Val PPL:  23.091\n",
      "\t当前学习率: 0.000317\n",
      "======================================================================\n",
      "批次 0, 损失: 3.3088, 学习率: 0.000318\n",
      "批次 100, 损失: 2.9615, 学习率: 0.000388\n",
      "批次 200, 损失: 2.9238, 学习率: 0.000458\n",
      "Epoch: 03 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 3.0572 | Train PPL:  21.267\n",
      "\t Val  Loss: 2.7050 |  Val PPL:  14.954\n",
      "\t当前学习率: 0.000476\n",
      "======================================================================\n",
      "批次 0, 损失: 2.7909, 学习率: 0.000477\n",
      "批次 100, 损失: 2.6827, 学习率: 0.000546\n",
      "批次 200, 损失: 2.5862, 学习率: 0.000616\n",
      "Epoch: 04 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 2.6765 | Train PPL:  14.534\n",
      "\t Val  Loss: 2.3942 |  Val PPL:  10.959\n",
      "\t当前学习率: 0.000634\n",
      "======================================================================\n",
      "批次 0, 损失: 2.5471, 学习率: 0.000635\n",
      "批次 100, 损失: 2.3132, 学习率: 0.000705\n",
      "批次 200, 损失: 2.3320, 学习率: 0.000775\n",
      "Epoch: 05 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 2.3588 | Train PPL:  10.578\n",
      "\t Val  Loss: 2.1412 |  Val PPL:   8.510\n",
      "\t当前学习率: 0.000793\n",
      "======================================================================\n",
      "批次 0, 损失: 1.9626, 学习率: 0.000794\n",
      "批次 100, 损失: 2.0325, 学习率: 0.000864\n",
      "批次 200, 损失: 2.0378, 学习率: 0.000934\n",
      "Epoch: 06 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 2.0684 | Train PPL:   7.913\n",
      "\t Val  Loss: 1.9324 |  Val PPL:   6.906\n",
      "\t当前学习率: 0.000952\n",
      "======================================================================\n",
      "批次 0, 损失: 1.7949, 学习率: 0.000952\n",
      "批次 100, 损失: 1.7585, 学习率: 0.001022\n",
      "批次 200, 损失: 1.8603, 学习率: 0.001092\n",
      "Epoch: 07 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 1.8178 | Train PPL:   6.158\n",
      "\t Val  Loss: 1.8108 |  Val PPL:   6.115\n",
      "\t当前学习率: 0.001110\n",
      "======================================================================\n",
      "批次 0, 损失: 1.6094, 学习率: 0.001111\n",
      "批次 100, 损失: 1.6689, 学习率: 0.001181\n",
      "批次 200, 损失: 1.7282, 学习率: 0.001251\n",
      "Epoch: 08 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 1.6460 | Train PPL:   5.186\n",
      "\t Val  Loss: 1.7294 |  Val PPL:   5.637\n",
      "\t当前学习率: 0.001269\n",
      "======================================================================\n",
      "批次 0, 损失: 1.5333, 学习率: 0.001270\n",
      "批次 100, 损失: 1.5089, 学习率: 0.001340\n",
      "批次 200, 损失: 1.5930, 学习率: 0.001392\n",
      "Epoch: 09 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 1.5218 | Train PPL:   4.580\n",
      "\t Val  Loss: 1.6975 |  Val PPL:   5.460\n",
      "\t当前学习率: 0.001383\n",
      "======================================================================\n",
      "批次 0, 损失: 1.4199, 学习率: 0.001382\n",
      "批次 100, 损失: 1.3186, 学习率: 0.001350\n",
      "批次 200, 损失: 1.4717, 学习率: 0.001319\n",
      "Epoch: 10 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 1.3995 | Train PPL:   4.053\n",
      "\t Val  Loss: 1.6798 |  Val PPL:   5.365\n",
      "\t当前学习率: 0.001312\n",
      "======================================================================\n",
      "批次 0, 损失: 1.2149, 学习率: 0.001312\n",
      "批次 100, 损失: 1.2571, 学习率: 0.001284\n",
      "批次 200, 损失: 1.3607, 学习率: 0.001257\n",
      "Epoch: 11 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 1.2844 | Train PPL:   3.612\n",
      "\t Val  Loss: 1.6616 |  Val PPL:   5.268\n",
      "\t当前学习率: 0.001251\n",
      "======================================================================\n",
      "批次 0, 损失: 1.1252, 学习率: 0.001251\n",
      "批次 100, 损失: 1.1853, 学习率: 0.001226\n",
      "批次 200, 损失: 1.2932, 学习率: 0.001203\n",
      "Epoch: 12 | Time: 0m 4s | ★ 新最佳模型\n",
      "\tTrain Loss: 1.1863 | Train PPL:   3.275\n",
      "\t Val  Loss: 1.6497 |  Val PPL:   5.205\n",
      "\t当前学习率: 0.001198\n",
      "======================================================================\n",
      "批次 0, 损失: 0.9808, 学习率: 0.001197\n",
      "批次 100, 损失: 1.1427, 学习率: 0.001176\n",
      "批次 200, 损失: 1.1605, 学习率: 0.001156\n",
      "Epoch: 13 | Time: 0m 4s | 未改善 (1/10)\n",
      "\tTrain Loss: 1.1013 | Train PPL:   3.008\n",
      "\t Val  Loss: 1.6555 |  Val PPL:   5.236\n",
      "\t当前学习率: 0.001151\n",
      "======================================================================\n",
      "批次 0, 损失: 1.0172, 学习率: 0.001150\n",
      "批次 100, 损失: 0.9979, 学习率: 0.001131\n",
      "批次 200, 损失: 1.1130, 学习率: 0.001113\n",
      "Epoch: 14 | Time: 0m 4s | 未改善 (2/10)\n",
      "\tTrain Loss: 1.0280 | Train PPL:   2.796\n",
      "\t Val  Loss: 1.6645 |  Val PPL:   5.283\n",
      "\t当前学习率: 0.001109\n",
      "======================================================================\n",
      "批次 0, 损失: 0.9026, 学习率: 0.001108\n",
      "批次 100, 损失: 0.9187, 学习率: 0.001091\n",
      "批次 200, 损失: 1.0472, 学习率: 0.001075\n",
      "Epoch: 15 | Time: 0m 4s | 未改善 (3/10)\n",
      "\tTrain Loss: 0.9634 | Train PPL:   2.620\n",
      "\t Val  Loss: 1.6965 |  Val PPL:   5.455\n",
      "\t当前学习率: 0.001071\n",
      "======================================================================\n",
      "批次 0, 损失: 0.8439, 学习率: 0.001071\n",
      "批次 100, 损失: 0.9021, 学习率: 0.001056\n",
      "批次 200, 损失: 0.9815, 学习率: 0.001041\n",
      "Epoch: 16 | Time: 0m 4s | 未改善 (4/10)\n",
      "\tTrain Loss: 0.9059 | Train PPL:   2.474\n",
      "\t Val  Loss: 1.6899 |  Val PPL:   5.419\n",
      "\t当前学习率: 0.001037\n",
      "======================================================================\n",
      "批次 0, 损失: 0.8500, 学习率: 0.001037\n",
      "批次 100, 损失: 0.8163, 学习率: 0.001023\n",
      "批次 200, 损失: 0.8863, 学习率: 0.001010\n",
      "Epoch: 17 | Time: 0m 4s | 未改善 (5/10)\n",
      "\tTrain Loss: 0.8545 | Train PPL:   2.350\n",
      "\t Val  Loss: 1.7016 |  Val PPL:   5.483\n",
      "\t当前学习率: 0.001006\n",
      "======================================================================\n",
      "批次 0, 损失: 0.7120, 学习率: 0.001006\n",
      "批次 100, 损失: 0.8309, 学习率: 0.000993\n",
      "批次 200, 损失: 0.8282, 学习率: 0.000981\n",
      "Epoch: 18 | Time: 0m 4s | 未改善 (6/10)\n",
      "\tTrain Loss: 0.8089 | Train PPL:   2.245\n",
      "\t Val  Loss: 1.7276 |  Val PPL:   5.627\n",
      "\t当前学习率: 0.000978\n",
      "======================================================================\n",
      "批次 0, 损失: 0.6852, 学习率: 0.000978\n",
      "批次 100, 损失: 0.7584, 学习率: 0.000966\n",
      "批次 200, 损失: 0.8353, 学习率: 0.000955\n",
      "Epoch: 19 | Time: 0m 4s | 未改善 (7/10)\n",
      "\tTrain Loss: 0.7682 | Train PPL:   2.156\n",
      "\t Val  Loss: 1.7511 |  Val PPL:   5.761\n",
      "\t当前学习率: 0.000952\n",
      "======================================================================\n",
      "批次 0, 损失: 0.6976, 学习率: 0.000952\n",
      "批次 100, 损失: 0.7054, 学习率: 0.000941\n",
      "批次 200, 损失: 0.8089, 学习率: 0.000930\n",
      "Epoch: 20 | Time: 0m 4s | 未改善 (8/10)\n",
      "\tTrain Loss: 0.7324 | Train PPL:   2.080\n",
      "\t Val  Loss: 1.7592 |  Val PPL:   5.808\n",
      "\t当前学习率: 0.000928\n",
      "======================================================================\n",
      "批次 0, 损失: 0.6719, 学习率: 0.000927\n",
      "批次 100, 损失: 0.6677, 学习率: 0.000917\n",
      "批次 200, 损失: 0.7389, 学习率: 0.000908\n",
      "Epoch: 21 | Time: 0m 4s | 未改善 (9/10)\n",
      "\tTrain Loss: 0.7000 | Train PPL:   2.014\n",
      "\t Val  Loss: 1.7896 |  Val PPL:   5.987\n",
      "\t当前学习率: 0.000905\n",
      "======================================================================\n",
      "批次 0, 损失: 0.5928, 学习率: 0.000905\n",
      "批次 100, 损失: 0.7171, 学习率: 0.000896\n",
      "批次 200, 损失: 0.7501, 学习率: 0.000887\n",
      "Epoch: 22 | Time: 0m 4s | 未改善 (10/10)\n",
      "\tTrain Loss: 0.6702 | Train PPL:   1.955\n",
      "\t Val  Loss: 1.8146 |  Val PPL:   6.139\n",
      "\t当前学习率: 0.000884\n",
      "======================================================================\n",
      "早停触发：验证损失连续 10 个 epoch 未改善\n",
      "\n",
      "训练完成！\n",
      "最佳验证损失: 1.6497\n",
      "最佳验证困惑度: 5.205\n"
     ]
    }
   ],
   "source": [
    "# 训练参数\n",
    "N_EPOCHS = 50\n",
    "CLIP = 1.0\n",
    "best_valid_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"开始训练模型...\")\n",
    "print(f\"总训练步数（约）: {len(train_iterator) * N_EPOCHS}\")\n",
    "print(f\"Warmup 步数: 2000\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    mins, secs = divmod(int(elapsed), 60)\n",
    "    \n",
    "    is_best = valid_loss < best_valid_loss\n",
    "    if is_best:\n",
    "        best_valid_loss = valid_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "        tag = \"★ 新最佳模型\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        tag = f\"未改善 ({patience_counter}/{patience})\"\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02d} | Time: {mins}m {secs}s | {tag}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train PPL: {math.exp(min(train_loss, 20)):7.3f}')\n",
    "    print(f'\\t Val  Loss: {valid_loss:.4f} |  Val PPL: {math.exp(min(valid_loss, 20)):7.3f}')\n",
    "    print(f'\\t当前学习率: {optimizer.lr:.6f}')\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"早停触发：验证损失连续 {patience} 个 epoch 未改善\")\n",
    "        break\n",
    "    \n",
    "print(\"\\n训练完成！\")\n",
    "print(f\"最佳验证损失: {best_valid_loss:.4f}\")\n",
    "print(f\"最佳验证困惑度: {math.exp(best_valid_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c173fd8",
   "metadata": {},
   "source": [
    "随着训练的进行，训练损失和验证损失应该逐渐下降，表明模型在学习翻译任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba79ca",
   "metadata": {},
   "source": [
    "## 4. 推理解码算法\n",
    "\n",
    "### 4.1 贪婪搜索 (Greedy Search)\n",
    "\n",
    "贪婪搜索在每一步选择概率最高的词作为当前输出，速度快但可能陷入局部最优。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a3b08713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    model.eval()\n",
    "    \n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    \n",
    "    for i in range(max_len-1):\n",
    "        L = ys.size(1)\n",
    "        pad_mask = (ys != TRG.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(1).repeat(1, 1, L, 1)\n",
    "        look_ahead = torch.triu(torch.ones((1, 1, L, L), device=device), diagonal=1) == 0\n",
    "        trg_mask = pad_mask & look_ahead\n",
    "        \n",
    "        out = model.decode(memory, src_mask, ys, trg_mask)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        \n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        \n",
    "        if next_word == TRG.vocab.stoi['<eos>']:\n",
    "            break\n",
    "            \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4c3e2",
   "metadata": {},
   "source": [
    "### 4.2 束搜索 (Beam Search)\n",
    "\n",
    "束搜索通过保留多个候选序列，能产生质量更高的翻译结果，但速度较慢。\n",
    "\n",
    "束搜索在每步保留`beam_size`个候选序列，通过多路径探索避免局部最优。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6c9a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, src, src_mask, max_len, start_symbol, beam_size, length_penalty=0.6):\n",
    "    model.eval()\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    memory = model.encode(src, src_mask)\n",
    "    beams = [([start_symbol], 0.0)]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        all_candidates = []\n",
    "        \n",
    "        for seq, score in beams:\n",
    "            if seq[-1] == TRG.vocab.stoi['<eos>']:\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "                \n",
    "            ys = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "            \n",
    "            L = ys.size(1)\n",
    "            pad_mask = (ys != TRG.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(1).repeat(1, 1, L, 1)\n",
    "            look_ahead = torch.triu(torch.ones((1, 1, L, L), device=device), diagonal=1) == 0\n",
    "            trg_mask = pad_mask & look_ahead\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = model.decode(memory, src_mask, ys, trg_mask)\n",
    "                prob = model.generator(out[:, -1])\n",
    "                log_prob = F.log_softmax(prob, dim=1)\n",
    "                \n",
    "            topk_prob, topk_idx = torch.topk(log_prob, beam_size, dim=1)\n",
    "            \n",
    "            for j in range(beam_size):\n",
    "                candidate_seq = seq + [topk_idx[0, j].item()]\n",
    "                candidate_score = score + topk_prob[0, j].item()\n",
    "                all_candidates.append((candidate_seq, candidate_score))\n",
    "                \n",
    "        ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "        beams = ordered[:beam_size]\n",
    "        \n",
    "        if all(seq[-1] == TRG.vocab.stoi['<eos>'] for seq, _ in beams):\n",
    "            break\n",
    "            \n",
    "    best_seq, best_score = max(\n",
    "        beams,\n",
    "        key=lambda x: x[1] / (len(x[0]) ** length_penalty),\n",
    "    )\n",
    "            \n",
    "    return best_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ee8be",
   "metadata": {},
   "source": [
    "## 5. 模型评估\n",
    "\n",
    "### 5.1 翻译函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f0589f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, src_field, trg_field, max_len=50, beam_size=5):\n",
    "    model.eval()\n",
    "\n",
    "    # 文本预处理\n",
    "    sentence = sentence.strip().lower()\n",
    "\n",
    "    # 分词 + 边界标记 + 数值化\n",
    "    tokenized = src_field.tokenize(sentence)\n",
    "    tokenized = [src_field.init_token] + tokenized + [src_field.eos_token]\n",
    "    numericalized = [src_field.vocab.stoi.get(token, src_field.vocab.stoi.get(src_field.unk_token, 0))\n",
    "                     for token in tokenized]\n",
    "\n",
    "    src_tensor = torch.LongTensor(numericalized).unsqueeze(0).to(device)\n",
    "    src_mask = (src_tensor != src_field.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "    # 调用束搜索解码\n",
    "    trg_indexes = beam_search_decode(\n",
    "        model,\n",
    "        src_tensor,\n",
    "        src_mask,\n",
    "        max_len,\n",
    "        trg_field.vocab.stoi[trg_field.init_token],\n",
    "        beam_size\n",
    "    )\n",
    "\n",
    "    # 索引转词\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    # 去掉边界符号\n",
    "    if trg_tokens and trg_tokens[0] == trg_field.init_token:\n",
    "        trg_tokens = trg_tokens[1:]\n",
    "    if trg_tokens and trg_tokens[-1] == trg_field.eos_token:\n",
    "        trg_tokens = trg_tokens[:-1]\n",
    "    \n",
    "    # 过滤特殊 token\n",
    "    trg_tokens = [tok for tok in trg_tokens \n",
    "                  if tok not in ['<unk>', '<pad>', trg_field.init_token, trg_field.eos_token]]\n",
    "\n",
    "    return ' '.join(trg_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718fea3",
   "metadata": {},
   "source": [
    "### 5.2 BLEU 分数评估\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) 是机器翻译中最常用的自动评估指标，通过比较机器翻译输出与参考翻译的 n-gram 重叠度来评估质量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "91a427d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算 BLEU 分数: 100%|██████████| 1000/1000 [02:32<00:00,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 分数: 26.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(data, model, src_field, trg_field, max_len=50, beam_size=5):\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for example in tqdm(data, desc=\"计算 BLEU 分数\"):\n",
    "        src = vars(example)['src']\n",
    "        trg = vars(example)['trg']\n",
    "        \n",
    "        trg = [trg_field.init_token] + trg + [trg_field.eos_token]\n",
    "        trgs.append([trg])\n",
    "        \n",
    "        pred_trg = translate_sentence(' '.join(src), model, src_field, trg_field, max_len, beam_size)\n",
    "        pred_trgs.append(pred_trg.split())\n",
    "    \n",
    "    smooth = SmoothingFunction().method4\n",
    "    bleu_score = corpus_bleu(trgs, pred_trgs, smoothing_function=smooth)\n",
    "    \n",
    "    return bleu_score\n",
    "\n",
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load('best-model.pt', weights_only=True))\n",
    "\n",
    "# 计算测试集 BLEU 分数\n",
    "bleu_score = calculate_bleu(test_data, model, SRC, TRG)\n",
    "print(f'BLEU 分数: {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c65792",
   "metadata": {},
   "source": [
    "### 5.3 推理翻译\n",
    "\n",
    "通过实际示例验证模型的翻译效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8fc96029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "翻译效果展示（图片描述类句子）\n",
      "======================================================================\n",
      "\n",
      "1. 德语: Ein Mann sitzt auf einer Bank\n",
      "   英语: a man sitting on a bench .\n",
      "\n",
      "2. 德语: Eine Frau läuft durch den Park\n",
      "   英语: a woman running through the park .\n",
      "\n",
      "3. 德语: Zwei Kinder spielen im Garten\n",
      "   英语: two children are playing in the yard .\n",
      "\n",
      "4. 德语: Ein Hund springt über einen Zaun\n",
      "   英语: a dog jumps over a fence .\n",
      "\n",
      "5. 德语: Menschen stehen vor einem Gebäude\n",
      "   英语: people standing in front of a building .\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"翻译效果展示（图片描述类句子）\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "examples = [\n",
    "    \"Ein Mann sitzt auf einer Bank\",\n",
    "    \"Eine Frau läuft durch den Park\",\n",
    "    \"Zwei Kinder spielen im Garten\",\n",
    "    \"Ein Hund springt über einen Zaun\",\n",
    "    \"Menschen stehen vor einem Gebäude\",\n",
    "]\n",
    "\n",
    "for i, sent in enumerate(examples, 1):\n",
    "    translation = translate_sentence(sent, model, SRC, TRG, max_len=60, beam_size=5)\n",
    "    print(f\"\\n{i}. 德语: {sent}\")\n",
    "    print(f\"   英语: {translation}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe186b",
   "metadata": {},
   "source": [
    "## 6. 总结\n",
    "\n",
    "在本实验中，我们完成了以下内容：\n",
    "\n",
    "1. 使用 torchtext 加载 Multi30k 数据集，并进行分词和词汇表构建\n",
    "2. 构建 Transformer 模型并应用优化技术（标签平滑、学习率调度、梯度裁剪）\n",
    "3. 实现贪婪搜索和束搜索解码算法\n",
    "4. 使用 BLEU 分数评估翻译质量\n",
    "\n",
    "本实验成功地将 Transformer 模型应用于英德翻译任务，验证了多种优化技术的有效性。这些技术对于训练高质量的大模型至关重要，也是深度学习实践中不可或缺的部分。\n",
    "\n",
    "你可以尝试调整超参数（如模型大小、学习率调度参数、束搜索宽度等），观察它们对翻译质量的影响，进一步加深对机器翻译任务的理解。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
