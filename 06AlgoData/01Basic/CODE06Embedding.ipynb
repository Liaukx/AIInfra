{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d42a11c0",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# 手把手实现核心机制 Embedding 词嵌入(DONE)\n",
    "\n",
    "Author by：ZOMI\n",
    "\n",
    "在 Transformer 模型中，Embedding（嵌入）机制是将离散的文本符号转换为连续的向量表示的关键步骤。除了词嵌入本身，位置信息的编码也至关重要，这是因为 Transformer 模型本身不具备对序列顺序的感知能力。\n",
    "\n",
    "![](./images/Practice06Embedding01.png)\n",
    "\n",
    "本文将实现 Transformer 中的核心嵌入机制，包括三种主流的位置编码方式：\n",
    "\n",
    "- APE（Absolute Position Embedding，绝对位置嵌入）\n",
    "- RPE（Relative Position Embedding，相对位置嵌入）\n",
    "- RoPE（Rotary Position Embedding，旋转位置嵌入）\n",
    "\n",
    "同时，会实现一个支持中英文的分词器，构建完整的文本到向量的转换流程。\n",
    "\n",
    "## 1. 准备工作\n",
    "\n",
    "首先，需要导入必要的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8708c",
   "metadata": {},
   "source": [
    "## 2. 实现中英文分词器\n",
    "\n",
    "在进行嵌入之前，需要一个支持中英文的分词器。这里基于之前实现的 BPE 算法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}  # 词汇到 ID 的映射\n",
    "        self.merges = {}  # 合并历史\n",
    "        self.pattern = re.compile(r'([^\\u4e00-\\u9fff\\w\\s])|(\\s+)')  # 匹配非中英文、非单词字符和空白\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.bos_token = \"<bos>\"\n",
    "        self.eos_token = \"<eos>\"\n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"预处理文本，分离中英文和特殊字符\"\"\"\n",
    "        tokens = self.pattern.split(text)\n",
    "        tokens = [t for t in tokens if t and t.strip() != '']\n",
    "        \n",
    "        processed = []\n",
    "        for token in tokens:\n",
    "            if re.match(r'[\\u4e00-\\u9fff]+', token):  # 中文\n",
    "                processed.append(' '.join(list(token)))\n",
    "            elif re.match(r'[a-zA-Z0-9]+', token):  # 英文/数字\n",
    "                processed.append(' '.join(list(token)) + ' </w>')\n",
    "            else:  # 特殊字符\n",
    "                processed.append(token)\n",
    "        \n",
    "        return ' '.join(processed)\n",
    "    \n",
    "    def get_pairs(self, word):\n",
    "        \"\"\"获取词内部的相邻字符对\"\"\"\n",
    "        pairs = set()\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs.add((prev_char, char))\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        \"\"\"训练 BPE 模型\"\"\"\n",
    "        # 预处理语料\n",
    "        processed_corpus = [self.preprocess(text) for text in corpus]\n",
    "        \n",
    "        # 统计每个词的出现次数\n",
    "        word_counts = Counter(processed_corpus)\n",
    "        \n",
    "        # 初始化词汇表：所有单个字符，加上特殊标记\n",
    "        vocab = {self.unk_token: 0, self.pad_token: 1, \n",
    "                self.bos_token: 2, self.eos_token: 3}\n",
    "        next_id = 4\n",
    "        \n",
    "        # 收集所有独特字符\n",
    "        chars = set()\n",
    "        for word in processed_corpus:\n",
    "            for char in word.split():\n",
    "                chars.add(char)\n",
    "        \n",
    "        # 添加字符到词汇表\n",
    "        for char in chars:\n",
    "            if char not in vocab:\n",
    "                vocab[char] = next_id\n",
    "                next_id += 1\n",
    "        \n",
    "        # 开始合并过程\n",
    "        current_vocab_size = len(vocab)\n",
    "        while current_vocab_size < self.vocab_size:\n",
    "            # 统计所有相邻字符对的出现次数\n",
    "            pairs = defaultdict(int)\n",
    "            for word, count in word_counts.items():\n",
    "                chars = word.split()\n",
    "                if len(chars) < 2:\n",
    "                    continue\n",
    "                for pair in self.get_pairs(chars):\n",
    "                    pairs[pair] += count\n",
    "            \n",
    "            if not pairs:\n",
    "                break  # 没有更多可合并的对\n",
    "            \n",
    "            # 找到出现次数最多的字符对\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # 合并最佳字符对\n",
    "            new_token = ''.join(best_pair)\n",
    "            if new_token in vocab:\n",
    "                current_vocab_size = len(vocab)\n",
    "                continue\n",
    "                \n",
    "            vocab[new_token] = next_id\n",
    "            next_id += 1\n",
    "            self.merges[best_pair] = new_token\n",
    "            \n",
    "            # 更新词表\n",
    "            word_counts = self._merge_pair(word_counts, best_pair, new_token)\n",
    "            current_vocab_size = len(vocab)\n",
    "            \n",
    "            if current_vocab_size % 100 == 0:\n",
    "                print(f\"当前词汇表大小: {current_vocab_size}/{self.vocab_size}\")\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        print(f\"BPE 训练完成，最终词汇表大小: {len(self.vocab)}\")\n",
    "    \n",
    "    def _merge_pair(self, word_counts, pair, new_token):\n",
    "        \"\"\"将词表中的指定字符对合并为新的条目\"\"\"\n",
    "        merged_word_counts = defaultdict(int)\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        \n",
    "        for word, count in word_counts.items():\n",
    "            merged_word = pattern.sub(new_token, word)\n",
    "            merged_word_counts[merged_word] += count\n",
    "        \n",
    "        return merged_word_counts\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"将文本转换为 token 列表\"\"\"\n",
    "        if not self.vocab:\n",
    "            raise ValueError(\"BPE 模型尚未训练，请先调用 train 方法\")\n",
    "        \n",
    "        # 预处理文本\n",
    "        processed = self.preprocess(text)\n",
    "        words = processed.split()\n",
    "        \n",
    "        # 对每个词应用合并规则\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            if len(word) == 1:  # 单个字符直接作为 token\n",
    "                tokens.append(word)\n",
    "                continue\n",
    "            \n",
    "            # 初始化字符列表\n",
    "            chars = list(word)\n",
    "            # 应用所有合并规则\n",
    "            for (a, b), new_token in self.merges.items():\n",
    "                i = 0\n",
    "                while i < len(chars) - 1:\n",
    "                    if chars[i] == a and chars[i+1] == b:\n",
    "                        # 合并这两个字符\n",
    "                        chars = chars[:i] + [new_token] + chars[i+2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "            \n",
    "            tokens.extend(chars)\n",
    "        \n",
    "        # 后处理：移除词尾标记中的空格\n",
    "        tokens = [token.replace(' </w>', '</w>') for token in tokens]\n",
    "\n",
    "        # 添加首尾标记\n",
    "        tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        return tokens\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"将 token 列表转换为 ID 列表\"\"\"\n",
    "        return [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "    \n",
    "    def __call__(self, text, max_length=None):\n",
    "        \"\"\"将文本转换为 ID 序列\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        ids = self.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # 截断或填充到指定长度\n",
    "        if max_length is not None:\n",
    "            if len(ids) > max_length:\n",
    "                ids = ids[:max_length]\n",
    "            else:\n",
    "                ids = ids + [self.vocab[self.pad_token]] * (max_length - len(ids))\n",
    "        \n",
    "        return torch.tensor(ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998dd57",
   "metadata": {},
   "source": [
    "这个 BPE 分词器相比之前的版本做了一些改进：\n",
    "\n",
    "- 增加了特殊标记（unk、pad、bos、eos）\n",
    "- 实现了 token 到 ID 的映射\n",
    "- 增加了`__call__`方法，可以直接将文本转换为 ID 张量\n",
    "- 支持指定最大长度，自动进行截断或填充\n",
    "\n",
    "## 3. 词嵌入（Word Embedding）\n",
    "\n",
    "词嵌入是将离散的 token ID 转换为连续的向量表示，是所有 Transformer 模型的基础组件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, padding_idx=None):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # 创建嵌入层\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: 形状为 [batch_size, seq_len] 的整数张量\n",
    "        返回: 形状为 [batch_size, seq_len, embedding_dim] 的词嵌入张量\n",
    "        \"\"\"\n",
    "        # 将 ID 转换为向量\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        \n",
    "        # 通常会对嵌入向量进行缩放\n",
    "        return embeddings * math.sqrt(self.embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839b174",
   "metadata": {},
   "source": [
    "词嵌入层的工作原理：\n",
    "\n",
    "1. 本质上是一个查找表（lookup table），将每个 token ID 映射到一个固定维度的向量\n",
    "2. 这些向量是可学习的参数，在训练过程中不断优化\n",
    "3. 通常会对嵌入向量进行缩放（乘以嵌入维度的平方根），这是 Transformer 原论文中的做法\n",
    "\n",
    "### 3.1 绝对位置嵌入（APE）\n",
    "\n",
    "绝对位置嵌入是 Transformer 原论文中使用的位置编码方式，直接将位置信息编码为固定或可学习的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embedding_dim, learnable=False):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        if learnable:\n",
    "            # 可学习的位置嵌入\n",
    "            self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        else:\n",
    "            # 固定的正弦余弦位置编码（Transformer 原论文）\n",
    "            position_embedding = torch.zeros(max_seq_len, embedding_dim)\n",
    "            position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * \n",
    "                                (-math.log(10000.0) / embedding_dim))\n",
    "            \n",
    "            # 偶数维度使用正弦函数\n",
    "            position_embedding[:, 0::2] = torch.sin(position * div_term)\n",
    "            \n",
    "            # 奇数维度使用余弦函数\n",
    "            position_embedding[:, 1::2] = torch.cos(position * div_term)\n",
    "            \n",
    "            # 注册为不可学习的参数\n",
    "            self.register_buffer('position_embedding', position_embedding.unsqueeze(0))\n",
    "        \n",
    "        self.learnable = learnable\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 形状为 [batch_size, seq_len, embedding_dim] 的词嵌入张量\n",
    "        返回: 形状相同的词嵌入+位置嵌入张量\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        if self.learnable:\n",
    "            # 生成位置索引 [0, 1, ..., seq_len-1]\n",
    "            positions = torch.arange(seq_len, device=x.device).expand(batch_size, -1)\n",
    "            pos_emb = self.position_embedding(positions)\n",
    "        else:\n",
    "            # 使用预计算的位置编码\n",
    "            pos_emb = self.position_embedding[:, :seq_len, :]\n",
    "        \n",
    "        # 将词嵌入和位置嵌入相加\n",
    "        return x + pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9055f3",
   "metadata": {},
   "source": [
    "绝对位置嵌入的特点：\n",
    "\n",
    "- 有两种实现方式：可学习的参数或固定的正弦余弦函数\n",
    "- 固定的正弦余弦编码具有更好的外推性，能处理比训练时更长的序列\n",
    "- 实现简单，直接将位置嵌入与词嵌入相加\n",
    "\n",
    "### 3.2. 相对位置嵌入（RPE）\n",
    "\n",
    "相对位置嵌入考虑的是 tokens 之间的相对距离，而不是绝对位置。这在许多场景下更符合直觉："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_relative_position=100):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_relative_position = max_relative_position\n",
    "        \n",
    "        # 相对位置范围: [-max_relative_position, max_relative_position]\n",
    "        # 为了用索引表示，偏移 max_relative_position，范围变为 [0, 2*max_relative_position]\n",
    "        self.relative_embedding = nn.Embedding(2 * max_relative_position + 1, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 形状为 [batch_size, seq_len, embedding_dim] 的词嵌入张量\n",
    "        返回: 包含相对位置信息的注意力偏置\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 生成相对位置矩阵\n",
    "        range_vec = torch.arange(seq_len, device=x.device)\n",
    "        range_mat = range_vec[:, None] - range_vec[None, :]  # [seq_len, seq_len]\n",
    "        \n",
    "        # 将相对位置限制在 [-max_relative_position, max_relative_position]\n",
    "        range_mat_clamped = torch.clamp(\n",
    "            range_mat, \n",
    "            -self.max_relative_position, \n",
    "            self.max_relative_position\n",
    "        )\n",
    "        \n",
    "        # 偏移到非负索引\n",
    "        relative_pos_ids = range_mat_clamped + self.max_relative_position\n",
    "        \n",
    "        # 获取相对位置嵌入\n",
    "        relative_pos_emb = self.relative_embedding(relative_pos_ids)  # [seq_len, seq_len, embedding_dim]\n",
    "        \n",
    "        # 计算相对位置注意力偏置\n",
    "        # [batch_size, seq_len, seq_len]\n",
    "        attention_bias = torch.matmul(x, relative_pos_emb.transpose(1, 2))\n",
    "        \n",
    "        return attention_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07216c7f",
   "metadata": {},
   "source": [
    "相对位置嵌入的特点：\n",
    "\n",
    "- 编码的是 token 之间的相对距离，而非绝对位置\n",
    "- 更符合语言的特性，因为词语间的关系更多取决于它们的相对位置\n",
    "- 通常作为注意力分数的偏置项使用，而不是直接与词嵌入相加\n",
    "\n",
    "### 3.3 旋转位置嵌入（RoPE）\n",
    "\n",
    "旋转位置嵌入（RoPE）是一种较新的位置编码方式，通过旋转操作将位置信息融入到词向量中，在许多大模型中表现出色：\n",
    "\n",
    "![](./images/Practice06Embedding02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=1024, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # 预计算频率\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        # 预计算位置编码\n",
    "        self._precompute_rotary_embeddings(max_seq_len)\n",
    "        \n",
    "    def _precompute_rotary_embeddings(self, max_seq_len):\n",
    "        \"\"\"预计算旋转位置编码\"\"\"\n",
    "        seq_len = max_seq_len\n",
    "        # 计算位置索引 [0, 1, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len, dtype=torch.float32)\n",
    "        \n",
    "        # 计算频率：[seq_len, dim/2]\n",
    "        freqs = torch.einsum('i,j->ij', positions, self.inv_freq)\n",
    "        \n",
    "        # 计算余弦和正弦值\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)  # [seq_len, dim]\n",
    "        self.register_buffer(\"cos_emb\", emb.cos())\n",
    "        self.register_buffer(\"sin_emb\", emb.sin())\n",
    "        \n",
    "    def rotate_half(self, x):\n",
    "        \"\"\"将 x 的后半部分旋转\"\"\"\n",
    "        x1 = x[..., :x.size(-1)//2]\n",
    "        x2 = x[..., x.size(-1)//2:]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 形状为 [batch_size, seq_len, dim] 的张量\n",
    "        返回: 加入旋转位置编码的张量\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        \n",
    "        # 如果序列长度超过预计算的最大值，则重新计算\n",
    "        if seq_len > self.max_seq_len:\n",
    "            self._precompute_rotary_embeddings(seq_len)\n",
    "            self.max_seq_len = seq_len\n",
    "        \n",
    "        # 获取当前序列长度的余弦和正弦值\n",
    "        cos = self.cos_emb[:seq_len, :].unsqueeze(0)  # [1, seq_len, dim]\n",
    "        sin = self.sin_emb[:seq_len, :].unsqueeze(0)  # [1, seq_len, dim]\n",
    "        \n",
    "        # 应用旋转位置编码: x * cos + rotate_half(x) * sin\n",
    "        return x * cos + self.rotate_half(x) * sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c3d74",
   "metadata": {},
   "source": [
    "RoPE 的核心原理：\n",
    "\n",
    "- 通过旋转操作将位置信息编码到词向量中\n",
    "- 满足旋转不变性，即相对位置编码只与相对距离有关\n",
    "- 在长文本处理上表现优异，已被应用于 LLaMA、GPT-NeoX 等模型\n",
    "\n",
    "## 4. 完整 Embedding 实现\n",
    "\n",
    "现在将词嵌入和三种位置嵌入组合起来，形成完整的嵌入模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3977810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, \n",
    "                 position_encoding_type=\"ape\", padding_idx=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 词嵌入层\n",
    "        self.word_embedding = WordEmbedding(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "        # 根据类型选择位置编码\n",
    "        self.position_encoding_type = position_encoding_type\n",
    "        \n",
    "        if position_encoding_type == \"ape\":\n",
    "            self.position_encoding = AbsolutePositionEmbedding(\n",
    "                max_seq_len=max_seq_len,\n",
    "                embedding_dim=embedding_dim,\n",
    "                learnable=False  # 使用 Transformer 原论文的正弦余弦编码\n",
    "            )\n",
    "        elif position_encoding_type == \"rpe\":\n",
    "            self.position_encoding = RelativePositionEmbedding(\n",
    "                embedding_dim=embedding_dim,\n",
    "                max_relative_position=max_seq_len // 2\n",
    "            )\n",
    "        elif position_encoding_type == \"rope\":\n",
    "            self.position_encoding = RotaryPositionEmbedding(\n",
    "                dim=embedding_dim,\n",
    "                max_seq_len=max_seq_len\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的位置编码类型: {position_encoding_type}\")\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: 形状为 [batch_size, seq_len] 的整数张量\n",
    "        返回: 嵌入结果和可能的注意力偏置\n",
    "        \"\"\"\n",
    "        # 获取词嵌入\n",
    "        word_emb = self.word_embedding(input_ids)\n",
    "        \n",
    "        # 应用位置编码\n",
    "        if self.position_encoding_type == \"rpe\":\n",
    "            # RPE 返回注意力偏置，词嵌入保持不变\n",
    "            attention_bias = self.position_encoding(word_emb)\n",
    "            return word_emb, attention_bias\n",
    "        else:\n",
    "            # APE 和 RoPE 直接修改词嵌入\n",
    "            embeddings = self.position_encoding(word_emb)\n",
    "            return embeddings, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427989e",
   "metadata": {},
   "source": [
    "这个完整的嵌入模块：\n",
    "\n",
    "- 集成了词嵌入和三种位置编码\n",
    "- 根据配置自动选择位置编码类型\n",
    "- 处理了不同位置编码的输出差异（有些返回嵌入，有些返回注意力偏置）\n",
    "\n",
    "## 5. 测试 Embedding\n",
    "\n",
    "让用中英文文本测试实现的 Embedding："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aecc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 准备训练数据（中英文混合）\n",
    "corpus = [\n",
    "    \"自然语言处理是人工智能的一个重要分支。\",\n",
    "    \"Transformer 模型在 NLP 领域取得了巨大成功。\",\n",
    "    \"RoPE 是一种高效的位置编码方式。\",\n",
    "    \"Word embedding converts tokens to vectors.\",\n",
    "    \"Attention is all you need.\",\n",
    "    \"Rotary position embedding improves long text understanding.\",\n",
    "    \"我爱自然语言处理和深度学习。\",\n",
    "    \"Python 是实现深度学习算法的好工具。\"\n",
    "]\n",
    "\n",
    "# 2. 训练 BPE 分词器\n",
    "bpe = BPE(vocab_size=500)\n",
    "bpe.train(corpus)\n",
    "\n",
    "# 3. 准备测试文本\n",
    "test_texts = [\n",
    "    \"Transformer 中的嵌入机制很重要。\",\n",
    "    \"Rotary Position Embedding is powerful.\"\n",
    "]\n",
    "\n",
    "# 4. 将文本转换为 ID\n",
    "max_length = 20\n",
    "input_ids = []\n",
    "for text in test_texts:\n",
    "    ids = bpe(text, max_length=max_length)\n",
    "    input_ids.append(ids)\n",
    "input_ids = torch.stack(input_ids)  # 形状: [2, 20]\n",
    "\n",
    "# 5. 测试三种位置编码\n",
    "embedding_dim = 128\n",
    "vocab_size = len(bpe.vocab)\n",
    "padding_idx = bpe.vocab[bpe.pad_token]\n",
    "\n",
    "# 测试 APE\n",
    "print(\"测试绝对位置嵌入（APE）:\")\n",
    "ape_embedding = TransformerEmbedding(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    max_seq_len=max_length,\n",
    "    position_encoding_type=\"ape\",\n",
    "    padding_idx=padding_idx\n",
    ")\n",
    "\n",
    "ape_output, _ = ape_embedding(input_ids)\n",
    "print(f\"APE 输出形状: {ape_output.shape}\")  # 应为 [2, 20, 128]\n",
    "\n",
    "# 测试 RPE\n",
    "print(\"\\n 测试相对位置嵌入（RPE）:\")\n",
    "rpe_embedding = TransformerEmbedding(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    max_seq_len=max_length,\n",
    "    position_encoding_type=\"rpe\",\n",
    "    padding_idx=padding_idx\n",
    ")\n",
    "\n",
    "rpe_output, rpe_bias = rpe_embedding(input_ids)\n",
    "print(f\"RPE 输出形状: {rpe_output.shape}\")  # 应为 [2, 20, 128]\n",
    "print(f\"RPE 注意力偏置形状: {rpe_bias.shape}\")  # 应为 [2, 20, 20]\n",
    "\n",
    "# 测试 RoPE\n",
    "print(\"\\n 测试旋转位置嵌入（RoPE）:\")\n",
    "rope_embedding = TransformerEmbedding(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    max_seq_len=max_length,\n",
    "    position_encoding_type=\"rope\",\n",
    "    padding_idx=padding_idx\n",
    ")\n",
    "rope_output, _ = rope_embedding(input_ids)\n",
    "print(f\"RoPE 输出形状: {rope_output.shape}\")  # 应为 [2, 20, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe33ce",
   "metadata": {},
   "source": [
    "运行上述测试代码，会得到类似以下的输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "当前词汇表大小: 100/500\n",
    "当前词汇表大小: 200/500\n",
    "当前词汇表大小: 300/500\n",
    "当前词汇表大小: 400/500\n",
    "当前词汇表大小: 500/500\n",
    "BPE 训练完成，最终词汇表大小: 500\n",
    "测试绝对位置嵌入（APE）:\n",
    "APE 输出形状: torch.Size([2, 20, 128])\n",
    "\n",
    "测试相对位置嵌入（RPE）:\n",
    "RPE 输出形状: torch.Size([2, 20, 128])\n",
    "RPE 注意力偏置形状: torch.Size([2, 20, 20])\n",
    "\n",
    "测试旋转位置嵌入（RoPE）:\n",
    "RoPE 输出形状: torch.Size([2, 20, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f8df4",
   "metadata": {},
   "source": [
    "结果表明 BPE 分词器成功训练并构建了包含 500 个 token 的词汇表，三种位置嵌入机制都能正常工作，并输出预期形状的张量。APE 和 RoPE 直接修改词嵌入向量，而 RPE 则生成额外的注意力偏置。\n",
    "\n",
    "## 6. 总结与思考\n",
    "\n",
    "| 位置编码类型 | 优点 | 缺点 | 适用场景 |\n",
    "|------------|------|------|---------|\n",
    "| APE（绝对） | 实现简单，计算高效 | 长序列外推性差，绝对位置意义有限 | 短文本任务，需要简单实现的场景 |\n",
    "| RPE（相对） | 更符合语言特性，关注相对关系 | 实现复杂，计算成本高 | 对上下文关系敏感的任务 |\n",
    "| RoPE（旋转） | 数学优雅，外推性好，计算高效 | 理解难度较大 | 长文本处理，LLM  |\n",
    "\n",
    "通过本文的实现，掌握了 Transformer 中核心的嵌入机制，包括：\n",
    "\n",
    "1. 支持中英文的 BPE 分词器，能够将原始文本转换为 token ID\n",
    "2. 词嵌入层，将离散 ID 转换为连续向量\n",
    "3. 三种主流的位置编码方式：APE、RPE 和 RoPE\n",
    "\n",
    "这些组件是构建 Transformer 模型的基础，理解它们的工作原理对于深入掌握 LLM 至关重要。在实际应用中，可以根据具体任务特点选择合适的位置编码方式。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
