{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6546cfb5",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra  )版权许可-->\n",
    "\n",
    "# 搭建迷你 Transformer(DONE)\n",
    "\n",
    "> Author by: lwh & ZOMI\n",
    "\n",
    "## Transformer 知识原理\n",
    "\n",
    "本节将从 Transformer 的核心定位出发，先介绍其与传统序列模型（如 RNN、LSTM）的核心差异，再拆解编码器的关键模块及各模块的设计目的——帮助读者建立“为什么需要这些模块”的认知，为后续代码实现奠定原理基础。\n",
    "\n",
    "Transformer 是一种基于 自注意力机制（Self-Attention）的 AI 模型结构，用于处理序列数据（如文本、语音、时间序列等）。相较于传统的循环神经网络（RNN、LSTM），Transformer 有以下几个核心优势：\n",
    "\n",
    "- 并行计算能力强：抛弃了序列依赖的结构，使训练更快。这是因为 RNN 类模型需按序列逐步计算，每个时间步的输出依赖前一步结果，而 Transformer 的自注意力机制可一次性处理整个序列，不存在时序依赖关系，能充分利用 GPU 的并行算力；\n",
    "- 长距离依赖建模能力强：自注意力机制可以直接建模任意位置间的关系。RNN/LSTM 需通过时序传递间接捕捉长距离关联，易出现信息衰减，而自注意力通过计算任意位置对的注意力权重，可直接捕捉远距离 token 的依赖；\n",
    "- 结构模块化：由多层编码器和解码器堆叠构成，易于扩展和修改。每个模块（如注意力、前馈网络）功能独立，可按需调整（如增加注意力头数、修改前馈网络维度）。\n",
    "\n",
    "在本实验中，仅关注编码器（Encoder）部分的精简实现，其结构主要由以下模块组成：\n",
    "\n",
    "1. 词嵌入（Embedding）：将输入的 token 序列（如词索引）映射为稠密向量，形成初始特征表示。该过程将离散的符号（如“猫”对应索引 5）转化为连续的向量空间表示，使模型能通过向量运算捕捉语义关联；\n",
    "2. 位置编码（Positional Encoding）：由于 Transformer 完全抛弃了循环结构，无法像 RNN 那样通过时序顺序感知位置，因此需要手动注入位置信息。本实验使用论文提出的正余弦位置编码，其核心优势在于周期性和泛化性——通过不同频率的三角函数，模型可区分不同位置的相对关系，且能自然适配训练时未见过的更长序列（因位置 pos+k 的编码可通过 pos 的编码推导，无需额外参数学习）；\n",
    "3. 自注意力机制（Self-Attention）：通过计算每个位置之间的关系（注意力权重），让模型自主学习哪些位置更重要。其本质是对序列的不同部分进行加权组合：对每个 token（Query），计算它与所有 token（Key）的匹配度，再用该匹配度对 token 的信息（Value）加权求和，最终得到融合全局上下文的新表示；\n",
    "4. 残差连接 + 层归一化（Residual + LayerNorm）：为了解决深层网络中梯度消失和训练不稳定的问题，在注意力层和前馈网络后分别添加残差连接与 LayerNorm。残差连接提供梯度传播“捷径”，避免梯度在深层传递中衰减；LayerNorm 通过对每个样本的特征维度做归一化（均值 0、方差 1），防止数据分布偏移，两者配合大幅提升模型的训练稳定性和表达能力；\n",
    "5. 前馈网络（Feed-Forward Network）：对每个位置的表示分别通过两层全连接网络，进一步提取特征并引入非线性表达。由于自注意力层是“全局关联”操作，前馈网络则聚焦于“单个位置的局部特征细化”，两者分工协作，让模型既懂全局关系，又能捕捉局部细节。\n",
    "\n",
    "![](./images/Practice01MiniTranformer01.png)\n",
    "\n",
    "最终，编码器的每个模块都以“子层 → 残差连接 → 层归一化”的方式组成结构块，构成了一个可堆叠的 Transformer 编码器框架——堆叠层数越多，模型捕捉复杂模式的能力越强（但训练成本也越高）。\n",
    "\n",
    "## Transformer 编码实现\n",
    "\n",
    "本节将基于 PyTorch 逐步实现迷你 Transformer 编码器，从基础工具库导入开始，依次实现位置编码、缩放点积注意力、编码器类，并通过假数据测试模型功能。每段代码都会结合原理解释“为什么这么写”，帮助读者将代码与理论对应。\n",
    "\n",
    "首先导入所需的 PyTorch 和数学库：PyTorch 提供张量操作和神经网络模块，math 库用于计算缩放因子等数学操作，这是实现后续模块的基础依赖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae998158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db238a5b",
   "metadata": {},
   "source": [
    "与论文一致使用固定的正余弦位置编码方式。注意编码维度必须为偶数——这是因为编码需将维度平分为两部分，分别填充正弦（sin）和余弦（cos）值，确保每个位置的编码在不同维度上有差异化的频率特征，从而精准区分序列中不同位置的 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_pos_encoding(seq_len: int, d_model: int) -> torch.Tensor:\n",
    "    if d_model % 2 != 0:\n",
    "        raise ValueError(\"d_model 必须为偶数\")\n",
    "\n",
    "    # 生成位置向量和维度向量：pos 对应序列中每个 token 的位置（0 到 seq_len-1），i 对应编码的维度索引（0 到 d_model//2-1）\n",
    "    pos = torch.arange(0, seq_len).unsqueeze(1).float() # shape: (seq_len, 1)，增加维度以便后续广播\n",
    "    i = torch.arange(0, d_model // 2).float() # shape: (d_model/2,)，对应 sin/cos 的频率参数\n",
    "\n",
    "    # 计算频率除数项：10000^(2i/d_model)，频率随维度索引 i 增大而降低（高频对应短距离位置区分，低频对应长距离）\n",
    "    denom = torch.pow(10000, 2 * i / d_model) # shape: (d_model/2,)\n",
    "    angle = pos / denom # shape: (seq_len, d_model/2)，通过广播实现每个位置与每个频率的组合\n",
    "\n",
    "    # 初始化编码矩阵：最终输出为(seq_len, d_model)，每个位置对应 d_model 维的编码\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "    # 填入 sin 和 cos：偶数维度用 sin（捕捉位置的“快速变化”特征），奇数维度用 cos（捕捉位置的“缓慢变化”特征）\n",
    "    pe[:, 0::2] = torch.sin(angle)  # 偶数维度（0,2,4...）\n",
    "    pe[:, 1::2] = torch.cos(angle)  # 奇数维度（1,3,5...）\n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9096eb20",
   "metadata": {},
   "source": [
    "这个函数返回一个 `(seq_len, d_model)` 的位置编码张量，用于为输入序列添加位置信息。该位置编码函数完成后，就可以进入 Transformer 的核心：注意力机制的实现。\n",
    "\n",
    "这里实现的是点积注意力机制，它会使用三个输入张量：\n",
    "\n",
    "- `q`（Query）：表示当前 token 想“关注什么”（如当前 token 的特征向量）；\n",
    "- `k`（Key）：表示序列中每个位置的“关键词”（如每个 token 的特征向量，用于与 Query 匹配）；\n",
    "- `v`（Value）：表示每个位置实际携带的“信息内容”（如每个 token 的详细特征，将根据注意力权重被加权）。\n",
    "\n",
    "注意力机制的核心逻辑是：计算 `q` 与所有 `k` 的匹配程度（打分），通过 softmax 将打分转化为注意力权重（权重和为 1，代表每个位置的重要性占比），最后用权重对 `v` 加权求和，得到融合了全局上下文的新表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v):\n",
    "\n",
    "    # 计算注意力打分矩阵：q 与 k 的转置点积（捕捉 q 与每个 k 的匹配度），然后除以 sqrt(d_k) 进行缩放\n",
    "    # d_k 是 q/k 的最后一维维度，缩放的目的是：当 d_k 较大时，点积结果方差会增大，导致 softmax 输出过于陡峭（梯度消失）\n",
    "    # 除以 sqrt(d_k)可将方差稳定在 1 左右，保障注意力权重的合理分布\n",
    "    scores = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))  # shape: (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # 对打分矩阵使用 softmax，得到注意力权重：权重在 seq_len_k 维度求和为 1，代表每个 k 对当前 q 的重要性\n",
    "    weights = torch.softmax(scores, dim=-1)  # shape: (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # 使用注意力权重加权求和 v，得到注意力输出：融合了所有 k 位置的信息，且突出了重要位置\n",
    "    return weights @ v  # shape: (..., seq_len_q, d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b5664",
   "metadata": {},
   "source": [
    "接下来构建一个迷你版的 Transformer 编码器类 `MiniTransformerEncoder`，它整合了前面实现的核心模块，形成一个完整的编码器单元。该类包含以下核心模块：\n",
    "\n",
    "- 嵌入层（Embedding）：将输入的 token 索引映射为 d_model 维向量，是模型的初始特征输入；\n",
    "- 注意力层（Self-Attention）：通过线性层将嵌入向量映射为 q、k、v，再调用缩放点积注意力，提取全局上下文关联；\n",
    "- 前馈网络（Feedforward）：采用“升维-非线性-降维”结构，对每个位置的特征独立细化（与注意力层的“全局关联”形成互补）；\n",
    "- 层归一化（LayerNorm）：分别应用在注意力子层和前馈子层之后，配合残差连接缓解梯度消失，保障训练稳定。\n",
    "\n",
    "![](./images/Practice01MiniTranformer02.png)\n",
    "\n",
    "此外，为每个位置添加了固定的正余弦位置编码（Positional Encoding），使模型可以识别 token 的先后顺序。需要注意：直接对词嵌入张量 `x_embed` 与位置编码 `pe` 使用加法（`x = x_embed + pe`），这是因为：\n",
    "\n",
    "- `x_embed` 的形状是 `(batch_size, seq_len, d_model)`（本实验中 batch_size=1）；\n",
    "- `pe` 的形状是 `(seq_len, d_model)`；\n",
    "- PyTorch 的广播机制会自动将 `pe` 扩展为 `(batch_size, seq_len, d_model)`，确保两者维度匹配后逐元素相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # 词嵌入层：将 vocab_size 个 token 映射为 d_model 维向量\n",
    "        self.linear_q = nn.Linear(d_model, d_model)         # Q 映射层：将嵌入向量转化为 Query（适配注意力计算）\n",
    "        self.linear_k = nn.Linear(d_model, d_model)         # K 映射层：将嵌入向量转化为 Key\n",
    "        self.linear_v = nn.Linear(d_model, d_model)         # V 映射层：将嵌入向量转化为 Value\n",
    "        self.attn_output = nn.Linear(d_model, d_model)      # 注意力输出映射：将注意力结果投影回 d_model 维（增强表达能力）\n",
    "        self.norm1 = nn.LayerNorm(d_model)                  # 第一个 LayerNorm：用于注意力子层后的归一化\n",
    "\n",
    "        # 前馈网络：两层全连接+ReLU 激活，中间维度设为 d_model*2（升维以捕捉更复杂特征）\n",
    "        self.ffn = nn.Sequential(                           \n",
    "            nn.Linear(d_model, d_model * 2),  # 升维：从 d_model 到 d_model*2\n",
    "            nn.ReLU(),                        # 非线性激活：引入非线性，提升模型拟合能力\n",
    "            nn.Linear(d_model * 2, d_model)   # 降维：回到 d_model，与残差连接维度匹配\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)                  # 第二个 LayerNorm：用于前馈子层后的归一化\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)  # 获取序列长度（x 形状为(batch_size, seq_len)）\n",
    "        x_embed = self.embedding(x)  # 词嵌入：(batch_size, seq_len) → (batch_size, seq_len, d_model)\n",
    "        # 生成位置编码并添加到嵌入向量：注入位置信息\n",
    "        pe = sinusoidal_pos_encoding(seq_len, x_embed.size(-1))\n",
    "        x = x_embed + pe  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # 自注意力子层：计算全局上下文关联\n",
    "        q = self.linear_q(x)  # Query 映射：(batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        k = self.linear_k(x)  # Key 映射\n",
    "        v = self.linear_v(x)  # Value 映射\n",
    "        attn = scaled_dot_product_attention(q, k, v)  # 注意力计算：(batch_size, seq_len, d_model)\n",
    "        # 残差连接 + LayerNorm：x（原始输入）与注意力结果相加，再归一化（缓解梯度消失）\n",
    "        x = self.norm1(x + self.attn_output(attn))\n",
    "\n",
    "        # 前馈子层：细化每个位置的局部特征\n",
    "        ff = self.ffn(x)  # 前馈计算：(batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        # 残差连接 + LayerNorm：再次通过残差和归一化稳定训练\n",
    "        x = self.norm2(x + ff)\n",
    "        return x  # 输出：(batch_size, seq_len, d_model)，每个位置的上下文特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1659d",
   "metadata": {},
   "source": [
    "这一结构就是一个基本的 Transformer 编码器块，具备捕捉上下文关联、感知位置信息、细化局部特征的核心能力，是更复杂 Transformer 模型（如 BERT、GPT）的基础单元。\n",
    "\n",
    "为了测试模型输出是否符合预期，我们用一个假输入 `[3, 1, 7]`（代表序列长度为 3 的 token 索引）来运行前向传播。这里 batch_size=1（单条样本），模型会输出每个 token 的上下文特征向量，我们通过打印输出验证模型功能是否正常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7050e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型：词汇表大小 50（假设包含 50 个不同 token），模型维度 d_model=16（小维度便于测试）\n",
    "model = MiniTransformerEncoder(vocab_size=50, d_model=16)\n",
    "# 假输入：shape 为[1, 3] → (batch_size=1, seq_len=3)\n",
    "dummy_input = torch.tensor([[3, 1, 7]])  \n",
    "# 前向传播：获取模型输出\n",
    "output = model(dummy_input)\n",
    "print(\"=== 输出 ===\")\n",
    "print(output.detach().numpy())  #  detach() 脱离计算图，避免梯度干扰，转为 numpy 便于查看\n",
    "print(\"=== 结束 ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "=== 输出 ===\n",
    "[[[-1.5413152   1.6801366   0.05804453  0.24212095 -1.0774754\n",
    "    0.02384599 -0.12485133 -0.6640581  -0.88564116  1.0022273\n",
    "   -1.7447253  -0.11834528 -0.32912147  0.8888434   1.3979812\n",
    "    1.1923339 ]\n",
    "  [ 0.7319907   0.89123964  0.20003487 -0.07367807 -1.0843173\n",
    "    0.2291747  -1.023271    0.09876721 -0.5554284   0.05615921\n",
    "   -0.26637512 -1.0664971   0.01956824  1.8496377  -1.9267299\n",
    "    1.9197246 ]\n",
    "  [ 1.0861049  -0.38979262  1.8310152   0.47594324 -0.07559342\n",
    "   -0.44681358 -1.166736    1.6186275  -1.5705074  -0.19102472\n",
    "    0.40794823  1.0515163  -1.3678914  -0.19411024 -1.1033916\n",
    "    0.03470579]]]\n",
    "=== 结束 ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0c814",
   "metadata": {},
   "source": [
    "从输出结果可见，模型成功输出了形状为 `(1, 3, 16)` 的张量（对应 `batch_size=1, seq_len=3, d_model=16`），每个 token 都得到了 16 维的上下文特征向量——这表明迷你编码器的所有模块都正常工作，实现了预期功能。至此完成了一个最小的 Transformer 编码器搭建。它结构清晰、功能完整，非常适合用作 Transformer 学习的入门代码框架。\n",
    "\n",
    "## 总结与思考\n",
    "\n",
    "本实验通过“原理拆解→代码实现→功能验证”的流程，完成了迷你 Transformer 编码器的搭建。明确了 Transformer 编码器的核心模块分工——词嵌入负责“离散转连续”，位置编码解决“时序感知”，自注意力捕捉“全局关联”，残差+LayerNorm 保障“训练稳定”，前馈网络细化“局部特征”，各模块协同实现序列建模。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
