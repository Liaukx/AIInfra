1
00:00:00,000 --> 00:00:02,300
内容/录制:Z0MI 酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,433 --> 00:00:03,100
哎呀哈喽

3
00:00:03,133 --> 00:00:04,900
大家好我是那个

4
00:00:04,900 --> 00:00:06,166
猫会喵喵叫

5
00:00:06,200 --> 00:00:07,333
狗会汪汪叫

6
00:00:07,333 --> 00:00:09,333
我会一给我了 giaogiao 的

7
00:00:09,333 --> 00:00:10,200
ZOMI

8
00:00:10,200 --> 00:00:12,366
一给我 giaogiao

9
00:00:12,366 --> 00:00:13,566
我最近不知道为什么

10
00:00:13,566 --> 00:00:15,000
giaogiao 这个特别火

11
00:00:15,333 --> 00:00:16,466
今天我们来到了

12
00:00:16,466 --> 00:00:19,833
整个 MOE 的核心原理的可视化

13
00:00:19,833 --> 00:00:21,500
划重点了划重点了

14
00:00:21,533 --> 00:00:24,000
这里面叫做 MOE 的核心原理

15
00:00:24,000 --> 00:00:25,233
最核心的内容

16
00:00:25,566 --> 00:00:27,633
那在整个 MOE 的核心原理

17
00:00:27,633 --> 00:00:29,033
其实 ZOMI 只是

18
00:00:29,033 --> 00:00:30,300
没有太多自己的内容了

19
00:00:30,333 --> 00:00:31,366
更多的是解读

20
00:00:31,366 --> 00:00:34,400
网上就是 YouTube 的这一篇视频

21
00:00:34,400 --> 00:00:35,566
那我们现在来打开一下

22
00:00:35,566 --> 00:00:36,766
YouTube 这个内容

23
00:00:37,233 --> 00:00:37,866
那其实

24
00:00:37,866 --> 00:00:39,533
ZOMI 主要还是在解读

25
00:00:39,566 --> 00:00:41,433
YouTube 这一个

26
00:00:41,433 --> 00:00:46,066
A Visual Guide to Mixture of Experts (MoE) in LLMs

27
00:00:46,066 --> 00:00:48,300
这么一个 19 分钟的视频了

28
00:00:48,333 --> 00:00:49,133
那这个视频

29
00:00:49,133 --> 00:00:50,733
可能 ZOMI 会解读的时间

30
00:00:50,733 --> 00:00:52,000
会比他相对长一点

31
00:00:52,000 --> 00:00:54,366
因为毕竟我是中文

32
00:00:54,666 --> 00:00:56,866
是吧那在整个内容里面

33
00:00:56,866 --> 00:00:58,866
可能 ZOMI 做一个简单的划分

34
00:00:58,866 --> 00:00:59,333
首先

35
00:00:59,366 --> 00:01:02,266
我们会看一下整个 MOE 的基本的原理

36
00:01:02,266 --> 00:01:04,766
因为之前有小朋友告诉 ZOMI

37
00:01:04,800 --> 00:01:05,066
哎呀

38
00:01:05,066 --> 00:01:07,733
ZOMI 你讲了一些的或者一堆的论文了

39
00:01:07,933 --> 00:01:10,366
实际上我是看不懂或者搞不明白

40
00:01:10,400 --> 00:01:11,966
在整个 moe 的架构

41
00:01:11,966 --> 00:01:13,600
或者 moe 到底长什么样子

42
00:01:13,600 --> 00:01:15,833
因此今天我们重点来去看看

43
00:01:15,833 --> 00:01:17,833
moe 的核心的论文

44
00:01:17,833 --> 00:01:21,900
或 moe 的核心架构里面的相关的细节

45
00:01:22,000 --> 00:01:22,600
那同样

46
00:01:22,600 --> 00:01:23,633
我们刚才说到了

47
00:01:23,633 --> 00:01:24,933
有了基本的原理之后

48
00:01:24,966 --> 00:01:27,033
我们看一下整个 moe 的路由

49
00:01:27,366 --> 00:01:28,366
什么叫路由

50
00:01:28,366 --> 00:01:30,000
MOE 的路由到底是指啥

51
00:01:30,000 --> 00:01:31,533
为什么要有路由

52
00:01:31,533 --> 00:01:33,133
那讲完路由之后

53
00:01:33,133 --> 00:01:36,066
就看一下整个网络模型的架构了

54
00:01:36,066 --> 00:01:38,666
就是整个 moetransformar

55
00:01:38,666 --> 00:01:41,833
再加上 FFN 层是怎么去组成的

56
00:01:41,833 --> 00:01:43,333
那有了很多专家之后

57
00:01:43,366 --> 00:01:45,800
我们需要对专家进行均衡负载

58
00:01:45,800 --> 00:01:47,800
那均衡负载到底意味着什么

59
00:01:47,966 --> 00:01:50,266
均衡负载的计算是怎么算的

60
00:01:50,266 --> 00:01:52,333
具体的数学是怎么表示的

61
00:01:52,400 --> 00:01:54,733
那有了军行负载的专家很多

62
00:01:54,733 --> 00:01:56,800
现在我们在真正推理的时候

63
00:01:56,800 --> 00:01:58,766
会选择具体的专家

64
00:01:58,766 --> 00:02:01,200
所以我们这里面就要叫做 Keep Top-K

65
00:02:01,733 --> 00:02:04,433
选择最核心的前 k 个专家

66
00:02:04,433 --> 00:02:06,433
来进行学习对应的内容

67
00:02:06,433 --> 00:02:07,466
在推理的时候

68
00:02:07,466 --> 00:02:09,933
就选择这几个专家进行推理

69
00:02:10,000 --> 00:02:11,800
但是在训练的过程当中

70
00:02:11,800 --> 00:02:14,000
我们还是要对那么多专家

71
00:02:14,000 --> 00:02:16,233
做一个辅助损失的

72
00:02:16,233 --> 00:02:18,533
因为我们既有很多专家

73
00:02:18,566 --> 00:02:20,333
专家之间要做均衡负载

74
00:02:20,333 --> 00:02:23,233
又要选择几个专家进行专门的学习

75
00:02:23,233 --> 00:02:23,900
所以这里面

76
00:02:23,933 --> 00:02:26,866
我们就提出了一个辅助损失的内容

77
00:02:27,033 --> 00:02:28,333
当然了专家

78
00:02:28,366 --> 00:02:30,933
为了保证我们的一个训练的稳定性

79
00:02:30,933 --> 00:02:32,600
所以加了一个专家容量

80
00:02:32,733 --> 00:02:34,866
避免某些专家学的特别多

81
00:02:34,866 --> 00:02:36,533
某些专家学的特别的少

82
00:02:36,566 --> 00:02:37,466
那整体的内容

83
00:02:37,466 --> 00:02:39,866
我们就会分开这几个

84
00:02:39,866 --> 00:02:41,700
跟大家去过过的

85
00:02:41,733 --> 00:02:43,333
或者跟大家去介绍的

86
00:02:43,533 --> 00:02:45,600
那回到整个系列视频

87
00:02:45,600 --> 00:02:46,266
整个 moe

88
00:02:46,266 --> 00:02:48,533
可能 ZOMI 会分开十几个小视频

89
00:02:48,600 --> 00:02:50,233
跟大家一起去学习的

90
00:02:50,233 --> 00:02:52,033
那最后还会跟大家一起手撕

91
00:02:52,033 --> 00:02:53,666
一个 moe 的代码

92
00:02:54,066 --> 00:02:56,700
也就是除了了解原理

93
00:02:56,733 --> 00:02:58,166
看对应的论文

94
00:02:58,166 --> 00:03:00,433
看他的所有的相关的架构

95
00:03:00,433 --> 00:03:02,366
然后会自己手撸一个

96
00:03:02,400 --> 00:03:04,933
然后分布一些可能简单的小作业

97
00:03:04,933 --> 00:03:06,600
供大家去学习的

98
00:03:06,600 --> 00:03:09,266
也希望大家持续性的关注 ZOMI

99
00:03:09,966 --> 00:03:11,666
我们现在来到了第一个内容

100
00:03:11,666 --> 00:03:14,300
看一下 introduction MOE 的基本原理

101
00:03:14,433 --> 00:03:15,133
其实说实话

102
00:03:15,133 --> 00:03:17,000
ZOMI 也不知道为什么要录这么多视频

103
00:03:17,000 --> 00:03:19,766
不过既然开始了就不要结束了吧

104
00:03:19,766 --> 00:03:20,866
一直坚持着吧

105
00:03:23,266 --> 00:03:25,766
那我们现在来看一下整个 MOE 专家

106
00:03:25,800 --> 00:03:26,266
实际上

107
00:03:26,266 --> 00:03:28,033
所谓的专家

108
00:03:28,133 --> 00:03:30,733
主要是指我们这里面的内容

109
00:03:30,733 --> 00:03:31,800
那这些专家

110
00:03:31,800 --> 00:03:32,433
实际上

111
00:03:32,433 --> 00:03:33,066
主要是

112
00:03:33,066 --> 00:03:35,500
提供我们一个网络模型的性能了

113
00:03:35,533 --> 00:03:36,266
因为以前

114
00:03:36,266 --> 00:03:37,966
我们就一个 FFN 层

115
00:03:38,000 --> 00:03:40,166
也就是一个简单的神经网络

116
00:03:40,200 --> 00:03:42,166
现在我们把这个 FFN

117
00:03:42,166 --> 00:03:45,033
拓展到很多个 FFN 里面

118
00:03:45,033 --> 00:03:46,866
然后让不同的专家

119
00:03:46,866 --> 00:03:48,833
去学习不同的知识

120
00:03:48,966 --> 00:03:49,733
那这里面

121
00:03:49,733 --> 00:03:51,166
在整个 MOE 架构里面

122
00:03:51,166 --> 00:03:53,366
有两个主要的元素

123
00:03:53,366 --> 00:03:54,600
或有两个主要的内容

124
00:03:54,600 --> 00:03:55,033
第一个

125
00:03:55,033 --> 00:03:56,366
就是所谓的专家

126
00:03:56,400 --> 00:03:57,233
那这里面的专家

127
00:03:57,233 --> 00:03:59,100
我们刚才讲到了 Expert

128
00:03:59,133 --> 00:03:59,633
实际上

129
00:03:59,633 --> 00:04:01,466
它不是所谓的什么一个概念

130
00:04:01,566 --> 00:04:04,800
它是神经网络里面的一种计算

131
00:04:04,800 --> 00:04:06,633
或者一个神经网络层

132
00:04:06,766 --> 00:04:07,466
那这个层

133
00:04:07,466 --> 00:04:08,766
我们叫 FFN

134
00:04:08,800 --> 00:04:09,533
这个 FFN

135
00:04:09,533 --> 00:04:12,866
就是 fit forward network 前馈神经网络

136
00:04:12,866 --> 00:04:16,033
就说白了就是一个简单的矩阵层

137
00:04:16,333 --> 00:04:18,000
每一个或者每一个 expert

138
00:04:18,000 --> 00:04:20,633
都有大量的一个矩阵参数在字里面

139
00:04:20,633 --> 00:04:22,533
进行学习不同的内容的

140
00:04:23,000 --> 00:04:23,266
第二个

141
00:04:23,266 --> 00:04:24,900
我们叫做路由

142
00:04:24,933 --> 00:04:25,533
那路由

143
00:04:25,533 --> 00:04:26,066
有时候

144
00:04:26,066 --> 00:04:27,633
也叫做 gaten network

145
00:04:27,633 --> 00:04:29,333
也就是我们的门控网络

146
00:04:29,366 --> 00:04:31,433
因为在最原始

147
00:04:31,433 --> 00:04:33,433
在 Hinton 的那篇文章里面

148
00:04:33,433 --> 00:04:34,266
就说到了

149
00:04:34,266 --> 00:04:36,233
它其实是一个门控网络

150
00:04:36,433 --> 00:04:37,766
通过一个开关

151
00:04:37,800 --> 00:04:40,066
去决定我们到底什么时候

152
00:04:40,066 --> 00:04:42,233
去选择不同的专家

153
00:04:42,233 --> 00:04:42,833
所以说

154
00:04:42,833 --> 00:04:43,300
这里面

155
00:04:43,333 --> 00:04:44,866
就变成了一个门控网络

156
00:04:44,866 --> 00:04:46,666
但是随着我们的一个文章的

157
00:04:46,666 --> 00:04:49,533
引进和大家对整个计算的认识

158
00:04:49,566 --> 00:04:51,133
我们这里面就把它变成

159
00:04:51,133 --> 00:04:52,533
或者叫做 WOT 了

160
00:04:52,733 --> 00:04:53,533
这个 rotor

161
00:04:53,533 --> 00:04:55,466
就主要是指路由去决定

162
00:04:55,466 --> 00:04:57,533
我们书进来的那个 Token

163
00:04:57,566 --> 00:04:59,000
就是我们那个词源

164
00:04:59,066 --> 00:05:02,633
到底选择哪个专家进行计算和学习的

165
00:05:02,633 --> 00:05:05,133
那这个就是路由主要的作用

166
00:05:05,433 --> 00:05:07,300
其实在整个 LLM 

167
00:05:07,333 --> 00:05:09,833
特别是 MOE 的一个 LLM 

168
00:05:09,833 --> 00:05:10,900
加固里面

169
00:05:10,933 --> 00:05:14,066
我们每一层都会选择不同的专家

170
00:05:14,066 --> 00:05:15,100
所以每一层

171
00:05:15,133 --> 00:05:16,866
都会有不同的一个路由

172
00:05:16,866 --> 00:05:18,133
去做一个选择的

173
00:05:18,166 --> 00:05:18,866
那这里面

174
00:05:18,866 --> 00:05:22,100
蛮有意思的就是举一个具体的例子了

175
00:05:22,533 --> 00:05:22,833
实际上

176
00:05:22,833 --> 00:05:25,033
我们很多人去理解所谓的专家

177
00:05:25,033 --> 00:05:26,100
就理解可能

178
00:05:26,133 --> 00:05:28,266
这里面每一层的每一个专家

179
00:05:28,266 --> 00:05:30,933
或者每一个小的网络模型

180
00:05:30,966 --> 00:05:32,966
具体学的是不同的知识

181
00:05:33,033 --> 00:05:34,566
例如专家一

182
00:05:34,600 --> 00:05:37,266
可能学的是一个 psychology 的知识

183
00:05:37,266 --> 00:05:38,500
然后专家 3

184
00:05:38,533 --> 00:05:40,333
学的是 Biology 的知识

185
00:05:40,600 --> 00:05:41,233
每个专家

186
00:05:41,233 --> 00:05:43,533
都有自己的学习的能力和内容

187
00:05:43,566 --> 00:05:45,866
但是在真正执行的环境当中

188
00:05:45,866 --> 00:05:47,966
或者真真正正数学计算

189
00:05:48,000 --> 00:05:49,233
和我们的模型结构

190
00:05:49,233 --> 00:05:52,466
真正学的不是所谓的这些专家的知识

191
00:05:52,466 --> 00:05:55,533
而是学习一个 syntactic information

192
00:05:55,566 --> 00:05:57,566
所谓的一个句法的信息

193
00:05:57,566 --> 00:05:58,600
语法的信息

194
00:05:59,066 --> 00:06:00,933
词源的关系的信息

195
00:06:01,033 --> 00:06:03,033
所以说没有所谓的专家一

196
00:06:03,033 --> 00:06:05,100
专门学习一个 psychology

197
00:06:05,133 --> 00:06:06,166
没有专家二

198
00:06:06,166 --> 00:06:08,200
专门学习 Biology 相关的知识

199
00:06:08,200 --> 00:06:10,800
他们更多的是共同的去学习

200
00:06:10,800 --> 00:06:12,266
我们的一个词源

201
00:06:12,266 --> 00:06:15,333
学习我们的 Token 相关的一个信息

202
00:06:15,366 --> 00:06:17,000
进入我们的数据的信息

203
00:06:17,033 --> 00:06:18,166
那我们现在来看一下

204
00:06:18,200 --> 00:06:19,966
整个路由的过程当中

205
00:06:19,966 --> 00:06:22,866
首先我们有一个具体的 Token 输进

206
00:06:22,866 --> 00:06:24,233
来我们的 input

207
00:06:24,233 --> 00:06:25,566
然后经过一个路由

208
00:06:25,600 --> 00:06:28,033
就会选择到某一个专家

209
00:06:28,033 --> 00:06:29,833
那这一层选择这个专家

210
00:06:30,033 --> 00:06:30,466
下一层

211
00:06:30,466 --> 00:06:33,066
我们同样有路由选择另外一个专家

212
00:06:33,066 --> 00:06:35,266
在下一层选择另外一个专家

213
00:06:35,266 --> 00:06:38,733
所以说如果我们纯粹的专家一

214
00:06:38,766 --> 00:06:40,800
是学习 psychology 的知识

215
00:06:40,800 --> 00:06:41,533
那可能

216
00:06:41,533 --> 00:06:43,566
每一次 psychology 的数据来的时候

217
00:06:43,566 --> 00:06:46,066
我都是清一色的选择 expert 1

218
00:06:46,400 --> 00:06:46,866
那但是

219
00:06:46,866 --> 00:06:49,366
在真正计算的时候并不是这个概念

220
00:06:49,400 --> 00:06:50,066
所以这里面

221
00:06:50,066 --> 00:06:50,900
就纠正很多人

222
00:06:50,933 --> 00:06:53,733
对专家理解的一个简单的概念

223
00:06:53,766 --> 00:06:55,266
那现在我们来看一下

224
00:06:55,266 --> 00:06:56,300
刚才

225
00:06:56,333 --> 00:06:59,266
只是简单的解释了 Moe

226
00:06:59,266 --> 00:07:00,633
这个专家的原理

227
00:07:00,633 --> 00:07:02,533
但实际上我们看一下 Moe

228
00:07:02,566 --> 00:07:04,833
在整个 transoMOE 的加工里面

229
00:07:05,000 --> 00:07:07,800
只是里面的很小的一个模块

230
00:07:07,800 --> 00:07:08,366
所以说

231
00:07:08,366 --> 00:07:09,833
我们现在来去看一下

232
00:07:09,833 --> 00:07:11,300
整个 transformer 架构

233
00:07:11,333 --> 00:07:14,333
或者整个 LLM LLM 加 moe

234
00:07:14,333 --> 00:07:15,466
是怎么去发展的

235
00:07:15,466 --> 00:07:17,700
那最主要的是左边的虚化

236
00:07:17,733 --> 00:07:19,766
这个图我们现在来看一下

237
00:07:19,766 --> 00:07:21,466
首先我们去回顾一下

238
00:07:21,466 --> 00:07:23,633
最标准的 transformer 的架构

239
00:07:23,633 --> 00:07:25,066
里面的 decoder only

240
00:07:25,533 --> 00:07:26,733
那所谓的 decode only

241
00:07:26,733 --> 00:07:29,333
就是我们输进去很多个 Token

242
00:07:29,366 --> 00:07:30,433
Token1 Token2

243
00:07:30,433 --> 00:07:31,666
Token3 Token 四

244
00:07:31,666 --> 00:07:33,433
每个 Token 在中文里面

245
00:07:33,433 --> 00:07:35,333
对应的是一个单词或者是一个字

246
00:07:35,366 --> 00:07:36,133
在英文里面

247
00:07:36,133 --> 00:07:38,400
是可能是真正的对应一个单词

248
00:07:38,433 --> 00:07:39,833
那有了 Token 之后

249
00:07:39,833 --> 00:07:40,500
那么 Token

250
00:07:40,533 --> 00:07:43,400
每一次会输进去我们的 position in bending

251
00:07:43,400 --> 00:07:45,333
把我们对应的一些 Token

252
00:07:45,333 --> 00:07:46,966
化成一个向量

253
00:07:47,066 --> 00:07:47,700
那这个项链

254
00:07:47,733 --> 00:07:49,933
就会丢给我们的一个传送门的 decorder

255
00:07:49,933 --> 00:07:52,766
然后 decorder 是由很多层去堆

256
00:07:52,766 --> 00:07:53,433
叠的所以

257
00:07:53,433 --> 00:07:54,433
我们这里面可以看到

258
00:07:54,433 --> 00:07:56,233
很多层 decorder 的 Block

259
00:07:56,233 --> 00:07:58,166
那最终输出那个 Token

260
00:07:58,400 --> 00:07:59,066
那这些 Token

261
00:07:59,066 --> 00:08:00,233
把它拼在一起

262
00:08:00,233 --> 00:08:02,566
就变成我们完整的一句话了

263
00:08:02,600 --> 00:08:03,200
那下面

264
00:08:03,200 --> 00:08:04,000
我们打开一下

265
00:08:04,000 --> 00:08:07,633
每个 decode Block 里面的一些具体的细节了

266
00:08:07,633 --> 00:08:09,300
那整个 decode Block 里面

267
00:08:09,333 --> 00:08:11,366
首先我们对输入的数据

268
00:08:11,366 --> 00:08:12,633
进行一个规划

269
00:08:12,633 --> 00:08:13,133
后面

270
00:08:13,166 --> 00:08:16,166
因为在我们在之前的视频里面讲到了

271
00:08:16,166 --> 00:08:17,800
以前前大家都会用 layer Norm

272
00:08:17,800 --> 00:08:19,600
现在都改成 IMS Norm 了

273
00:08:19,600 --> 00:08:20,600
那不管怎么样

274
00:08:20,600 --> 00:08:22,033
Norm 就是 nomalization

275
00:08:22,033 --> 00:08:24,133
对我们的数据进行归依化

276
00:08:24,200 --> 00:08:26,566
为什么输进去的时候要进行归一化

277
00:08:26,566 --> 00:08:28,433
是因为我们经过计算之后

278
00:08:28,433 --> 00:08:31,266
你会发现有些数据参数量很高

279
00:08:31,266 --> 00:08:32,900
有些数据的参数量很低

280
00:08:32,933 --> 00:08:33,866
有正有负

281
00:08:34,033 --> 00:08:35,866
所以我们希望把所有的数据

282
00:08:35,866 --> 00:08:38,700
都拉到一条平均线上或者平行线上

283
00:08:38,733 --> 00:08:40,066
所以我们都会对数据

284
00:08:40,066 --> 00:08:41,433
进行一个归一化

285
00:08:41,433 --> 00:08:42,166
归一化之后

286
00:08:42,200 --> 00:08:45,200
就进行整个 Transformer 架构最核心的计算

287
00:08:45,200 --> 00:08:47,366
也就是 attention 的计算

288
00:08:47,366 --> 00:08:48,733
自注意力机制

289
00:08:49,066 --> 00:08:50,066
整个 attention

290
00:08:50,066 --> 00:08:50,233
之后

291
00:08:50,233 --> 00:08:53,133
attention 是 QKV 的相乘

292
00:08:53,166 --> 00:08:54,566
算完 QKV 之后

293
00:08:54,566 --> 00:08:56,333
经过一个稠密的计算

294
00:08:56,333 --> 00:08:57,533
那这个稠密的计算

295
00:08:57,533 --> 00:08:59,233
在整个 Transformer 加构的时候

296
00:08:59,266 --> 00:09:02,733
主要是去更好的捕捉我们词源

297
00:09:02,766 --> 00:09:05,566
或者我们 Token 句子相关的信息

298
00:09:05,600 --> 00:09:07,866
那在真正捕捉相关的信息的时候

299
00:09:07,866 --> 00:09:10,066
我们看一下左边的这个图的展开

300
00:09:10,400 --> 00:09:11,733
在整个 FFN 层

301
00:09:11,733 --> 00:09:14,333
实际上它是一个非常稠密的模型

302
00:09:14,333 --> 00:09:17,233
也就是简单的一个前馈神经网络

303
00:09:17,233 --> 00:09:18,266
对应输入的 Token

304
00:09:18,266 --> 00:09:19,900
然后经过一个非常复杂的

305
00:09:19,933 --> 00:09:21,400
我们首先它不是单层的

306
00:09:21,400 --> 00:09:23,200
神经网络而是多层的

307
00:09:23,433 --> 00:09:25,733
一开始可能我们的一个 inbending size

308
00:09:25,766 --> 00:09:29,633
只有 512 然后对 inbending size 扩充到 2048

309
00:09:29,633 --> 00:09:30,066
然后

310
00:09:30,066 --> 00:09:32,966
再对我们的 inbending size 进行一个压缩

311
00:09:33,000 --> 00:09:34,133
然后变成 512

312
00:09:34,133 --> 00:09:36,233
再进行输出我们的 Token

313
00:09:36,233 --> 00:09:37,733
那通过这种方式

314
00:09:37,766 --> 00:09:39,466
去使得我们的网络模型

315
00:09:39,466 --> 00:09:42,733
学习到更多的一个密集相关的信息

316
00:09:43,133 --> 00:09:45,433
那有了这个相关的内容之后

317
00:09:45,566 --> 00:09:48,666
现在我们把这些非常稠密的信息

318
00:09:48,666 --> 00:09:50,566
然后做一个分割

319
00:09:50,733 --> 00:09:53,466
那每一个类型或者每一个参数量

320
00:09:53,466 --> 00:09:55,433
我们就分开不同的 export

321
00:09:55,566 --> 00:09:58,133
所以说把整个 FFN 层

322
00:09:58,133 --> 00:10:00,333
就切分成很多个小 group

323
00:10:00,666 --> 00:10:01,566
所以我们这里面

324
00:10:01,600 --> 00:10:04,533
就把这些小故事本叫做每一个专家

325
00:10:04,600 --> 00:10:07,766
专家是学习模型里面的信息

326
00:10:07,766 --> 00:10:09,866
而不是学习所谓的我们认知的这种

327
00:10:09,866 --> 00:10:11,300
刚才讲到的 biology

328
00:10:11,333 --> 00:10:13,266
psychology 相关的信息

329
00:10:13,266 --> 00:10:13,966
那这里面

330
00:10:14,000 --> 00:10:15,200
我们就对专家

331
00:10:15,200 --> 00:10:17,866
或者对我们的神经元进行分组

332
00:10:18,033 --> 00:10:19,900
从而得到我们的一个专家

333
00:10:19,933 --> 00:10:21,666
那我们可以看一下这里面

334
00:10:21,666 --> 00:10:23,633
因为我们需要去分组了

335
00:10:23,633 --> 00:10:25,733
所以说真正去做连接的时候

336
00:10:25,766 --> 00:10:27,066
某一个时间段

337
00:10:27,266 --> 00:10:28,966
上面的三个神经元

338
00:10:29,000 --> 00:10:31,200
只跟 ASP4 进行连接

339
00:10:31,266 --> 00:10:33,333
可能另外一个时间线

340
00:10:33,533 --> 00:10:34,600
三个神经元

341
00:10:34,600 --> 00:10:36,933
就跟我们另外一个 ASP 进行连接

342
00:10:36,933 --> 00:10:37,733
那这个时候

343
00:10:37,733 --> 00:10:40,133
我们叫做 spares model

344
00:10:40,433 --> 00:10:41,433
稀疏的模型

345
00:10:41,433 --> 00:10:42,866
而不是全连接

346
00:10:42,933 --> 00:10:44,333
全连接的那种方式

347
00:10:44,333 --> 00:10:45,966
我们叫做稠密的模型

348
00:10:45,966 --> 00:10:47,466
也就这种是稠密的

349
00:10:47,466 --> 00:10:49,033
那现在我们分组了

350
00:10:49,033 --> 00:10:51,266
每次只连接模型参数量里面

351
00:10:51,266 --> 00:10:52,533
神经元的一部分

352
00:10:52,566 --> 00:10:54,933
所以我们叫做稀疏的模型

353
00:10:54,933 --> 00:10:56,733
稀疏的 s 的架构

354
00:10:57,566 --> 00:10:57,933
但是

355
00:10:57,933 --> 00:11:00,466
每一个专家都是一个完整的 FFN

356
00:11:00,466 --> 00:11:03,133
所以我们会把一个简单的小参数

357
00:11:03,166 --> 00:11:03,833
进行扩大

358
00:11:03,833 --> 00:11:04,833
然后再减少

359
00:11:04,833 --> 00:11:06,533
然后就变成这种方式

360
00:11:06,566 --> 00:11:08,033
来去做一个实现的

361
00:11:08,266 --> 00:11:10,566
而且这里面我们看一下真正的

362
00:11:10,600 --> 00:11:12,566
我们刚才讲到了有路由

363
00:11:12,666 --> 00:11:14,733
那我们的一开始的一个 Tok

364
00:11:14,766 --> 00:11:15,866
会经过一个路由

365
00:11:15,866 --> 00:11:18,466
给到我们的可能专家室进行计算

366
00:11:18,466 --> 00:11:19,500
然后进行输出

367
00:11:19,533 --> 00:11:20,666
那其他专家

368
00:11:20,666 --> 00:11:22,533
我们会常驻在内存里面

369
00:11:22,566 --> 00:11:24,366
虽然当前没有去选择

370
00:11:24,366 --> 00:11:25,566
没有激活去使用

371
00:11:25,566 --> 00:11:27,766
但是它是常驻在内存里面的

372
00:11:27,766 --> 00:11:29,200
可能在下一层

373
00:11:29,233 --> 00:11:31,066
会使用到这一层的砖家

374
00:11:31,066 --> 00:11:31,833
或下一次

375
00:11:31,833 --> 00:11:34,900
就使用到不同的砖家进行计算了

376
00:11:35,166 --> 00:11:36,633
我们现在回过头来看一下

377
00:11:36,633 --> 00:11:39,033
在整个全松板加工里面的 decodebock

378
00:11:39,200 --> 00:11:42,200
每次真正的就选择我们 top cake ASP

379
00:11:42,233 --> 00:11:44,033
当然这里面是以 top one

380
00:11:44,133 --> 00:11:46,400
就只选择一个专家作为一个试例

381
00:11:46,400 --> 00:11:48,266
最终得到我们的输出例

382
00:11:48,266 --> 00:11:50,300
我们输入的是 what is one

383
00:11:50,333 --> 00:11:52,200
plus one 然后这里面

384
00:11:52,200 --> 00:11:54,333
就经过一系列的专家的计算

385
00:11:54,333 --> 00:11:57,000
最终得到一个答案叫做 two 了

386
00:11:57,000 --> 00:11:58,233
那这么一个内容

387
00:11:58,266 --> 00:12:00,633
中间的这个路由的选择

388
00:12:00,633 --> 00:12:01,933
我们叫做 Paff

389
00:12:02,066 --> 00:12:02,566
有真正的

390
00:12:02,600 --> 00:12:04,066
我们经过很多个 PAP

391
00:12:04,066 --> 00:12:05,300
很多条路径

392
00:12:05,366 --> 00:12:07,933
去得到我们最终的答案的

393
00:12:07,933 --> 00:12:09,266
然后每一次

394
00:12:09,266 --> 00:12:11,500
因为我们在整个 LLM 的预测

395
00:12:11,533 --> 00:12:14,933
是预测下一个 Token 的最大的概率

396
00:12:14,933 --> 00:12:15,800
所以可以看到

397
00:12:15,800 --> 00:12:19,133
如果我去问这个大模型 what the MOE is

398
00:12:19,166 --> 00:12:21,733
然后这里面就回答下个单词 MOE

399
00:12:21,733 --> 00:12:23,400
要把 MOE 拼回来

400
00:12:23,400 --> 00:12:24,533
我们的一个输入

401
00:12:24,533 --> 00:12:25,400
然后再输出

402
00:12:25,400 --> 00:12:28,400
然后第一寸第二寸就输出 is 然

403
00:12:28,400 --> 00:12:29,966
后 y is 的 m e is

404
00:12:29,966 --> 00:12:32,333
m e is 然后不断的去累加

405
00:12:32,333 --> 00:12:32,866
所以这个

406
00:12:32,866 --> 00:12:34,333
就是整个 m e 加

407
00:12:34,366 --> 00:12:37,233
LLM 的一个相关的概念了

408
00:12:37,766 --> 00:12:40,766
那我们现在的简单的来去总结一下

409
00:12:40,766 --> 00:12:42,466
整个 MOE 架构的一个原理

410
00:12:42,466 --> 00:12:44,900
首先 MOE 架构就把我们的一个传送门

411
00:12:44,933 --> 00:12:47,033
decoder 里面的 FFN 层

412
00:12:47,066 --> 00:12:49,933
替换成多个专家的形态

413
00:12:49,966 --> 00:12:51,633
有多个 FFN 的形态

414
00:12:51,666 --> 00:12:52,533
每个 FFN

415
00:12:52,566 --> 00:12:55,000
是我们刚才的一个大的 FFN 的一个

416
00:12:55,000 --> 00:12:56,566
其中的参数量变小

417
00:12:56,566 --> 00:12:59,000
但是它是一个完整的 FFN 层

418
00:12:59,033 --> 00:13:00,333
那这个 FFN

419
00:13:00,366 --> 00:13:01,966
我们叫做 exper

420
00:13:02,600 --> 00:13:03,566
叫做专家

421
00:13:03,566 --> 00:13:05,000
因为有很多个专家

422
00:13:05,000 --> 00:13:07,000
所以叫做 mixture of exper

423
00:13:07,066 --> 00:13:08,700
混合专家的形态

424
00:13:08,733 --> 00:13:10,533
而不是 multiply of exper

425
00:13:10,533 --> 00:13:11,533
不是多专家

426
00:13:11,533 --> 00:13:12,666
而是混合专家

427
00:13:12,666 --> 00:13:14,933
这里面有很多个专家混合在一起

428
00:13:14,966 --> 00:13:17,233
每一层选择不同的专家

429
00:13:18,566 --> 00:13:20,533
虽然中语讲的有点啰嗦

430
00:13:20,533 --> 00:13:22,666
然而是怕大家看不懂啦

431
00:13:26,066 --> 00:13:27,833
那我们现在来到了第五内容

432
00:13:27,833 --> 00:13:29,266
就是路由的原理

433
00:13:29,266 --> 00:13:31,366
都 vote 那我们现在来看一下

434
00:13:31,400 --> 00:13:34,166
虽然我们刚才讲到这个专家

435
00:13:34,166 --> 00:13:36,666
但是应该怎么去选择每个专家

436
00:13:36,666 --> 00:13:38,033
就是我们的路由的问题

437
00:13:38,033 --> 00:13:39,466
也就是我们的每个专家

438
00:13:39,633 --> 00:13:41,033
由什么方式来选的

439
00:13:41,033 --> 00:13:42,433
由我们的一个路由

440
00:13:42,433 --> 00:13:44,133
那路由了到底是什么

441
00:13:44,166 --> 00:13:44,833
路由其实

442
00:13:44,833 --> 00:13:47,033
它具体是一个数学的公式

443
00:13:47,033 --> 00:13:48,033
数学的函数

444
00:13:48,066 --> 00:13:50,866
帮助我们更好的去选择每一个专家

445
00:13:51,266 --> 00:13:52,366
那我们现在来看一下

446
00:13:52,400 --> 00:13:55,266
它具体是怎么去计算的

447
00:13:55,466 --> 00:13:56,766
现在我们还是重点的

448
00:13:56,800 --> 00:13:58,866
打开整个 MOE 的一个层

449
00:13:58,866 --> 00:14:01,033
来看一下里面的具体的内容

450
00:14:01,033 --> 00:14:03,266
那首先 MOE 的这个路由

451
00:14:03,266 --> 00:14:06,533
实际上是由两个神经网络去组成的

452
00:14:06,566 --> 00:14:08,333
第一个就是 FFN

453
00:14:08,366 --> 00:14:09,566
那这个 FFN

454
00:14:09,566 --> 00:14:12,933
是一个很小的简单的小模型的层

455
00:14:13,166 --> 00:14:15,866
然后经过一个 Softmax 的函数的计算

456
00:14:15,866 --> 00:14:18,433
那大家如果学过对应的数学

457
00:14:18,433 --> 00:14:21,233
你会知道 Softmax 的计算之后就会输出

458
00:14:21,233 --> 00:14:24,300
我们假设我们现在的有 5 个专家

459
00:14:24,333 --> 00:14:25,433
或有 4 个专家

460
00:14:25,633 --> 00:14:26,433
我们就会输出

461
00:14:26,433 --> 00:14:29,266
四个专家对应的一个具体的概率

462
00:14:29,266 --> 00:14:31,433
也就是做一个最大释然值的

463
00:14:31,433 --> 00:14:32,633
一个具体的计算

464
00:14:32,733 --> 00:14:34,233
那算完 soft mass 后

465
00:14:34,233 --> 00:14:35,100
我们就知道了

466
00:14:35,133 --> 00:14:37,000
a 我们的一个每一个专家

467
00:14:37,000 --> 00:14:38,200
或者对应四个专家

468
00:14:38,200 --> 00:14:40,866
里面的最大概率是 0.45

469
00:14:40,866 --> 00:14:43,633
那这个时候我们就会去激活

470
00:14:43,633 --> 00:14:44,766
或者去选择

471
00:14:44,800 --> 00:14:46,933
最大概率的一个具体的专家

472
00:14:46,933 --> 00:14:49,600
当然了到底是选择一个还是选择两个

473
00:14:49,600 --> 00:14:51,866
就是 0.45 跟 0.31 都选

474
00:14:51,866 --> 00:14:53,033
还是选择其中一个

475
00:14:53,033 --> 00:14:55,533
是我们的一个 keep k 去决定的

476
00:14:55,566 --> 00:14:57,833
那我们现在还没讲到 keep talking

477
00:14:57,833 --> 00:14:58,533
我们是回到

478
00:14:58,566 --> 00:15:00,733
整个 Softmax 的一个具体的计算

479
00:15:00,933 --> 00:15:01,600
具体计算之后

480
00:15:01,600 --> 00:15:03,966
我们现在选择了一个具体的专家

481
00:15:04,166 --> 00:15:05,866
最后我们会对这个专家

482
00:15:05,866 --> 00:15:08,566
来进行一个加权求和的方式

483
00:15:08,600 --> 00:15:10,366
然后进行一个输出的

484
00:15:10,366 --> 00:15:11,766
那如果有多个专家

485
00:15:11,766 --> 00:15:13,533
就很好的做加权求和了

486
00:15:13,533 --> 00:15:15,566
但是我们现在只有一个专家

487
00:15:15,566 --> 00:15:16,333
所以没有关系

488
00:15:16,333 --> 00:15:17,333
我们只有一个专家

489
00:15:17,333 --> 00:15:19,600
直接输出的这种方式

490
00:15:20,433 --> 00:15:22,633
既然我们了解完整个 MOE 的架构

491
00:15:22,633 --> 00:15:23,933
和怎么去选择路由之后

492
00:15:23,966 --> 00:15:25,733
我们现在整体回顾过来

493
00:15:25,733 --> 00:15:28,366
看一下整个 MOE 的一个所谓的系数架构

494
00:15:28,366 --> 00:15:30,566
哈哈哈哈哈哈

495
00:15:30,566 --> 00:15:33,233
首先我们搞清楚稠密跟稀疏

496
00:15:33,233 --> 00:15:35,933
因为刚才讲到的稠密跟稀疏主要

497
00:15:35,966 --> 00:15:37,766
是指我们的一个 transformer

498
00:15:37,766 --> 00:15:39,533
还是 MOEtransformer

499
00:15:39,533 --> 00:15:40,433
完全的 transformer

500
00:15:40,433 --> 00:15:42,433
自己是一个稠密的架构

501
00:15:42,533 --> 00:15:44,600
MOE 自己是一个稀疏的架构

502
00:15:44,600 --> 00:15:46,400
但是 MOE 内部

503
00:15:46,400 --> 00:15:49,166
也有一个所谓的稠密跟稀疏的概念

504
00:15:49,433 --> 00:15:51,466
那所谓的 dance MO1

505
00:15:51,466 --> 00:15:53,766
也就是我们现在是一个

506
00:15:53,800 --> 00:15:54,966
dance 的一个模型

507
00:15:54,966 --> 00:15:56,933
然后是一个 MO1 的模型

508
00:15:56,933 --> 00:15:59,000
MO1 里面又分一个 dance 的模型

509
00:15:59,000 --> 00:16:02,866
然后也分一个 spares 的一个具体的专家

510
00:16:02,866 --> 00:16:04,700
所以这里面就需要分开了

511
00:16:04,733 --> 00:16:06,833
整个 transsuMOEr 里面

512
00:16:06,833 --> 00:16:08,333
是分开两个类型

513
00:16:08,366 --> 00:16:09,000
那我们现在

514
00:16:09,000 --> 00:16:11,200
主要聚焦到下面的这个模块

515
00:16:11,333 --> 00:16:13,000
回到我们的整个 PPT 里面

516
00:16:13,000 --> 00:16:15,833
我们看一下所谓的单词的 moe

517
00:16:15,833 --> 00:16:18,033
就是我们真正的去路由的时候

518
00:16:18,433 --> 00:16:20,033
所有的专家都选上

519
00:16:20,033 --> 00:16:22,033
那这种就是 dance 的 MO1

520
00:16:22,133 --> 00:16:22,933
那另外一种

521
00:16:22,933 --> 00:16:24,766
叫做 space 的 MO1

522
00:16:24,766 --> 00:16:25,333
那主要

523
00:16:25,333 --> 00:16:27,733
就是指我们可能在录入的过程中

524
00:16:27,733 --> 00:16:30,933
只选择某个 top k 的一个专家

525
00:16:30,933 --> 00:16:32,766
来进行一个激活

526
00:16:32,766 --> 00:16:35,633
那这种方式我们叫做 space 的 MO1

527
00:16:35,633 --> 00:16:36,466
那实际上

528
00:16:36,466 --> 00:16:37,900
我们可以看到

529
00:16:38,733 --> 00:16:41,000
在真正的运行过程当中

530
00:16:41,000 --> 00:16:42,933
或者在真正的一个环境

531
00:16:42,933 --> 00:16:45,000
现在来看到整个 MOE 的专家

532
00:16:45,000 --> 00:16:46,366
更倾向于这种说

533
00:16:46,366 --> 00:16:48,266
我们说到的稀疏 MOE

534
00:16:48,266 --> 00:16:49,733
还是 MOE 专家

535
00:16:49,766 --> 00:16:51,800
是一个稀疏模型结构的时候

536
00:16:51,800 --> 00:16:55,400
更多的是指这一种右边的 suppress 的 MOE

537
00:16:55,400 --> 00:16:57,533
我们真正在输出选择的时候

538
00:16:57,533 --> 00:16:59,533
只激活部分的专家

539
00:16:59,533 --> 00:17:02,466
然然后对专家进行一个 Aggregate

540
00:17:02,466 --> 00:17:05,366
也就是最终算完 keep talking 专家之后

541
00:17:05,400 --> 00:17:07,733
进行一个聚合 Aggregate

542
00:17:08,433 --> 00:17:09,500
下面我们打开

543
00:17:09,533 --> 00:17:10,366
整个 moe

544
00:17:10,366 --> 00:17:11,266
具体的计算

545
00:17:11,266 --> 00:17:12,533
是怎么去计算的

546
00:17:12,566 --> 00:17:14,200
首先我们的输入

547
00:17:14,200 --> 00:17:16,133
实际上是一个具体的矩阵

548
00:17:16,133 --> 00:17:17,833
或者非常大的一个矩阵

549
00:17:17,833 --> 00:17:20,466
那每个矩阵这里面是一个 bitch size

550
00:17:20,466 --> 00:17:21,900
然后这里面是一个 Token

551
00:17:21,933 --> 00:17:24,333
或者一个具体句句子的长度

552
00:17:24,333 --> 00:17:25,233
所以说一般来说

553
00:17:25,233 --> 00:17:26,566
我们的一个输入 transformer

554
00:17:26,600 --> 00:17:30,166
里面就有 b s h 相关的内容

555
00:17:30,166 --> 00:17:31,766
b 就是我们的 bitch size

556
00:17:31,866 --> 00:17:33,633
s 就是我们的文章的长度

557
00:17:33,633 --> 00:17:35,566
h 就是我们的 hidden ding

558
00:17:35,733 --> 00:17:36,666
那不管怎么样

559
00:17:36,666 --> 00:17:39,366
我们现在主要关注的是 b 跟 s 了

560
00:17:39,400 --> 00:17:41,866
就是我们的一个 bitch size 跟一个 sequence

561
00:17:41,933 --> 00:17:44,266
就组成了我们的 input 的一个矩阵

562
00:17:44,566 --> 00:17:46,033
那有了 input 矩阵之后

563
00:17:46,033 --> 00:17:48,100
我们需要注意这个 input 矩阵

564
00:17:48,133 --> 00:17:50,366
经过一个 FFN 层

565
00:17:50,433 --> 00:17:51,500
那这个 FFN 层

566
00:17:51,533 --> 00:17:53,400
就是我们所谓的路由程了

567
00:17:53,400 --> 00:17:54,533
路由的权重

568
00:17:54,533 --> 00:17:56,633
我们的 input x 乘以 w

569
00:17:56,766 --> 00:17:58,966
就等于我们的路由的权重

570
00:17:59,233 --> 00:18:01,966
接着我们的输出就是 HX

571
00:18:02,233 --> 00:18:05,133
HS 就是我们现在的一个简单的路由

572
00:18:05,166 --> 00:18:06,833
FFN 层的一个输出

573
00:18:06,833 --> 00:18:08,300
那有了 HS 之后

574
00:18:08,666 --> 00:18:11,633
HS 要经过一个 Softmax 的计算

575
00:18:11,766 --> 00:18:13,533
Softmax 主要是去计算

576
00:18:13,533 --> 00:18:16,133
我们的一个专家的概率分布的

577
00:18:16,133 --> 00:18:18,800
有多少个专家就有多少个概率分布

578
00:18:18,800 --> 00:18:20,600
最终就得到最硬的

579
00:18:20,600 --> 00:18:22,033
这个 GX GS

580
00:18:22,033 --> 00:18:23,533
就对应我们每个专家

581
00:18:23,600 --> 00:18:26,333
所对应的一个输出的概率

582
00:18:26,333 --> 00:18:27,566
那有了这个之后

583
00:18:27,566 --> 00:18:28,733
我们现在的路由

584
00:18:28,733 --> 00:18:30,333
就根据我们的概率分布

585
00:18:30,633 --> 00:18:32,466
去选择最合适的

586
00:18:32,466 --> 00:18:34,866
或者最合适下次要计算的专家了

587
00:18:34,866 --> 00:18:37,100
通过这种方式来去实现的

588
00:18:37,133 --> 00:18:38,433
来接下来我们看一下

589
00:18:38,433 --> 00:18:40,133
真正的我们可以看到

590
00:18:40,166 --> 00:18:42,200
整个 ES 的是有非常多的东西

591
00:18:42,200 --> 00:18:43,366
但是我们现在

592
00:18:43,366 --> 00:18:45,066
只算其中一层

593
00:18:45,233 --> 00:18:47,033
或者其中一个专家的输出

594
00:18:47,033 --> 00:18:48,033
也就是 EX

595
00:18:48,233 --> 00:18:49,266
EX 得到之后

596
00:18:49,333 --> 00:18:52,666
就会去乘以我们的一个具体的 GS

597
00:18:52,666 --> 00:18:54,300
也就是我们 Softmax

598
00:18:54,333 --> 00:18:56,433
得到的一个具体的概率分布

599
00:18:56,433 --> 00:18:58,100
乘以我们的一个专家的

600
00:18:58,133 --> 00:18:59,966
一个具体的输出的激活值

601
00:19:00,000 --> 00:19:01,533
然后就得到

602
00:19:01,533 --> 00:19:02,366
还没有得到了

603
00:19:02,366 --> 00:19:03,666
还要做加权求和

604
00:19:03,666 --> 00:19:05,766
因为如果有多个专家的时候

605
00:19:05,800 --> 00:19:07,933
那我们不需要对好多个专家

606
00:19:07,933 --> 00:19:09,866
进行一个加权求和

607
00:19:09,866 --> 00:19:11,366
那加权求和完之后

608
00:19:11,400 --> 00:19:13,800
我们就可以得到最终的输出了

609
00:19:13,800 --> 00:19:16,233
那这个输出是一个 speest MO1

610
00:19:16,233 --> 00:19:17,233
因为这里面

611
00:19:17,233 --> 00:19:18,566
有可能我们两个专家

612
00:19:18,600 --> 00:19:21,600
假设选择了专家一和专家 3 两条路由

613
00:19:21,600 --> 00:19:23,733
那我们就有两个具体的答案

614
00:19:23,733 --> 00:19:25,866
两个具体的矩阵把它拼在一起

615
00:19:26,066 --> 00:19:27,100
那经过这个内容

616
00:19:27,133 --> 00:19:30,466
我们就完全的明白整个 MOE 的架构

617
00:19:30,466 --> 00:19:32,066
是怎么去计算的啦

618
00:19:33,000 --> 00:19:34,800
今天的视频的内容有点长

619
00:19:34,800 --> 00:19:36,033
比原视频还要长

620
00:19:36,033 --> 00:19:39,033
因为中美的废话太多了迷岸灯

621
00:19:41,533 --> 00:19:43,433
我们现在来来到了第四个内容

622
00:19:43,433 --> 00:19:44,033
看一下

623
00:19:44,033 --> 00:19:45,233
负载均衡

624
00:19:45,400 --> 00:19:46,833
load balancing

625
00:19:46,866 --> 00:19:48,566
刚才只是讲到了

626
00:19:48,600 --> 00:19:50,666
整个 MOE 的专家

627
00:19:50,666 --> 00:19:52,333
具体是怎么去计算的

628
00:19:52,366 --> 00:19:54,800
每一步看上去很合理对不对

629
00:19:54,800 --> 00:19:56,433
但是我们的专家

630
00:19:56,433 --> 00:20:00,366
在整个地这个里面专家多达 256 个

631
00:20:00,400 --> 00:20:02,333
在整个 speech 传送网里面

632
00:20:02,333 --> 00:20:04,233
专家多达上万个

633
00:20:04,266 --> 00:20:05,633
那这种方式

634
00:20:05,633 --> 00:20:07,666
我们怎么保证每一个专家

635
00:20:07,666 --> 00:20:09,500
都能均衡的选择

636
00:20:10,200 --> 00:20:11,033
哎这里面

637
00:20:11,233 --> 00:20:12,533
之前刚才讲到

638
00:20:12,566 --> 00:20:13,933
其实最好的一个方式

639
00:20:13,933 --> 00:20:15,266
就是选择最大的一个

640
00:20:15,266 --> 00:20:16,433
概率分布但是

641
00:20:16,433 --> 00:20:18,866
这里面有一个最大的坏处

642
00:20:19,200 --> 00:20:21,266
就是我们在训练的时候

643
00:20:21,566 --> 00:20:23,433
有可能某个专家

644
00:20:23,433 --> 00:20:24,633
学的特别的快

645
00:20:24,633 --> 00:20:26,300
也就计算的特别的快

646
00:20:26,333 --> 00:20:27,766
然后其他专家的话

647
00:20:27,766 --> 00:20:28,533
有些专家

648
00:20:28,533 --> 00:20:30,000
计算的特别的慢

649
00:20:30,000 --> 00:20:31,000
那这个时候

650
00:20:31,000 --> 00:20:33,033
如果我们在长期的训练过程当中

651
00:20:33,033 --> 00:20:35,166
就会发现我们的 pathway

652
00:20:35,333 --> 00:20:36,433
会自动的去选择

653
00:20:36,433 --> 00:20:38,433
我们算的特别快的一些专家

654
00:20:38,600 --> 00:20:40,733
不管我们的输入是什么数据

655
00:20:40,733 --> 00:20:42,666
不管我们的输入是什么类型

656
00:20:42,666 --> 00:20:45,100
到底是 psychology 还是 biology

657
00:20:45,133 --> 00:20:47,133
都会选择同一条路径

658
00:20:47,133 --> 00:20:49,266
就导致这些小的专家

659
00:20:49,266 --> 00:20:50,833
学的东西就越来越多

660
00:20:51,066 --> 00:20:51,966
其他的专家

661
00:20:52,000 --> 00:20:55,000
学的东西就越来越少或者不均衡了

662
00:20:55,000 --> 00:20:55,533
因此

663
00:20:55,533 --> 00:20:58,833
我们要做一个事情叫叫做均衡负载

664
00:20:58,833 --> 00:21:02,266
那均衡负载的英文叫做 load balancings

665
00:21:02,533 --> 00:21:03,266
去保证

666
00:21:03,266 --> 00:21:04,233
每一个专家

667
00:21:04,233 --> 00:21:06,166
都去学习不同的知识

668
00:21:06,200 --> 00:21:08,433
或者去学习不同的一个影像量

669
00:21:08,433 --> 00:21:09,733
相关的知识

670
00:21:10,433 --> 00:21:11,866
为了做均衡负载

671
00:21:11,866 --> 00:21:12,333
这里面

672
00:21:12,366 --> 00:21:15,200
就有了一个名字叫做 keep talk k

673
00:21:15,200 --> 00:21:16,333
那通过 keep talk k

674
00:21:16,333 --> 00:21:18,200
去实现我们的均衡负载的

675
00:21:18,200 --> 00:21:19,566
因此我们现在

676
00:21:19,566 --> 00:21:21,466
真正的来到了第五个内容

677
00:21:21,466 --> 00:21:22,900
也就是所谓的 keep talk k

678
00:21:22,933 --> 00:21:24,566
均衡负载的内容

679
00:21:24,966 --> 00:21:26,466
那均衡负载怎么去实现

680
00:21:26,466 --> 00:21:28,633
我们还是回到我们的具体的计算了

681
00:21:28,633 --> 00:21:29,333
那回顾一下

682
00:21:29,366 --> 00:21:30,966
刚才我们把数的 x

683
00:21:30,966 --> 00:21:32,766
乘以我们的一个路由 FFN

684
00:21:32,766 --> 00:21:34,733
然后得到我们的 HS

685
00:21:34,933 --> 00:21:36,733
HS 之后这里面蛮有意思的

686
00:21:36,733 --> 00:21:39,200
就是为了解决我们的均衡负载

687
00:21:39,200 --> 00:21:41,666
刚才在或者上一期的视频里面

688
00:21:41,666 --> 00:21:44,333
在分享那个 STMOE 里面就说到了

689
00:21:44,366 --> 00:21:45,533
我们需要注入噪

690
00:21:45,533 --> 00:21:47,600
声那所谓的噪声的注入

691
00:21:47,600 --> 00:21:49,133
就从这里面开始了

692
00:21:49,466 --> 00:21:50,933
我们引入一个模板

693
00:21:50,966 --> 00:21:52,733
跟我们的高斯噪声是相同的

694
00:21:52,733 --> 00:21:53,533
那这个模板

695
00:21:53,533 --> 00:21:56,033
跟这个 HS 的输出是一样的

696
00:21:56,033 --> 00:21:57,266
通过 x 乘以 w

697
00:21:57,266 --> 00:21:59,966
然后加上我们的一个模板的高斯噪声

698
00:22:00,033 --> 00:22:01,733
这种方式去引进来

699
00:22:01,766 --> 00:22:04,266
那这个人就所谓的噪声注入了

700
00:22:04,266 --> 00:22:05,333
更好的帮助

701
00:22:05,366 --> 00:22:07,933
我们去选择对应的专家

702
00:22:07,933 --> 00:22:10,866
让我们的专家就更加的一个均衡

703
00:22:11,266 --> 00:22:12,300
这里面的噪声

704
00:22:12,333 --> 00:22:13,733
不是个随机的噪声

705
00:22:13,733 --> 00:22:16,566
而是有选择性的帮我们去抑制

706
00:22:16,566 --> 00:22:18,400
当我们的某个专家

707
00:22:18,400 --> 00:22:20,066
选择的次数过于频繁

708
00:22:20,066 --> 00:22:21,466
过于过多的时候

709
00:22:21,566 --> 00:22:23,600
让他降低他的一个得分

710
00:22:23,866 --> 00:22:25,500
从而在训练的过程当中

711
00:22:25,533 --> 00:22:27,400
可以让我们的其他的专家

712
00:22:27,400 --> 00:22:29,000
都能够均衡的

713
00:22:29,000 --> 00:22:30,600
都能选那这里面

714
00:22:30,600 --> 00:22:31,966
到底怎么去选

715
00:22:31,966 --> 00:22:33,766
在整个 Softmax 计算的时候

716
00:22:33,766 --> 00:22:34,866
这里面就引入了一个

717
00:22:34,866 --> 00:22:37,033
所谓的 keep talk k 的一个计算

718
00:22:37,066 --> 00:22:38,166
那所谓的 keep talk k

719
00:22:38,200 --> 00:22:39,866
就对我们的一个 HS 的

720
00:22:39,866 --> 00:22:41,366
一个 Softmax 的输出

721
00:22:41,466 --> 00:22:42,900
做了一个具体的选择

722
00:22:42,933 --> 00:22:45,333
也就是这里面的 keep talk k HS2

723
00:22:45,333 --> 00:22:46,600
HS 是里面的一个输入

724
00:22:46,600 --> 00:22:47,166
然后 2

725
00:22:47,166 --> 00:22:47,733
就代表

726
00:22:47,733 --> 00:22:51,433
代表我只选择其中的两个专家

727
00:22:51,433 --> 00:22:52,366
其他的专家

728
00:22:52,400 --> 00:22:54,433
我至于父无从来不去选

729
00:22:55,233 --> 00:22:58,133
那最终我们可以看到 Softmax 的输出

730
00:22:58,166 --> 00:23:00,200
就变成这种形态了

731
00:23:00,200 --> 00:23:02,233
不是说很多条柱子

732
00:23:02,233 --> 00:23:04,233
而是只有两条柱子

733
00:23:04,233 --> 00:23:04,933
那这里面

734
00:23:04,966 --> 00:23:06,166
我们可以看到

735
00:23:06,366 --> 00:23:08,933
只有这两条柱子是真正有值的

736
00:23:08,933 --> 00:23:09,966
那 publict

737
00:23:09,966 --> 00:23:11,533
为 0 的那些柱子

738
00:23:11,533 --> 00:23:13,433
我们就视为复无从

739
00:23:13,600 --> 00:23:14,766
所以说从这种方式

740
00:23:14,766 --> 00:23:18,033
我们就只选择其中的某些专家

741
00:23:18,033 --> 00:23:18,533
进行一个

742
00:23:18,566 --> 00:23:19,366
计算

743
00:23:20,200 --> 00:23:21,200
通过这种方式

744
00:23:21,200 --> 00:23:22,733
在每一场每一次的时候

745
00:23:22,733 --> 00:23:24,200
选择一些复无从

746
00:23:24,200 --> 00:23:26,633
或者是抑制一些复无从

747
00:23:26,633 --> 00:23:29,266
从而使得我们一些训练不足的专家

748
00:23:29,266 --> 00:23:31,900
能够赶上那些被频繁选择的专家

749
00:23:32,000 --> 00:23:34,833
拥有更多的被选择的机会

750
00:23:35,666 --> 00:23:37,366
那刚才讲到了这种方式

751
00:23:37,400 --> 00:23:39,800
路由到我们所选择的专家

752
00:23:39,800 --> 00:23:41,800
也叫做那个 Token chose

753
00:23:41,800 --> 00:23:43,333
也就是 Token 的选择

754
00:23:43,333 --> 00:23:45,066
那跟我们刚才的 TikTok king

755
00:23:45,066 --> 00:23:46,133
是一个概念

756
00:23:46,166 --> 00:23:49,333
允许我们选择具体的一个专家

757
00:23:49,333 --> 00:23:50,000
那每一次

758
00:23:50,000 --> 00:23:51,466
都是不同的专家的

759
00:23:51,466 --> 00:23:53,300
然后 TikTok king 的一个路由

760
00:23:53,333 --> 00:23:55,033
就是只选择某些专家

761
00:23:55,033 --> 00:23:55,933
其中一些专家

762
00:23:55,966 --> 00:23:57,633
就变成自负无穷

763
00:23:58,566 --> 00:23:59,733
既然有我们的路由

764
00:23:59,733 --> 00:24:01,133
有我们的 MOE 的专家

765
00:24:01,133 --> 00:24:01,466
然后

766
00:24:01,466 --> 00:24:04,433
现在对 Softmax 就对我们的一个路由

767
00:24:04,433 --> 00:24:05,900
选择了一个 Keep Top-K

768
00:24:05,966 --> 00:24:06,666
那接下来

769
00:24:06,666 --> 00:24:09,700
我们看一下一个辅助的损失

770
00:24:09,733 --> 00:24:11,766
austery 的一个 locks

771
00:24:12,133 --> 00:24:12,800
那我们看一下

772
00:24:12,800 --> 00:24:15,433
所谓的辅助损失有什么作用

773
00:24:15,433 --> 00:24:16,833
为了去提升

774
00:24:16,833 --> 00:24:17,700
就辅助损失

775
00:24:17,733 --> 00:24:19,166
实际上还是为了去提升

776
00:24:19,166 --> 00:24:20,866
我们的一个均衡负载

777
00:24:20,866 --> 00:24:22,733
所以才引入了一个

778
00:24:22,766 --> 00:24:24,233
Salary 的一个 loss

779
00:24:24,400 --> 00:24:25,733
那这 a salary 的 loss

780
00:24:25,733 --> 00:24:28,366
实际上也可以叫做 load balance 的 loss

781
00:24:28,366 --> 00:24:29,933
要均衡负载的 loss

782
00:24:29,933 --> 00:24:32,533
因为我们在正向的时候是计算

783
00:24:32,566 --> 00:24:34,333
但是我们在正向的时候

784
00:24:34,333 --> 00:24:35,800
通过引用两个内容

785
00:24:35,800 --> 00:24:37,733
一个是高斯的造成

786
00:24:37,733 --> 00:24:40,333
第二个是 tiptop k 来去抑制

787
00:24:40,333 --> 00:24:42,400
我们某些专家选的特别多

788
00:24:42,433 --> 00:24:44,700
但是真正决定关键因素的

789
00:24:44,733 --> 00:24:46,333
是学习的过程

790
00:24:46,333 --> 00:24:47,233
怎么去学习

791
00:24:47,233 --> 00:24:48,266
怎么去抑制

792
00:24:48,266 --> 00:24:48,966
那这里面

793
00:24:49,000 --> 00:24:51,533
就引用一个 authory 的一个 loss

794
00:24:51,666 --> 00:24:52,366
那我们想象

795
00:24:52,400 --> 00:24:52,966
一下

796
00:24:52,966 --> 00:24:55,366
假设我们现在输进去的每一个单词

797
00:24:55,466 --> 00:24:57,933
都会有不同的概率的分布

798
00:24:57,966 --> 00:25:00,000
那现在我们看一下路由的概率

799
00:25:00,000 --> 00:25:01,733
就每个单词对应的路由概率

800
00:25:01,733 --> 00:25:02,666
假设一个 what

801
00:25:02,933 --> 00:25:04,966
它真正的不是说一个专家

802
00:25:05,000 --> 00:25:06,333
因为我们刚才讲到的

803
00:25:06,333 --> 00:25:08,833
它没有一个专家专门去选每个单词

804
00:25:09,000 --> 00:25:10,400
而是专家之间

805
00:25:10,400 --> 00:25:13,033
去学习不同的隐藏的信息

806
00:25:13,033 --> 00:25:14,033
那假设 what

807
00:25:14,033 --> 00:25:15,900
可能是 135 专家

808
00:25:16,000 --> 00:25:18,033
is 可能是第二个专家

809
00:25:18,033 --> 00:25:20,100
然后最后的 s 破

810
00:25:20,133 --> 00:25:23,233
这个单词是一跟 6 的专家

811
00:25:23,233 --> 00:25:25,500
那每一个专家的概率分布

812
00:25:25,533 --> 00:25:26,566
都是不一样的

813
00:25:26,566 --> 00:25:29,366
我们路由到不同的概率分布里面

814
00:25:29,400 --> 00:25:30,533
最核心的概念

815
00:25:30,533 --> 00:25:32,033
就是我们现在

816
00:25:32,033 --> 00:25:34,633
要加入一个 importance 的一个因子

817
00:25:34,666 --> 00:25:36,100
选择每一个专家

818
00:25:36,133 --> 00:25:38,066
去计算每一个专家的

819
00:25:38,066 --> 00:25:39,766
对于整个网络模型

820
00:25:39,800 --> 00:25:41,733
或者对于整一个训练过程当中

821
00:25:41,733 --> 00:25:42,666
的重要性

822
00:25:42,933 --> 00:25:43,400
那这里面

823
00:25:43,400 --> 00:25:46,800
我们把刚才很多个 Token 的一个

824
00:25:46,833 --> 00:25:50,700
第一个 ASP 的一个值进行一个累加

825
00:25:50,766 --> 00:25:51,466
累加成为了

826
00:25:51,466 --> 00:25:54,033
我们就知道了第一个专家的一个

827
00:25:54,033 --> 00:25:55,500
或者所有专家里面的

828
00:25:55,533 --> 00:25:57,733
每一个专家的重要性

829
00:25:57,733 --> 00:25:59,066
有了重要性之后

830
00:25:59,066 --> 00:26:02,633
我们就可以通过一个 coefficient every 选择

831
00:26:02,633 --> 00:26:04,033
这相关的变量系数

832
00:26:04,033 --> 00:26:05,700
去抑制我们最核心

833
00:26:05,733 --> 00:26:08,000
就假设我们都选第一个专家的时候

834
00:26:08,000 --> 00:26:10,200
把这个专家就抑制下来

835
00:26:10,200 --> 00:26:13,266
然后让其他专家就更均衡的选择

836
00:26:13,333 --> 00:26:16,766
通过这种方式去做我们的一个辅助

837
00:26:16,766 --> 00:26:19,000
损失的在真正计算的时候

838
00:26:19,000 --> 00:26:19,866
就把我们的标准

839
00:26:19,866 --> 00:26:22,166
查出一个具体的一个均值

840
00:26:22,200 --> 00:26:22,633
然后

841
00:26:22,633 --> 00:26:26,166
就作为我们的一个 coefficient veriance

842
00:26:26,200 --> 00:26:27,333
相关的内容

843
00:26:27,366 --> 00:26:28,766
那我们假设

844
00:26:28,766 --> 00:26:31,033
刚才讲到的某一个专家特

845
00:26:31,033 --> 00:26:34,066
别的高然后已经非常的高了 1.1 了

846
00:26:34,066 --> 00:26:37,066
那这个时候我们看一下其他的专家

847
00:26:37,066 --> 00:26:38,633
就除了第一个专家以外

848
00:26:38,633 --> 00:26:39,766
我们其他专家

849
00:26:40,033 --> 00:26:42,500
整体的一个 CV 是非常的低的

850
00:26:42,533 --> 00:26:44,400
所以我们现在就要对第一个专家

851
00:26:44,400 --> 00:26:46,066
来进行一个抑制的

852
00:26:46,066 --> 00:26:47,466
那 Australia loss

853
00:26:47,466 --> 00:26:49,033
就是处理这一个内容

854
00:26:49,233 --> 00:26:49,700
这里面

855
00:26:49,733 --> 00:26:52,633
我们引入一个变量叫做 w importance

856
00:26:52,633 --> 00:26:53,866
那这个 w importance

857
00:26:53,866 --> 00:26:55,666
就是一个变量的系数

858
00:26:55,666 --> 00:26:58,766
然后去控制我们的一个 Australia loss

859
00:26:59,033 --> 00:27:01,033
最终就使得我们所有的专家

860
00:27:01,033 --> 00:27:02,300
都更加的均衡

861
00:27:02,333 --> 00:27:04,466
在我们的训练的过程当中

862
00:27:05,000 --> 00:27:05,366
当然了

863
00:27:05,366 --> 00:27:07,400
大家要注意的就是这个 Oseri los

864
00:27:07,400 --> 00:27:08,766
它不是整个 los

865
00:27:08,766 --> 00:27:10,966
而是把我们的 los 加到

866
00:27:10,966 --> 00:27:13,366
或者把我们的 austerly 的 loss 的

867
00:27:13,366 --> 00:27:14,766
一个辅助损失了

868
00:27:14,766 --> 00:27:18,033
加到整体的损失里面去计算的

869
00:27:18,033 --> 00:27:18,933
所以说这个

870
00:27:18,966 --> 00:27:22,066
叫做辅助损失或者均衡负载的损失

871
00:27:22,066 --> 00:27:24,300
而不是指整个网络模型的损失

872
00:27:24,333 --> 00:27:26,766
以前该用什么损失函数去计算

873
00:27:26,766 --> 00:27:29,233
现在只是加了一个变量

874
00:27:29,233 --> 00:27:31,633
或者加了一个损失的值进去

875
00:27:31,633 --> 00:27:33,700
做一个混合的损失

876
00:27:34,266 --> 00:27:35,866
那了解完刚才所有的内容了

877
00:27:35,866 --> 00:27:37,666
我们现在已经非常的清楚

878
00:27:37,666 --> 00:27:39,766
整个 MOE 的价格是怎么去计算的

879
00:27:39,800 --> 00:27:42,466
里面 step by step 每一步的详细的内容

880
00:27:44,133 --> 00:27:44,466
现在

881
00:27:44,466 --> 00:27:46,866
我们简单的看一个额外的知识

882
00:27:46,866 --> 00:27:48,700
就是所谓的专家容量

883
00:27:48,733 --> 00:27:52,400
Asper 的一个 capacity 引诱这个专家容量

884
00:27:52,400 --> 00:27:53,033
同样

885
00:27:53,033 --> 00:27:56,100
是为了解决我们的整个 MOE 专家的

886
00:27:56,133 --> 00:27:56,766
一个训练

887
00:27:56,766 --> 00:27:58,433
均衡负载的问题

888
00:27:58,433 --> 00:28:00,833
这里面就举一个具体的例子

889
00:28:00,833 --> 00:28:01,500
那首先

890
00:28:01,533 --> 00:28:02,833
我们有四个专家

891
00:28:02,833 --> 00:28:04,166
那其中的第四

892
00:28:04,200 --> 00:28:07,533
个专家就 F F N 4 s P 4 只处理一个 Token

893
00:28:07,600 --> 00:28:08,166
但是

894
00:28:08,166 --> 00:28:10,366
其他你可以看到非常多的 Token

895
00:28:10,366 --> 00:28:12,866
都丢给了 FFNE 进行处理

896
00:28:12,866 --> 00:28:15,133
那这个时候我们可以看到 FFN1

897
00:28:15,166 --> 00:28:16,133
就处理了很多

898
00:28:16,133 --> 00:28:18,200
不同的分布对不同的 Token

899
00:28:18,266 --> 00:28:20,333
FFN4 就其实你会发现

900
00:28:20,366 --> 00:28:22,533
还是非常的一个不均衡的

901
00:28:22,533 --> 00:28:25,033
反正 ASP1 处理的数据很多

902
00:28:25,033 --> 00:28:27,300
ASP 数处理的数据很少

903
00:28:27,533 --> 00:28:30,533
所以 MO1 专家在输进去 Token 之前

904
00:28:30,533 --> 00:28:33,166
就加了一个叫做 ASPEC ST

905
00:28:33,333 --> 00:28:34,733
然后专家的容量

906
00:28:34,733 --> 00:28:36,733
去限制每一个专家

907
00:28:36,733 --> 00:28:38,966
能够最大处理的一个 Token 数

908
00:28:38,966 --> 00:28:39,533
所以说

909
00:28:39,533 --> 00:28:41,833
加了这个专家容量之后

910
00:28:41,833 --> 00:28:43,666
FFN1 就 S1

911
00:28:43,666 --> 00:28:46,100
最多你只能够处理三个专家

912
00:28:46,333 --> 00:28:48,733
那这个时候我们就把剩下的专家

913
00:28:48,733 --> 00:28:50,866
都丢给 FN 4 进行处理

914
00:28:50,866 --> 00:28:52,100
通过这种方式

915
00:28:52,133 --> 00:28:54,966
使得我们整个网络模型的计算

916
00:28:54,966 --> 00:28:56,633
更加的均衡了

917
00:28:56,633 --> 00:28:58,333
所以说可以看到 MOE

918
00:28:58,366 --> 00:29:00,566
这个网络模型相关计算的时候

919
00:29:00,666 --> 00:29:01,533
最大的问题

920
00:29:01,566 --> 00:29:03,800
就需要解决均衡负载的问题

921
00:29:03,800 --> 00:29:06,866
让我们的网络模型的训练更加稳定

922
00:29:06,866 --> 00:29:10,166
那我们的 s Pro 的可就设置为 3

923
00:29:10,233 --> 00:29:11,166
让每个专家

924
00:29:11,200 --> 00:29:14,000
都更加的好去选择我们输入的 Token

925
00:29:14,000 --> 00:29:17,633
去限制我们输入的 Token 相关的内容了

926
00:29:18,133 --> 00:29:18,533
那最后

927
00:29:18,533 --> 00:29:21,133
我们来个总结和思考

928
00:29:24,600 --> 00:29:25,666
在今天的内容

929
00:29:25,666 --> 00:29:27,033
主要分开有点长

930
00:29:27,033 --> 00:29:28,333
中文有点啰嗦

931
00:29:28,600 --> 00:29:31,433
跟大家去简单的讲了整个 MOE 的架构

932
00:29:31,433 --> 00:29:32,866
然后看一下 MOE 了

933
00:29:32,866 --> 00:29:33,900
既然有很多 ASP

934
00:29:33,933 --> 00:29:34,966
所以我们要路由

935
00:29:34,966 --> 00:29:36,366
那运用路由的过程当中

936
00:29:36,366 --> 00:29:38,666
就会给到很多不同的小专家

937
00:29:38,666 --> 00:29:39,933
那专家多了我们

938
00:29:39,966 --> 00:29:42,166
就需要做均衡的负载

939
00:29:42,166 --> 00:29:43,666
均衡负载过程当中

940
00:29:43,666 --> 00:29:46,300
就是涉及到选哪个专家

941
00:29:46,333 --> 00:29:48,866
然后 loss 函数是怎么算的

942
00:29:48,866 --> 00:29:50,566
还有 Token 的输入

943
00:29:50,600 --> 00:29:52,633
确实我们也要做一个约束的

944
00:29:52,633 --> 00:29:53,500
所以基本上

945
00:29:53,533 --> 00:29:54,733
下面的 456

946
00:29:54,733 --> 00:29:56,266
都是做均衡负载

947
00:29:56,266 --> 00:29:58,500
保证我们的一个训练的稳定性

948
00:29:58,533 --> 00:29:59,800
保证我们每个专家

949
00:29:59,800 --> 00:30:01,466
都学到相关的内容

950
00:30:01,533 --> 00:30:02,566
今天的视频

951
00:30:02,566 --> 00:30:03,433
就先到这里为止

952
00:30:03,433 --> 00:30:03,966
谢谢各位

953
00:30:04,000 --> 00:30:04,800
拜了个拜

