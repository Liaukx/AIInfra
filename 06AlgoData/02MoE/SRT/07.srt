1
00:00:00,000 --> 00:00:02,400
内容/录制:Z0MI 酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,400 --> 00:00:03,000
哈喽大家好

3
00:00:03,000 --> 00:00:04,600
我是那个生活不讲道理

4
00:00:04,600 --> 00:00:06,566
只求游戏护体的 ZOMI

5
00:00:09,633 --> 00:00:11,966
今天我们回到整个 MoE 系列里面

6
00:00:12,000 --> 00:00:14,633
跟大家去总读一下相关的论文

7
00:00:14,633 --> 00:00:17,333
今天主要是跟大家分享两篇论文了

8
00:00:17,366 --> 00:00:19,766
第一篇就是叫 GLaM

9
00:00:19,766 --> 00:00:20,866
怎么不知道怎么拼

10
00:00:20,966 --> 00:00:23,366
第二篇叫做 ST-MoE

11
00:00:23,366 --> 00:00:25,033
分别有两篇论文

12
00:00:25,033 --> 00:00:25,733
那么有意思

13
00:00:25,766 --> 00:00:27,833
就是我们整体的回顾一下

14
00:00:27,833 --> 00:00:29,933
之前讲的一系列的视频

15
00:00:29,966 --> 00:00:31,633
之前我们从奠基工作

16
00:00:31,633 --> 00:00:35,033
90 年代初期的一个 Intern 里面

17
00:00:35,033 --> 00:00:36,166
发表的一篇论文了

18
00:00:36,200 --> 00:00:38,200
就是 MoE 的最初行的架构开始

19
00:00:38,200 --> 00:00:41,766
然后到了 RNN 时代谷歌的这篇文章了

20
00:00:41,766 --> 00:00:42,166
其实

21
00:00:42,166 --> 00:00:45,466
才真正的迎来了 MoE 架构的繁荣

22
00:00:45,466 --> 00:00:48,500
到了整个 Transformer 的 encoder decoder 时代

23
00:00:48,666 --> 00:00:51,300
很著名的三篇文章是 Gshard

24
00:00:51,466 --> 00:00:52,633
还有 switch Transformer

25
00:00:52,633 --> 00:00:55,033
还有今天去分享的 ST-MoE

26
00:00:55,166 --> 00:00:56,633
那今天重要点

27
00:00:56,633 --> 00:00:59,133
还会来到了智能涌现的时代

28
00:00:59,166 --> 00:01:00,366
真正的 GPT 时代

29
00:01:00,366 --> 00:01:01,766
也就是用了 Transformer

30
00:01:01,766 --> 00:01:03,866
decode only 的这个内容

31
00:01:03,966 --> 00:01:05,333
那在下面的内容

32
00:01:05,333 --> 00:01:08,466
主要是跟大家去分享 GLaM 跟 DeepSeek

33
00:01:08,466 --> 00:01:10,100
幻方量化相关的文章

34
00:01:10,133 --> 00:01:12,233
今天主要是在 2021 年

35
00:01:12,233 --> 00:01:15,900
GLaM 跟 Transformer 时代的 ST-MoE

36
00:01:15,933 --> 00:01:17,066
这里面加了一篇文章

37
00:01:17,066 --> 00:01:20,066
叫 ST-MoE 相关的论文进行解读

38
00:01:20,200 --> 00:01:22,866
那我们整体的目录大纲

39
00:01:22,866 --> 00:01:25,066
在整个 MoE 混合专家里面

40
00:01:25,066 --> 00:01:25,900
我们现在

41
00:01:25,933 --> 00:01:27,466
还是在 MoE 的相关

42
00:01:27,466 --> 00:01:28,900
核心工作的论文里面

43
00:01:28,933 --> 00:01:29,766
进行深度

44
00:01:29,766 --> 00:01:32,233
跟大家去做一个简单的解读

45
00:01:32,966 --> 00:01:33,833
那我们现在

46
00:01:33,833 --> 00:01:35,766
马上来到一个新的内容了

47
00:01:35,800 --> 00:01:38,866
我们组读一下 ST-MoE 这篇文章

48
00:01:40,466 --> 00:01:40,900
首先

49
00:01:40,933 --> 00:01:43,666
我们还是要简单的回顾一下 ST-MoE

50
00:01:43,666 --> 00:01:44,866
整体来说 ST-MoE

51
00:01:44,866 --> 00:01:47,433
他提出了一个稀疏很多专家的模型

52
00:01:47,433 --> 00:01:50,866
最重要的这篇文章去解决稀疏模型

53
00:01:50,866 --> 00:01:53,466
就 MoE 架构的训练不稳定性的问题

54
00:01:53,466 --> 00:01:55,366
还有迁移学习的问题

55
00:01:55,400 --> 00:01:57,133
那后面的大部分的文章

56
00:01:57,133 --> 00:01:57,766
都基本上

57
00:01:57,766 --> 00:02:00,233
继承了 ST-MoE 的一个相关

58
00:02:00,233 --> 00:02:01,466
训练稳定性的内容

59
00:02:01,466 --> 00:02:04,033
就为了使得混合专家模型

60
00:02:04,033 --> 00:02:05,466
在训练的过程当中

61
00:02:05,733 --> 00:02:06,433
更加合理

62
00:02:06,433 --> 00:02:09,766
更加有效的发挥我们混合专家的能力

63
00:02:09,800 --> 00:02:13,866
所以说 ST-MoE 奠定了整个 MoE 专家

64
00:02:13,866 --> 00:02:17,233
在训练稳定性的一个整体的核心工作

65
00:02:17,933 --> 00:02:20,366
整篇文章其实蛮多的内容

66
00:02:20,366 --> 00:02:22,133
就是提出了自己的路由

67
00:02:22,133 --> 00:02:22,733
Z-loss

68
00:02:22,733 --> 00:02:24,766
还有模型规模跟性能的一个验证

69
00:02:24,766 --> 00:02:27,033
还有模型架构的一个设计的原则

70
00:02:27,033 --> 00:02:27,466
另外的话

71
00:02:27,466 --> 00:02:29,766
还提出了一个均衡专家的负载

72
00:02:29,800 --> 00:02:31,433
还有对应的迁移学习

73
00:02:31,466 --> 00:02:32,366
那我们现在

74
00:02:32,400 --> 00:02:33,066
时不宜迟

75
00:02:33,066 --> 00:02:35,300
马上打开对应的文章

76
00:02:36,366 --> 00:02:37,200
我们现在

77
00:02:37,200 --> 00:02:39,000
打开了 ST-MoE 这篇文章了

78
00:02:39,000 --> 00:02:39,533
蛮有意思

79
00:02:39,533 --> 00:02:41,933
我们还是要一起去看一下标题

80
00:02:41,933 --> 00:02:43,000
ZOMI 比较关心

81
00:02:43,000 --> 00:02:44,133
首先标题就说了

82
00:02:44,133 --> 00:02:46,433
我去设计一个更加 stable

83
00:02:46,466 --> 00:02:47,700
更加稳定

84
00:02:47,733 --> 00:02:51,166
还有可转移的一个 MoE 的专家

85
00:02:51,166 --> 00:02:52,266
那这里面重点

86
00:02:52,266 --> 00:02:53,466
在整个标题里面

87
00:02:53,466 --> 00:02:56,466
就强调了这篇文章的核心 stable

88
00:02:56,533 --> 00:02:58,233
还有 transferable

89
00:02:58,233 --> 00:02:59,833
也就是可转移

90
00:02:59,833 --> 00:03:03,300
那从一个作者的正文可以看到

91
00:03:03,733 --> 00:03:06,133
最有意思的就是谷歌 reserch 的大牛

92
00:03:06,133 --> 00:03:07,733
jeff Dean 也参与进来了

93
00:03:07,733 --> 00:03:10,000
然后还有我们会讲下一篇文章

94
00:03:10,000 --> 00:03:12,566
GLaM 的一个第一作者 Nan Du

95
00:03:12,566 --> 00:03:14,466
然后也参与进来了

96
00:03:14,766 --> 00:03:15,166
那现在

97
00:03:15,166 --> 00:03:17,366
我们看一下整体的主要的摘要

98
00:03:17,366 --> 00:03:19,600
那摘要就表明了 scaling

99
00:03:19,600 --> 00:03:21,333
在自然语言处理里面

100
00:03:21,333 --> 00:03:23,933
已经取得了一个非常好的效果了

101
00:03:23,933 --> 00:03:25,800
就是当时候 21 年的时候

102
00:03:25,800 --> 00:03:27,033
已经有了 scaringer 了

103
00:03:27,033 --> 00:03:29,566
或者大家都已经在提 skringer 的方式了

104
00:03:29,600 --> 00:03:30,233
但是

105
00:03:30,233 --> 00:03:32,433
带来一个非常高的一个代价

106
00:03:32,433 --> 00:03:34,133
这个代价就是我们现在

107
00:03:34,166 --> 00:03:38,133
很难稳定的去训练整个 MoE 的模型

108
00:03:38,433 --> 00:03:38,833
不过

109
00:03:38,833 --> 00:03:41,700
在最新的 M1 的专家的一些文章里面

110
00:03:41,733 --> 00:03:42,566
就说到了

111
00:03:42,566 --> 00:03:45,366
其实你去训练一些 M1 的模型

112
00:03:45,366 --> 00:03:46,866
或者对它进行一个微调了

113
00:03:46,933 --> 00:03:49,333
其实会带来一个很不确定

114
00:03:49,333 --> 00:03:50,833
或者不稳定性

115
00:03:50,833 --> 00:03:53,266
还有 uncertain 相关的内容

116
00:03:53,333 --> 00:03:54,333
那这篇文章

117
00:03:54,333 --> 00:03:56,666
就解决这两个内容

118
00:03:56,666 --> 00:03:58,133
就为了让模型

119
00:03:58,166 --> 00:04:00,800
特别是 MoE 的专家训练的更加稳定

120
00:04:00,800 --> 00:04:02,400
因此就发表了

121
00:04:02,400 --> 00:04:05,766
或者发布了一系列的这个 ST-MoE 的模型

122
00:04:05,766 --> 00:04:06,733
那这里面

123
00:04:06,733 --> 00:04:09,066
主要是用 ST-MoE 32B

124
00:04:09,400 --> 00:04:10,733
而去证明它的一个稳定性

125
00:04:10,733 --> 00:04:11,800
和可迁移性

126
00:04:11,800 --> 00:04:13,766
在很多的校任务里面

127
00:04:13,766 --> 00:04:15,633
就取得了非常好的效果

128
00:04:15,800 --> 00:04:16,433
整篇文章

129
00:04:16,433 --> 00:04:18,666
还是围绕着稳定性来去开展

130
00:04:19,366 --> 00:04:20,766
接着我们看一下第二页

131
00:04:20,766 --> 00:04:21,966
第二页还蛮有意思

132
00:04:21,966 --> 00:04:23,733
里面就有一个 content

133
00:04:23,733 --> 00:04:25,200
就对应的目录大纲

134
00:04:25,233 --> 00:04:27,033
那这里面第三个小节

135
00:04:27,033 --> 00:04:28,533
就可能比较关心

136
00:04:28,566 --> 00:04:31,766
就专门的去提升或者专门的去研究

137
00:04:31,766 --> 00:04:34,533
stable life 就是一个稳定性

138
00:04:34,533 --> 00:04:35,933
在 M1 专家的时候

139
00:04:35,933 --> 00:04:38,233
还有一些 FI tolding 的性能

140
00:04:38,433 --> 00:04:39,900
最后第五章跟第六章

141
00:04:39,933 --> 00:04:42,200
更多是一些实验性的内容了

142
00:04:42,200 --> 00:04:42,966
那我们重点

143
00:04:42,966 --> 00:04:46,366
打开第三节跟第四节相关的内容

144
00:04:46,833 --> 00:04:47,366
首先

145
00:04:47,400 --> 00:04:50,200
introduction 就是第一个的背景介绍

146
00:04:50,200 --> 00:04:50,833
蛮有意思

147
00:04:50,833 --> 00:04:52,166
反正现在

148
00:04:52,200 --> 00:04:53,833
不管罗列了非常多的稀

149
00:04:53,833 --> 00:04:54,733
疏的专家

150
00:04:54,800 --> 00:04:57,400
但是这些专家的参数量虽然很大

151
00:04:57,400 --> 00:04:59,933
然后可以取得相对好一点的效果

152
00:04:59,933 --> 00:05:00,933
比崇明模型

153
00:05:00,933 --> 00:05:02,466
但是训练的稳定性

154
00:05:02,466 --> 00:05:04,100
还是比较不乐观

155
00:05:04,466 --> 00:05:07,100
这里面就讲到了 HF 不稳定性的问题

156
00:05:07,133 --> 00:05:09,766
其实之前没有很好的去解决

157
00:05:09,800 --> 00:05:11,733
所以说这篇文章的 in

158
00:05:11,733 --> 00:05:12,666
它的一个目标

159
00:05:12,666 --> 00:05:13,500
就是去解决

160
00:05:13,533 --> 00:05:15,966
我们在实际训练的过程当中

161
00:05:16,166 --> 00:05:17,766
让 M1 专家

162
00:05:17,766 --> 00:05:18,800
更加的稳定

163
00:05:18,800 --> 00:05:20,133
更好的去收敛

164
00:05:20,166 --> 00:05:22,833
那整篇文章的 contribution

165
00:05:22,833 --> 00:05:24,466
就是它的一个贡献了

166
00:05:24,733 --> 00:05:26,466
点就讲了几个东西

167
00:05:26,466 --> 00:05:26,833
第一个

168
00:05:26,833 --> 00:05:28,166
就是 stability

169
00:05:28,200 --> 00:05:29,366
也就是稳定性了

170
00:05:29,366 --> 00:05:30,866
还是一切围绕着稳定性

171
00:05:30,933 --> 00:05:31,266
第二个

172
00:05:31,266 --> 00:05:33,733
介绍了一个路由的 Z-Loss

173
00:05:33,766 --> 00:05:34,733
那这个 Z-Loss

174
00:05:34,733 --> 00:05:37,933
主要还是去解决不稳定性的问题

175
00:05:37,933 --> 00:05:39,933
所以一切围绕着不稳定性

176
00:05:39,933 --> 00:05:43,333
然后对一些模型的规模

177
00:05:43,533 --> 00:05:46,133
也做了一个不同的一个测试

178
00:05:46,133 --> 00:05:47,366
和解决方案了

179
00:05:47,366 --> 00:05:48,833
最后就是模型加构

180
00:05:48,833 --> 00:05:50,566
这里面的模型加构其实总比学

181
00:05:50,633 --> 00:05:52,766
没有太多的新的东西

182
00:05:52,800 --> 00:05:54,533
更多的是对稳定性

183
00:05:54,533 --> 00:05:56,433
提出一些新的内容

184
00:05:57,066 --> 00:05:57,533
那接着

185
00:05:57,566 --> 00:05:58,733
就在 back 馆

186
00:05:58,733 --> 00:06:00,133
back 馆就是对应的背景

187
00:06:00,133 --> 00:06:03,400
那这背景就是传统的 MoE 的专家

188
00:06:03,400 --> 00:06:04,966
他怎么去计算

189
00:06:04,966 --> 00:06:06,133
例如我的 sport i

190
00:06:06,133 --> 00:06:07,000
怎么计算

191
00:06:07,000 --> 00:06:08,166
有了 sport i 之后

192
00:06:08,166 --> 00:06:10,633
我的 gate 门控网络的怎么计算

193
00:06:10,633 --> 00:06:11,466
那基本上

194
00:06:11,466 --> 00:06:12,666
会在 ZOMI 之前

195
00:06:12,666 --> 00:06:14,966
一系列的一种视频里面

196
00:06:15,000 --> 00:06:18,033
跟大家已经分享过相关的内容了

197
00:06:18,033 --> 00:06:19,366
有朋友觉得哎

198
00:06:19,400 --> 00:06:21,533
我看了一系列的 MoE 的这种论文

199
00:06:21,533 --> 00:06:24,000
我还是不知道 MoE 长什么样子没

200
00:06:24,000 --> 00:06:26,733
关系朱米会在下一期的视频

201
00:06:26,733 --> 00:06:28,166
或者后面两个视频

202
00:06:28,166 --> 00:06:30,066
跟大家重点的去看一看

203
00:06:30,266 --> 00:06:32,366
MoE 的可视化的内容

204
00:06:33,066 --> 00:06:34,300
那这篇文章来说

205
00:06:34,333 --> 00:06:37,333
还是为了改善这个硬件的利用率

206
00:06:37,333 --> 00:06:38,800
大部分的 MoE 的架构

207
00:06:38,800 --> 00:06:40,600
其实都是去实现

208
00:06:40,600 --> 00:06:43,133
每个专家在静态的 patch 下面了

209
00:06:43,133 --> 00:06:45,200
然后去一个执行

210
00:06:45,433 --> 00:06:46,766
这个也是对应谷歌

211
00:06:46,800 --> 00:06:48,400
当时候谷歌研究 MOV

212
00:06:48,400 --> 00:06:50,600
主要是在 TPU 上面去研究

213
00:06:50,600 --> 00:06:53,433
所以说比较关心的就是 static batch

214
00:06:53,433 --> 00:06:54,633
那现在我们看到

215
00:06:54,633 --> 00:06:56,533
很多的一个 ASIC 的 GPU

216
00:06:56,733 --> 00:06:58,766
其实或者 ASIC 的一个 MPU

217
00:06:58,766 --> 00:07:01,466
都是主要是处理一个静态的数据

218
00:07:01,466 --> 00:07:02,333
动态的数据

219
00:07:02,366 --> 00:07:05,233
说实话相对来说硬件的性能

220
00:07:05,233 --> 00:07:06,266
没那么的好

221
00:07:06,633 --> 00:07:07,533
针对底层的编辑器

222
00:07:07,566 --> 00:07:09,533
也做了很多相关的优化

223
00:07:09,800 --> 00:07:10,766
那不管怎么样

224
00:07:10,766 --> 00:07:13,133
在 introduction 跟后面的第二个内容

225
00:07:13,133 --> 00:07:13,766
我们可以看到

226
00:07:13,766 --> 00:07:16,000
大部分都是讲一些背景相关的介绍

227
00:07:16,200 --> 00:07:17,033
我们现在来重点

228
00:07:17,033 --> 00:07:18,900
还是来到了一个第三个内容

229
00:07:18,933 --> 00:07:20,633
也就是 stablebility

230
00:07:20,666 --> 00:07:23,066
stable 稳定性相关的内容

231
00:07:23,066 --> 00:07:25,533
去看一下怎么去训练一个稀疏的模型

232
00:07:25,566 --> 00:07:26,766
那这个稀疏的模型

233
00:07:26,766 --> 00:07:27,400
主要就是讲

234
00:07:27,400 --> 00:07:30,033
MoE 的一个模型结构

235
00:07:30,366 --> 00:07:31,166
我们应该有一次

236
00:07:31,166 --> 00:07:33,400
就看一下左边的这个图了

237
00:07:33,400 --> 00:07:35,533
就是我们训练的时候的稳定性的问题

238
00:07:35,566 --> 00:07:36,133
可以看到

239
00:07:36,133 --> 00:07:38,566
左边的就是一个不稳定的训练了

240
00:07:38,566 --> 00:07:40,733
虽然终于觉得左边这个图有点扯淡

241
00:07:40,733 --> 00:07:43,133
随着训练的时间越来越长

242
00:07:43,133 --> 00:07:44,666
训练 step 数越来越长

243
00:07:44,766 --> 00:07:45,866
但是 loss

244
00:07:45,866 --> 00:07:47,100
突然就跑飞了

245
00:07:47,133 --> 00:07:48,000
那这个跑飞

246
00:07:48,000 --> 00:07:49,633
是什么原因引起

247
00:07:49,633 --> 00:07:50,833
这篇文章下面没

248
00:07:50,833 --> 00:07:52,333
有做一个重点的分析

249
00:07:52,633 --> 00:07:55,033
说到了可能我有这么一个可能性

250
00:07:55,066 --> 00:07:55,833
但钟敏觉得

251
00:07:55,833 --> 00:07:56,700
可能性不太多

252
00:07:56,733 --> 00:07:58,066
那现在我们训练大模型

253
00:07:58,066 --> 00:07:59,266
更多的是这种方式

254
00:07:59,266 --> 00:08:00,666
训着训着突然跑飞了

255
00:08:00,666 --> 00:08:01,333
然后又回来了

256
00:08:01,366 --> 00:08:01,866
突然跑飞了

257
00:08:01,866 --> 00:08:02,533
又回来了

258
00:08:02,566 --> 00:08:03,766
突然跑飞了又回来了

259
00:08:03,766 --> 00:08:05,966
这么一种线的形态

260
00:08:05,966 --> 00:08:07,733
那我们看一下右边的这个

261
00:08:07,733 --> 00:08:10,066
就是稳定训练的一个内容

262
00:08:10,066 --> 00:08:12,433
随着我们训练的 step 数增加

263
00:08:12,433 --> 00:08:13,266
整体的 loss

264
00:08:13,266 --> 00:08:14,700
市场下降的趋势

265
00:08:14,733 --> 00:08:16,600
没有说一些突然跑飞的问题

266
00:08:16,600 --> 00:08:16,966
当然

267
00:08:16,966 --> 00:08:19,666
左边的这种突然跑飞也是会存在

268
00:08:19,666 --> 00:08:21,966
不过现在在大模型结构里面的话

269
00:08:22,000 --> 00:08:24,400
大家已经基本上模型结构收敛

270
00:08:24,400 --> 00:08:27,233
就不会出现太多的这种问题

271
00:08:27,733 --> 00:08:28,733
整篇文章

272
00:08:28,733 --> 00:08:29,333
说到了

273
00:08:29,333 --> 00:08:31,333
为了解决这个训练不稳定性问题

274
00:08:31,333 --> 00:08:32,066
就是上面

275
00:08:32,066 --> 00:08:32,866
对应这个图

276
00:08:32,866 --> 00:08:34,966
所以就提出了 3 种方法

277
00:08:35,033 --> 00:08:38,166
第一种就是删除乘法的一个相互影响

278
00:08:38,433 --> 00:08:41,500
那第二种就是注入模型的噪声

279
00:08:41,533 --> 00:08:44,166
第三种就约束一个激活值

280
00:08:44,166 --> 00:08:45,666
跟梯度结合

281
00:08:45,666 --> 00:08:48,933
这些我们就发明了一个 vote Z-Loss

282
00:08:49,166 --> 00:08:50,166
Z-Loss 也好了

283
00:08:50,166 --> 00:08:52,833
就是路由的 Z-Loss 的一个损失函数

284
00:08:52,833 --> 00:08:55,866
去解决稳定性问题

285
00:08:55,933 --> 00:08:58,066
那我们接下来要往下看一看

286
00:08:58,766 --> 00:08:59,600
那整篇文章

287
00:08:59,600 --> 00:09:00,733
所有的实验

288
00:09:00,733 --> 00:09:03,200
都是围绕一个非常大规模

289
00:09:03,200 --> 00:09:05,333
稳定性的一个研究

290
00:09:05,333 --> 00:09:06,000
那重点

291
00:09:06,000 --> 00:09:09,000
是对一个网络模型

292
00:09:09,000 --> 00:09:10,366
进行各种各样的研究

293
00:09:10,366 --> 00:09:11,533
然后不同的层

294
00:09:11,533 --> 00:09:12,600
不同的 FFN

295
00:09:12,600 --> 00:09:14,466
不同的修改容量因子

296
00:09:14,466 --> 00:09:15,300
还有评价

297
00:09:15,333 --> 00:09:17,266
容量因子相关的内容

298
00:09:17,266 --> 00:09:18,333
那这里面就说到了

299
00:09:18,366 --> 00:09:21,333
我怎么去改这个 T5-XL 的模型

300
00:09:21,333 --> 00:09:23,833
然后怎么去对它进行模改相关的实验

301
00:09:24,266 --> 00:09:24,900
那其实

302
00:09:24,933 --> 00:09:28,333
比较核心的还是 3 下面的几个小节

303
00:09:28,333 --> 00:09:28,833
那第一个

304
00:09:28,833 --> 00:09:29,266
小节

305
00:09:29,266 --> 00:09:32,333
就是消除乘法互相影响的时候

306
00:09:32,366 --> 00:09:33,533
的一个稳定性

307
00:09:33,866 --> 00:09:34,466
第一个很有意思

308
00:09:34,466 --> 00:09:37,300
就是消除乘法一个互相的影响

309
00:09:37,366 --> 00:09:39,800
我们回顾整个 Transformer 架构的时候

310
00:09:39,800 --> 00:09:40,666
你会发现

311
00:09:40,666 --> 00:09:42,166
乘法之间

312
00:09:42,200 --> 00:09:43,933
是不断的去相乘

313
00:09:43,933 --> 00:09:46,366
反正这一层网络模型的架构

314
00:09:46,366 --> 00:09:47,166
使用了乘法

315
00:09:47,233 --> 00:09:48,500
下一层网络模型架构

316
00:09:48,533 --> 00:09:49,800
又使用各样的乘法

317
00:09:49,800 --> 00:09:50,233
然后

318
00:09:50,233 --> 00:09:52,933
又加入一种相关激活

319
00:09:52,966 --> 00:09:54,733
还是激活值的方式

320
00:09:54,933 --> 00:09:57,133
反正就这篇文章就觉得这种方式

321
00:09:57,133 --> 00:09:58,000
可能不太好

322
00:09:58,000 --> 00:09:59,633
所以就改了几个东西

323
00:09:59,633 --> 00:09:59,966
第一个

324
00:10:00,000 --> 00:10:03,066
就是使用了一个 GELU 记录

325
00:10:03,066 --> 00:10:06,033
去代替了一个传统的 v 录的方式

326
00:10:06,033 --> 00:10:06,900
我们可以看到

327
00:10:06,933 --> 00:10:07,566
一般来说

328
00:10:07,566 --> 00:10:10,200
我们在大模型训练的时候都会加 v 录

329
00:10:10,200 --> 00:10:10,933
但是后面

330
00:10:10,933 --> 00:10:12,666
基本上会用了一个记录

331
00:10:12,666 --> 00:10:14,233
那这也是这篇文章说到了

332
00:10:14,233 --> 00:10:17,433
用了记录更好的做一个模型的收敛

333
00:10:17,766 --> 00:10:20,766
第二个就是把我们经常用到的 layer Lama

334
00:10:20,766 --> 00:10:22,400
全缩码加工用到的 layer Lama

335
00:10:22,400 --> 00:10:24,633
慢慢的换成一个 RMSNorm

336
00:10:24,666 --> 00:10:28,233
也就是对应的均方根的一个归一化

337
00:10:28,233 --> 00:10:29,900
那后面的大模型训练

338
00:10:29,933 --> 00:10:32,766
基本上都慢慢的转成了一个 RMSNorm

339
00:10:32,766 --> 00:10:33,933
相关的内容

340
00:10:33,933 --> 00:10:35,966
去进行一个学习

341
00:10:36,333 --> 00:10:37,800
那通过一个 RMSNorm

342
00:10:37,800 --> 00:10:39,733
跟我们刚才看到的角度

343
00:10:39,733 --> 00:10:42,433
两种乘法或者两种计算的方式

344
00:10:42,433 --> 00:10:45,500
就能很好地去提升 stability

345
00:10:45,533 --> 00:10:46,466
稳定性

346
00:10:47,000 --> 00:10:49,333
那这个是关于我们乘法的一个计算

347
00:10:49,333 --> 00:10:50,966
会对函数的计算

348
00:10:51,133 --> 00:10:52,733
第 3.2 来就讲到了哎

349
00:10:52,733 --> 00:10:55,233
除了刚才的乘法的替换以外

350
00:10:55,233 --> 00:10:57,666
我们现在还可以加入更多的噪声

351
00:10:57,666 --> 00:10:59,666
那这些噪声怎么去增加

352
00:10:59,666 --> 00:11:02,966
这里面就讲到了在模型训练的时候

353
00:11:03,000 --> 00:11:04,766
加入了更多的噪声

354
00:11:04,766 --> 00:11:07,433
例如铸造商和 job out

355
00:11:07,433 --> 00:11:08,566
都能够很好

356
00:11:08,600 --> 00:11:10,666
去提升模型的稳定性

357
00:11:10,733 --> 00:11:13,033
但是训练的模型更稳定

358
00:11:13,033 --> 00:11:14,033
带来一个问题

359
00:11:14,033 --> 00:11:15,433
就是这里面说到了

360
00:11:15,433 --> 00:11:17,133
会导致模型的质量

361
00:11:17,166 --> 00:11:20,066
或者模型的一个效果下降

362
00:11:20,066 --> 00:11:21,933
所以说稳定性这个问题

363
00:11:21,966 --> 00:11:24,400
就是如果我们需要更稳定的网络模型

364
00:11:24,400 --> 00:11:25,933
我们可以加入更多的噪声

365
00:11:25,933 --> 00:11:27,533
让训练的过程当中

366
00:11:27,600 --> 00:11:30,533
更好的去平衡我们所有的一个激活值

367
00:11:30,566 --> 00:11:31,833
但是也可能

368
00:11:31,833 --> 00:11:34,333
导致模型效果不好

369
00:11:34,533 --> 00:11:36,833
那为了体解决模型效果不好

370
00:11:36,833 --> 00:11:39,833
之后又来到了第三个内容了

371
00:11:39,966 --> 00:11:40,433
第三个 6

372
00:11:40,433 --> 00:11:43,566
就是限制对应的激活词

373
00:11:43,600 --> 00:11:45,000
和对应的梯度

374
00:11:45,000 --> 00:11:47,433
使得整个模型的效果更加好

375
00:11:47,433 --> 00:11:49,700
然后模型训练更加稳定

376
00:11:49,766 --> 00:11:50,333
同样

377
00:11:50,333 --> 00:11:53,066
上面这些都是引用之前历史的论文

378
00:11:53,133 --> 00:11:55,833
那 next 就是我们研究完一系列之后

379
00:11:56,066 --> 00:11:56,533
接下来

380
00:11:56,566 --> 00:11:59,233
就需要设置一个新的一个路由器了

381
00:11:59,233 --> 00:12:02,300
那路由器的计算就主要使用 FP32

382
00:12:02,333 --> 00:12:05,333
而不是使用 FP8 或者 FP16

383
00:12:05,333 --> 00:12:06,433
相关的内容

384
00:12:06,533 --> 00:12:09,266
再随着一个 largest scale

385
00:12:09,266 --> 00:12:11,100
就是网络模型的规模

386
00:12:11,133 --> 00:12:11,866
越来越大

387
00:12:11,866 --> 00:12:12,833
就非常有必要

388
00:12:12,833 --> 00:12:15,333
去设计一个新的一个损失函数

389
00:12:15,733 --> 00:12:18,633
Z-Losss 去提升模型的稳定性

390
00:12:18,633 --> 00:12:21,100
那下面就看一下这条 Z-Losss 的公式

391
00:12:21,200 --> 00:12:23,166
首先这里面的一个 n

392
00:12:23,166 --> 00:12:24,966
对应的是专家的数量

393
00:12:24,966 --> 00:12:27,400
b 对应的是一个 TOKEN 的数量

394
00:12:27,433 --> 00:12:29,900
对应这里面用了两个方式

395
00:12:29,933 --> 00:12:30,833
一个是指数

396
00:12:30,833 --> 00:12:31,533
再取对数

397
00:12:31,566 --> 00:12:33,366
然后除以 TOKEN 数量

398
00:12:33,733 --> 00:12:34,800
这种方式的计算

399
00:12:34,800 --> 00:12:37,000
就表明了我进入陆游的时候

400
00:12:37,000 --> 00:12:39,466
去惩罚较大 logist 的值

401
00:12:39,566 --> 00:12:41,433
就对 logist 的值

402
00:12:41,433 --> 00:12:42,833
就我们专家算出来的值

403
00:12:42,833 --> 00:12:43,900
进行一个惩罚

404
00:12:44,000 --> 00:12:45,633
然后如果我们比较小的时候

405
00:12:45,633 --> 00:12:46,366
就不惩罚

406
00:12:46,400 --> 00:12:47,466
那这样的方式

407
00:12:47,466 --> 00:12:50,100
就使得我们尽可能的减少

408
00:12:50,166 --> 00:12:53,066
进入指数函数里面的较大的误差

409
00:12:53,866 --> 00:12:55,966
也就是对一些损失值

410
00:12:56,000 --> 00:12:58,166
或者梯度激活值较大的时候

411
00:12:58,166 --> 00:12:59,400
进行一个惩罚

412
00:12:59,400 --> 00:13:00,033
让网

413
00:13:00,033 --> 00:13:02,733
整个网络模型更加的均衡

414
00:13:02,800 --> 00:13:04,833
参数值更加均衡

415
00:13:04,833 --> 00:13:05,766
通过这种方式

416
00:13:05,800 --> 00:13:08,533
使得模型训练更加稳定

417
00:13:09,066 --> 00:13:09,466
当然

418
00:13:09,466 --> 00:13:11,633
这个 Z-Losss 不是自己去训练

419
00:13:11,633 --> 00:13:12,700
也不是单独

420
00:13:12,733 --> 00:13:14,933
它是跟其他的一个损失函数

421
00:13:14,933 --> 00:13:15,833
组合起来

422
00:13:15,833 --> 00:13:17,233
所以叫做 l TOTO

423
00:13:17,333 --> 00:13:20,200
l TOTO 是由三个损失函数组成

424
00:13:20,200 --> 00:13:21,800
第一个就是 LCE

425
00:13:21,800 --> 00:13:23,666
也就是交叉商损失函数

426
00:13:23,666 --> 00:13:26,366
第二个就是 LB2 对应的 switch

427
00:13:26,400 --> 00:13:29,600
像 so one 里面的一个均衡负载的 los

428
00:13:29,600 --> 00:13:34,066
第三个就是 l z 里面的一个路由 Z-Loss

429
00:13:34,066 --> 00:13:37,033
通过这三个损失函数的累加

430
00:13:37,033 --> 00:13:40,033
使得网络模型训练更加稳定

431
00:13:40,733 --> 00:13:42,833
那第三点事就讲到了 A1

432
00:13:42,833 --> 00:13:43,533
我这里面

433
00:13:43,566 --> 00:13:45,200
模型的精度也很重要

434
00:13:45,200 --> 00:13:46,166
我们现在

435
00:13:46,166 --> 00:13:47,566
经常训练大模型

436
00:13:47,566 --> 00:13:49,766
都是用 BF16 来去训练

437
00:13:49,766 --> 00:13:51,000
那这个 BF16

438
00:13:51,000 --> 00:13:53,566
最早期的来源是谷歌的这

439
00:13:53,566 --> 00:13:57,400
篇文章里面用 TPU 的 BF16 来证明

440
00:13:57,400 --> 00:13:58,400
整个大模型

441
00:13:58,400 --> 00:14:01,566
特别是 MoE 架构的训练的可获取性

442
00:14:01,566 --> 00:14:02,833
或者可实现性

443
00:14:02,833 --> 00:14:04,766
那真正幻方了 DeepSeek

444
00:14:04,800 --> 00:14:06,733
现在大家觉得非常惊艳的是

445
00:14:06,733 --> 00:14:07,466
它第一个

446
00:14:07,466 --> 00:14:10,766
把这个 position formate 也是重新刷新了

447
00:14:10,800 --> 00:14:13,966
然后使用了一个 FP 8 来去训练

448
00:14:13,966 --> 00:14:16,466
那谷歌的这篇文章 STMoE

449
00:14:16,466 --> 00:14:18,366
使用的是一个 BF16

450
00:14:18,733 --> 00:14:20,000
所以说这两篇文章

451
00:14:20,000 --> 00:14:22,433
在就 DeepSeek 跟 STM 一样

452
00:14:22,466 --> 00:14:25,433
幻方跟谷歌在整个 MoE 加固里面

453
00:14:25,433 --> 00:14:27,666
说实话这两篇文章举足轻重

454
00:14:27,666 --> 00:14:30,033
所以今天跟大家去简单的唠一唠

455
00:14:30,366 --> 00:14:30,966
那我们看一下

456
00:14:30,966 --> 00:14:32,666
整个低精度

457
00:14:32,666 --> 00:14:35,166
FP16 的一个训练的一个好处

458
00:14:35,200 --> 00:14:37,533
那第一个就是较低的精度格式

459
00:14:37,533 --> 00:14:39,200
可以通过降低

460
00:14:39,200 --> 00:14:40,233
处理器

461
00:14:40,233 --> 00:14:42,333
跟内存之间的一个通讯成本

462
00:14:42,366 --> 00:14:44,133
communication 六个

463
00:14:44,133 --> 00:14:45,600
就是计算的成本

464
00:14:45,633 --> 00:14:46,900
精度格式少了

465
00:14:46,933 --> 00:14:48,800
我们用的晶体管也小了

466
00:14:48,800 --> 00:14:49,366
第三个

467
00:14:49,366 --> 00:14:52,766
就是一个存储的压力也减少了

468
00:14:52,766 --> 00:14:53,600
通过这种方式

469
00:14:53,600 --> 00:14:54,600
就说明了

470
00:14:54,600 --> 00:14:57,533
AB 26 的精度格式也是很能用

471
00:14:57,533 --> 00:14:59,366
那接着我们就往下看一下

472
00:14:59,366 --> 00:15:01,066
到了第四个内容

473
00:15:01,066 --> 00:15:01,666
第四个内容

474
00:15:01,666 --> 00:15:04,366
更多的是一个 fine tooling 的一个 performance

475
00:15:04,466 --> 00:15:06,533
因为在整个 Transformer 的时代

476
00:15:06,566 --> 00:15:09,200
我们整个 Transformer 架构是有一个 incord

477
00:15:09,200 --> 00:15:10,166
一个 decord

478
00:15:10,200 --> 00:15:12,566
当时候的训练模式有两种

479
00:15:12,566 --> 00:15:14,833
第一种就是大规模的预训练

480
00:15:14,833 --> 00:15:15,300
第二种

481
00:15:15,333 --> 00:15:17,533
就是对我们预训练完的模型

482
00:15:17,833 --> 00:15:18,566
进行一个微调

483
00:15:18,600 --> 00:15:21,333
来获得下游任务的一个性能的提升

484
00:15:21,333 --> 00:15:23,133
所以针对每一个下游任务

485
00:15:23,133 --> 00:15:25,000
都需要进行一个微调

486
00:15:25,000 --> 00:15:26,933
但是我们现在的大模型

487
00:15:27,066 --> 00:15:28,133
越来越少微调了

488
00:15:28,166 --> 00:15:29,600
是因为整个大模型

489
00:15:29,600 --> 00:15:31,833
用的是 decorder only 的架构

490
00:15:31,833 --> 00:15:32,933
那模型的算法

491
00:15:32,966 --> 00:15:34,266
架构也演进了

492
00:15:34,266 --> 00:15:36,700
所以这篇论文的后面的 fitune

493
00:15:36,733 --> 00:15:39,400
总理觉得可以跟大家简单的过一过

494
00:15:39,400 --> 00:15:41,400
第一个就是稀疏的模型

495
00:15:41,400 --> 00:15:42,600
容易过敏和

496
00:15:42,666 --> 00:15:45,133
就我们可以看一下这里面的一些参数

497
00:15:45,166 --> 00:15:47,666
那通过 space 的相关的一个模型

498
00:15:47,666 --> 00:15:48,433
可以看到

499
00:15:48,433 --> 00:15:49,566
越稀疏的模型

500
00:15:49,600 --> 00:15:50,766
越难去训练

501
00:15:50,766 --> 00:15:52,400
没有那么容易的去训练

502
00:15:52,400 --> 00:15:53,766
所以我们稀疏的模型

503
00:15:53,766 --> 00:15:55,800
有可能是导致过敏和

504
00:15:56,066 --> 00:15:58,033
既然稀疏的模型容易过敏和

505
00:15:58,033 --> 00:16:00,866
所以在小模型的方块性就有待提升

506
00:16:00,866 --> 00:16:02,333
然后操叉

507
00:16:02,366 --> 00:16:03,866
也是非常的敏感

508
00:16:03,866 --> 00:16:05,300
就 MoE 的架构的训练

509
00:16:05,333 --> 00:16:06,633
其实对参数来说

510
00:16:06,633 --> 00:16:07,633
是非常敏感

511
00:16:07,633 --> 00:16:11,166
所以需要重点去用心的去调参

512
00:16:11,466 --> 00:16:12,233
在 4.3 里面

513
00:16:12,233 --> 00:16:14,300
就讲到了稀疏跟稠密的模型

514
00:16:14,333 --> 00:16:16,600
需要不同的一个超参的调优

515
00:16:16,600 --> 00:16:19,366
所以大家现在会发现很多的大厂

516
00:16:19,366 --> 00:16:20,633
因为 DeepSeek 火起来之后

517
00:16:20,633 --> 00:16:23,733
就开始从稠密的模型架构了转向 MoE

518
00:16:23,866 --> 00:16:25,666
但是整个 MoE 的架构

519
00:16:25,666 --> 00:16:28,666
其实跟稠密模型的架构的一个超参

520
00:16:28,666 --> 00:16:31,933
用到的 Hyper Paramite 是不一样

521
00:16:31,966 --> 00:16:32,533
所以这里面

522
00:16:32,533 --> 00:16:33,366
就重点研究了

523
00:16:33,366 --> 00:16:36,166
学习率跟一个 bitch size 的数量

524
00:16:36,200 --> 00:16:38,400
对整个网络模型的影响

525
00:16:38,400 --> 00:16:41,600
那最后就还是一系列的这种研究

526
00:16:42,333 --> 00:16:43,400
那最后就讲到了

527
00:16:43,400 --> 00:16:44,966
一个网络模型

528
00:16:44,966 --> 00:16:46,266
稀疏的网络模型架构

529
00:16:46,266 --> 00:16:47,633
是怎么去实现

530
00:16:47,633 --> 00:16:49,300
那这里面整个网络模型架构

531
00:16:49,333 --> 00:16:50,600
就没有可视化的内容

532
00:16:50,600 --> 00:16:51,733
简单的几句话

533
00:16:51,733 --> 00:16:53,600
因为现在的 MoE 架构

534
00:16:53,600 --> 00:16:54,766
基本上已经收敛了

535
00:16:54,766 --> 00:16:56,800
我们将会在后面的一个可视化内容

536
00:16:56,800 --> 00:16:58,000
跟大家一起去看看

537
00:16:58,000 --> 00:17:00,033
MoE 架构到底长什么样子

538
00:17:00,600 --> 00:17:02,133
在这个网络我们新加坡里面

539
00:17:02,133 --> 00:17:03,866
就选择了一个 top two 的路由

540
00:17:03,866 --> 00:17:07,333
还有 1.5 的一个容量因子

541
00:17:07,366 --> 00:17:09,466
在每一个计算核心里面

542
00:17:09,466 --> 00:17:11,500
只放一个具体的专家

543
00:17:11,533 --> 00:17:13,866
也满足我们现在训练的一个方式

544
00:17:13,933 --> 00:17:15,400
在一个评估方案

545
00:17:15,400 --> 00:17:17,833
就修改容量因子

546
00:17:18,200 --> 00:17:18,600
最后

547
00:17:18,600 --> 00:17:21,433
还设计了一个稠密的一个成堆叠

548
00:17:21,433 --> 00:17:22,700
不过 ZOMI 也没搞清楚

549
00:17:22,733 --> 00:17:24,333
第三个内容指的是什么

550
00:17:24,600 --> 00:17:25,733
现在已经不太重要了

551
00:17:25,733 --> 00:17:26,633
因为这篇文章

552
00:17:26,633 --> 00:17:28,533
更多的是像我们刚才讲到

553
00:17:28,566 --> 00:17:30,066
其实 ZOMI 比较关心

554
00:17:30,066 --> 00:17:33,333
就是整个 ST MoE 这一个内容

555
00:17:33,366 --> 00:17:35,400
更重要的就是训练稳定性的问题

556
00:17:35,400 --> 00:17:37,833
把以前的一个 v 录了改成剧录

557
00:17:37,833 --> 00:17:40,433
把之前的 LayerNorm 改成一个 RMSNorm

558
00:17:40,433 --> 00:17:42,366
那后面的一些大模型也好

559
00:17:42,400 --> 00:17:43,933
全 SOMO 的 incord 也好

560
00:17:43,933 --> 00:17:45,566
基本上都参考这个方案

561
00:17:45,566 --> 00:17:47,966
就发现网络模型训练

562
00:17:47,966 --> 00:17:48,933
更加稳定

563
00:17:48,933 --> 00:17:49,466
那后面

564
00:17:49,466 --> 00:17:52,733
就是更多的 experience 相关的内容了

565
00:17:52,766 --> 00:17:55,566
那今天我们回到 PPT 里面

566
00:17:55,566 --> 00:17:57,133
已经跟大家介绍完

567
00:17:57,133 --> 00:17:59,733
ST-MoE 的相关的内容了

568
00:17:59,733 --> 00:18:00,266
那接下来

569
00:18:00,266 --> 00:18:03,300
我们分享下一篇文章相关的内容

570
00:18:04,033 --> 00:18:04,533
我们接下来

571
00:18:04,566 --> 00:18:08,000
来到了 GLaM 这篇文章的介绍

572
00:18:08,000 --> 00:18:10,133
为什么 GLaM 这篇文章的介绍

573
00:18:10,133 --> 00:18:12,766
会跟整个全松网时代的一个 ST-MoE

574
00:18:12,766 --> 00:18:13,533
放在一起

575
00:18:13,533 --> 00:18:17,833
是因为这篇文章的 GLaM 太简单了

576
00:18:17,833 --> 00:18:19,766
所以说你有一个好的 ID

577
00:18:19,800 --> 00:18:21,433
然后你赶上一个好的时机

578
00:18:21,433 --> 00:18:24,700
确实可以发表一篇好的文章哈哈哈

579
00:18:29,133 --> 00:18:29,433
那我们现

580
00:18:29,433 --> 00:18:33,066
在主要是打开 GLaM 一篇文章

581
00:18:33,066 --> 00:18:34,100
那 GLaM

582
00:18:34,133 --> 00:18:35,166
蛮有意思的就是

583
00:18:35,166 --> 00:18:35,733
其实

584
00:18:35,733 --> 00:18:38,366
最重要的就没什么太多的不一样

585
00:18:38,366 --> 00:18:40,600
我们留意一下它的一个标题

586
00:18:40,600 --> 00:18:44,733
叫做 efficient scaling of language model of MoE

587
00:18:44,733 --> 00:18:45,266
那可以看到

588
00:18:45,266 --> 00:18:45,700
它的文章

589
00:18:45,733 --> 00:18:49,066
好像除了说 scaling 大规模以外呀

590
00:18:49,066 --> 00:18:50,666
没有太多的新的东西了

591
00:18:50,666 --> 00:18:51,966
那第一作者我们看一下

592
00:18:52,000 --> 00:18:53,066
还有 Nan Du

593
00:18:53,066 --> 00:18:54,966
那蛮有意思的就是嗯

594
00:18:55,000 --> 00:18:57,400
Nan Du 他这个已经是刚才讲到

595
00:18:57,400 --> 00:18:59,033
ST-MoE 的第二作者

596
00:18:59,200 --> 00:18:59,666
那这里面

597
00:18:59,666 --> 00:19:02,433
我们就看一下它的一个整个网络模型

598
00:19:02,433 --> 00:19:03,933
或者看一下它下面的内容

599
00:19:04,033 --> 00:19:04,533
那第一个

600
00:19:04,566 --> 00:19:05,366
就是摘要

601
00:19:05,366 --> 00:19:06,600
对应摘要的事情就是

602
00:19:06,600 --> 00:19:08,566
为什么 GIAM 这么去命名

603
00:19:08,600 --> 00:19:09,833
g 不是谷歌的意思

604
00:19:09,833 --> 00:19:11,366
而是 geneticity

605
00:19:11,400 --> 00:19:12,633
就是生成的意思

606
00:19:12,633 --> 00:19:14,366
生成的大圆模型

607
00:19:14,400 --> 00:19:16,133
那这篇文章没有太多的东西

608
00:19:16,133 --> 00:19:17,233
就说到了哎

609
00:19:17,233 --> 00:19:21,066
现在 GPT3 这个 decoder only 的时代来了

610
00:19:21,066 --> 00:19:22,733
那我的 MoE 的架构

611
00:19:22,766 --> 00:19:25,533
也可以基于整个 GP3 这种选缩码

612
00:19:25,533 --> 00:19:27,333
decoder 来进行修改

613
00:19:27,400 --> 00:19:28,033
整体来说

614
00:19:28,033 --> 00:19:29,233
就证明了哎

615
00:19:29,233 --> 00:19:30,033
这种模式

616
00:19:30,033 --> 00:19:30,833
我们现在

617
00:19:30,833 --> 00:19:32,466
可以在一个

618
00:19:32,466 --> 00:19:35,566
shot 这种不同的领域里面

619
00:19:35,600 --> 00:19:37,366
取得比较好的效果

620
00:19:37,766 --> 00:19:39,033
因此这篇文章

621
00:19:39,033 --> 00:19:40,833
就讲了很多相关的内容

622
00:19:40,833 --> 00:19:43,233
其实相关的内容都是大量的实验

623
00:19:43,233 --> 00:19:44,700
那最核心的就是

624
00:19:44,766 --> 00:19:46,566
放大来看看这一个图案

625
00:19:46,566 --> 00:19:47,200
那这个图

626
00:19:47,200 --> 00:19:50,866
就没有了左边的 encoder 跟 decode 部分

627
00:19:51,133 --> 00:19:54,066
纯粹的像 GBT3 这种方式

628
00:19:54,133 --> 00:19:57,033
直接一个 decord 做一个整体的生成

629
00:19:57,033 --> 00:19:59,066
输进去的是一堆 Tokan

630
00:19:59,066 --> 00:19:59,733
那 Tokan

631
00:19:59,766 --> 00:20:00,633
经过一系列

632
00:20:00,633 --> 00:20:03,500
一个 MHA 相关的计算之后就

633
00:20:03,533 --> 00:20:05,633
来到了一个 MoE 的 gating 路由

634
00:20:05,633 --> 00:20:06,666
专家的选择

635
00:20:06,666 --> 00:20:06,933
然后

636
00:20:06,966 --> 00:20:09,400
我们可以选择很多个不同的专家

637
00:20:09,466 --> 00:20:10,133
那后面

638
00:20:10,166 --> 00:20:12,233
就继续往前行了

639
00:20:12,233 --> 00:20:13,333
就乾坤神经网络

640
00:20:13,366 --> 00:20:14,566
还有其他的部分了

641
00:20:14,600 --> 00:20:15,233
那么基本上

642
00:20:15,233 --> 00:20:18,966
就是魔改了整个 GP3 这种 decode only

643
00:20:19,000 --> 00:20:21,066
或者传送 decode only 的这种方式了

644
00:20:21,066 --> 00:20:22,466
那后面的文章的内容

645
00:20:22,466 --> 00:20:23,433
就没有太多了

646
00:20:23,433 --> 00:20:24,866
还有更多的是 experiences

647
00:20:24,866 --> 00:20:26,833
还有对应的摘要了

648
00:20:27,466 --> 00:20:28,100
所以说

649
00:20:28,133 --> 00:20:29,400
这篇文章的解读

650
00:20:29,400 --> 00:20:31,066
特别的简单

651
00:20:31,066 --> 00:20:32,566
没有太多的难题

652
00:20:32,600 --> 00:20:33,033
最主要

653
00:20:33,033 --> 00:20:35,366
还是找到了一个很好的切入点

654
00:20:35,466 --> 00:20:37,366
然后发了个了一篇文章

655
00:20:37,400 --> 00:20:38,533
然后这篇文章

656
00:20:38,533 --> 00:20:40,466
作为比较先进性的内容

657
00:20:40,466 --> 00:20:41,133
在当年

658
00:20:41,166 --> 00:20:43,533
所以说就取得一个比较好的效果

659
00:20:44,566 --> 00:20:46,400
那今天回报 PPT

660
00:20:46,400 --> 00:20:47,766
我们已经基本上呃

661
00:20:47,766 --> 00:20:49,400
跟大家讲完了

662
00:20:49,400 --> 00:20:52,633
穿梭网时代的最后一篇 ST-MoE

663
00:20:52,633 --> 00:20:53,033
然后

664
00:20:53,033 --> 00:20:57,500
迎接到了 GPT 的一个时代里面的 GLaM

665
00:20:57,533 --> 00:20:59,266
我们将最快在下个视频

666
00:20:59,266 --> 00:21:00,666
跟大家去分享一下

667
00:21:00,666 --> 00:21:02,500
DeepSeek M1 相关的一个文章

668
00:21:02,533 --> 00:21:04,000
那今天的内容先到这里为止

669
00:21:04,000 --> 00:21:04,466
谢谢各位

670
00:21:04,466 --> 00:21:05,266
拜了个拜

