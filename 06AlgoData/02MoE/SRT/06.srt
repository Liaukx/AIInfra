1
00:00:00,000 --> 00:00:02,450
内容/录制/字幕:Z0MI 酱，视频剪辑:梁嘉铭

2
00:00:02,450 --> 00:00:03,450
hello 大家好

3
00:00:03,450 --> 00:00:04,333
我是 ZOMI

4
00:00:04,333 --> 00:00:05,933
我们现在还是在 Moe

5
00:00:05,933 --> 00:00:08,533
就是整个环道专家系列里面的

6
00:00:08,533 --> 00:00:09,766
在论文走读

7
00:00:09,766 --> 00:00:12,366
今天我们更多的是看一下 speech 传说嘛

8
00:00:12,400 --> 00:00:14,533
这一个论文相关的内容

9
00:00:14,533 --> 00:00:17,400
那 speech 传说嘛是 2022 年

10
00:00:17,400 --> 00:00:18,933
谷歌发表的一篇文章

11
00:00:18,933 --> 00:00:21,000
当时候发表的是欧盘 AI

12
00:00:21,250 --> 00:00:24,533
已经推出了 GPT12 到准备到 3 了

13
00:00:24,533 --> 00:00:26,400
不过 GPT123

14
00:00:26,400 --> 00:00:28,000
走的还是稠密的

15
00:00:28,000 --> 00:00:29,766
全缩码的 decoder 的架构

16
00:00:29,800 --> 00:00:30,966
那 speech 全缩码

17
00:00:30,966 --> 00:00:33,766
更多的是在全缩码架构里面的

18
00:00:33,766 --> 00:00:35,333
incoder 跟 decoder 之间

19
00:00:35,333 --> 00:00:38,366
加了 moe 重要的是提升原声

20
00:00:38,366 --> 00:00:40,050
全缩码的一个能力

21
00:00:40,283 --> 00:00:42,200
我们将会在后面才讲到

22
00:00:42,200 --> 00:00:43,566
GPT 时代相关的

23
00:00:43,566 --> 00:00:45,366
很重要的几个文章

24
00:00:45,366 --> 00:00:47,850
我们现在已经来到了 123

25
00:00:47,850 --> 00:00:49,600
第三个看一下传说网的内容

26
00:00:49,600 --> 00:00:52,450
里面很重要的 switch 传说嘛

27
00:00:52,450 --> 00:00:54,883
这一篇论文在整个系列里面

28
00:00:54,883 --> 00:00:56,483
已经更新到这里了

29
00:00:56,483 --> 00:00:57,883
欢迎大家持续的关注

30
00:00:57,883 --> 00:00:59,766
如果觉得作品讲的不好的地方

31
00:00:59,766 --> 00:01:02,533
也希望大家能够给我提出一点意见

32
00:01:05,366 --> 00:01:06,883
在正式开始之前

33
00:01:06,883 --> 00:01:08,250
其实大部分的 PPT

34
00:01:08,250 --> 00:01:10,483
作品会放在自己的一个 gift 的链接

35
00:01:10,483 --> 00:01:11,850
而有些小伙伴

36
00:01:11,850 --> 00:01:13,566
觉得中米讲的一些论文

37
00:01:13,566 --> 00:01:14,766
自己不方便下载的话

38
00:01:14,766 --> 00:01:16,366
中米也给出了中英文版本

39
00:01:16,366 --> 00:01:17,650
放在夸克网盘

40
00:01:18,333 --> 00:01:18,800
那我们

41
00:01:18,800 --> 00:01:22,533
现在来到了 switch 传送宝文章的走读啦

42
00:01:22,533 --> 00:01:24,850
那我们现在马上开始第一个内容

43
00:01:25,366 --> 00:01:26,883
我们现在打开了这篇文章

44
00:01:26,883 --> 00:01:29,250
看一下 switch 全数码整个标题

45
00:01:29,250 --> 00:01:31,283
一般来说我们去读论文的时候

46
00:01:31,283 --> 00:01:33,850
聪敏更关心的是整个标题的内容

47
00:01:33,850 --> 00:01:34,600
那看一下标题

48
00:01:34,600 --> 00:01:36,650
很重要的就是 scaling to 区列

49
00:01:36,650 --> 00:01:38,133
也就是到了一个

50
00:01:38,133 --> 00:01:40,450
万亿规模参数的 MV 的架构了

51
00:01:40,450 --> 00:01:41,733
那这个 MV 架构

52
00:01:41,733 --> 00:01:43,166
比较明显的一个特质

53
00:01:43,166 --> 00:01:45,050
就是简单又高效

54
00:01:45,400 --> 00:01:46,800
在稀疏方面

55
00:01:46,800 --> 00:01:47,566
那蛮有意思的

56
00:01:47,566 --> 00:01:49,650
就是还是看一下他的一个作者

57
00:01:49,650 --> 00:01:52,566
这个作者还是大神小笑

58
00:01:52,800 --> 00:01:53,200
小笑

59
00:01:53,200 --> 00:01:56,200
大神在 MV 家给我提出了很多新的

60
00:01:56,200 --> 00:01:57,566
创新的方法

61
00:01:57,566 --> 00:01:58,050
那现在

62
00:01:58,050 --> 00:02:00,050
我们来看一下最核心的 s t

63
00:02:00,050 --> 00:02:01,800
也就是它对应的摘要

64
00:02:02,133 --> 00:02:02,883
里面就说到了

65
00:02:02,883 --> 00:02:04,600
一般的一个深度学习里面

66
00:02:04,600 --> 00:02:06,650
我们会对所有的参数

67
00:02:06,650 --> 00:02:07,800
进行一个计算的

68
00:02:07,800 --> 00:02:08,933
但是 MV 架构

69
00:02:08,933 --> 00:02:10,966
只是选择一些特殊的专家

70
00:02:10,966 --> 00:02:12,450
来进行一个计算

71
00:02:12,450 --> 00:02:14,133
也就是只对特殊的一些透

72
00:02:14,133 --> 00:02:14,933
可能进行计算

73
00:02:14,933 --> 00:02:17,166
而不是所有的数据

74
00:02:17,400 --> 00:02:19,333
但为什么 MV 没有火起来

75
00:02:19,333 --> 00:02:21,083
好 F2 尽管 MV 加购了

76
00:02:21,083 --> 00:02:22,283
有很多优势

77
00:02:22,283 --> 00:02:24,050
而且能够把模型仓数量做大

78
00:02:24,050 --> 00:02:25,166
模型仓数量做大了

79
00:02:25,166 --> 00:02:26,250
效果就变好

80
00:02:26,533 --> 00:02:27,800
最主要的问题就是

81
00:02:27,800 --> 00:02:29,966
非常的消耗我们的计算资源

82
00:02:29,966 --> 00:02:31,650
还有训练不稳定性

83
00:02:31,650 --> 00:02:32,966
为了解决这个问题

84
00:02:32,966 --> 00:02:35,483
因此就提出了 switch 全扫码了

85
00:02:35,483 --> 00:02:37,050
那整个 switch 全扫码

86
00:02:37,050 --> 00:02:39,333
最重要的一个本论文的工作

87
00:02:39,333 --> 00:02:42,050
就是简化了 M1 的路由的算法

88
00:02:42,050 --> 00:02:44,533
还有设计的一个比较直观的

89
00:02:44,533 --> 00:02:45,933
模型的结构

90
00:02:45,933 --> 00:02:48,050
并且通过工程的手段

91
00:02:48,050 --> 00:02:51,766
去提升了计算的耗时和通讯的耗时

92
00:02:51,766 --> 00:02:53,733
降低了计算和通讯的耗时了

93
00:02:53,733 --> 00:02:55,850
那我们的这篇文章的方法

94
00:02:55,850 --> 00:02:57,400
就使用了这一次

95
00:02:57,400 --> 00:02:59,883
使用了 BF 16 这个第一精度格式

96
00:02:59,883 --> 00:03:02,533
去训练这个 MO1 的大模型

97
00:03:02,733 --> 00:03:03,600
这个精度格式

98
00:03:03,600 --> 00:03:05,483
后面也成为英伟达的 A100

99
00:03:05,483 --> 00:03:07,366
H1 版里面的标准格式

100
00:03:07,366 --> 00:03:09,450
英伟达的 H1 版里面的 BF 16

101
00:03:09,450 --> 00:03:10,283
的精度格式

102
00:03:10,283 --> 00:03:14,533
也是参考谷歌的 TPU 来去实现的

103
00:03:14,533 --> 00:03:15,250
那后面

104
00:03:15,250 --> 00:03:15,566
就说

105
00:03:15,566 --> 00:03:18,566
我基于这个 t five 的一个基础网络

106
00:03:18,566 --> 00:03:20,800
模型加上 MOE

107
00:03:20,850 --> 00:03:22,683
然后就使得模型的效果

108
00:03:22,683 --> 00:03:24,366
在多语言的任务里面

109
00:03:24,366 --> 00:03:26,600
就取得很好的成绩了

110
00:03:26,600 --> 00:03:27,283
那最核心的

111
00:03:27,283 --> 00:03:29,366
就是对我们的 MOE 的架构了

112
00:03:29,366 --> 00:03:30,533
进行一个整改

113
00:03:30,650 --> 00:03:31,450
那我们现在

114
00:03:31,450 --> 00:03:33,650
先来看看里面的一个大纲目录

115
00:03:33,883 --> 00:03:35,366
那大概目录里面蛮有意思的

116
00:03:35,366 --> 00:03:37,933
中理可能觉得一开始写的挺好的

117
00:03:37,933 --> 00:03:38,850
但是阅读下来了

118
00:03:38,850 --> 00:03:42,283
发现最核心最核心的也就 2.1 跟 2.2

119
00:03:42,283 --> 00:03:45,283
也就是简化我们的一个稀疏路由

120
00:03:45,283 --> 00:03:48,166
然后提升我们的一个稀疏路由的方法

121
00:03:48,166 --> 00:03:51,083
最后将很多的算法结合起来

122
00:03:51,083 --> 00:03:52,933
就变成我们的 switch 全扫码了

123
00:03:53,200 --> 00:03:54,200
有了 speech 传操版

124
00:03:54,200 --> 00:03:58,333
就怎么去提升我们的一个训练的性能

125
00:03:58,333 --> 00:03:59,850
和微调的性能了

126
00:03:59,850 --> 00:04:00,566
接下来

127
00:04:00,566 --> 00:04:01,800
scaling property

128
00:04:01,800 --> 00:04:03,166
也就是 scaling 的特性

129
00:04:03,400 --> 00:04:04,400
那 scaling 这个

130
00:04:04,400 --> 00:04:05,000
你会发现

131
00:04:05,000 --> 00:04:06,600
其实我们现在的大模型

132
00:04:06,600 --> 00:04:07,650
包括 open AI

133
00:04:07,650 --> 00:04:10,400
专门出了一篇论文叫做 scaling low

134
00:04:10,400 --> 00:04:12,566
我们后面会跟大家一起去解读

135
00:04:12,566 --> 00:04:13,966
scaling low 那篇论文的

136
00:04:13,966 --> 00:04:15,483
不过这个 scaling pop 体

137
00:04:15,483 --> 00:04:18,133
终于觉得可能就不需要详细的展开了

138
00:04:18,133 --> 00:04:20,400
因为在当时候写这篇论文的时候

139
00:04:20,733 --> 00:04:22,166
斯格林诺这个法则

140
00:04:22,166 --> 00:04:23,450
其实已经出现了

141
00:04:23,450 --> 00:04:23,600
但

142
00:04:23,600 --> 00:04:26,483
这不是斯格林诺展示最好的一篇论文

143
00:04:26,483 --> 00:04:28,883
还有一些下游的任务

144
00:04:29,283 --> 00:04:29,566
当然了

145
00:04:29,566 --> 00:04:32,050
钟迷可能比较关心的就是 distillation

146
00:04:32,050 --> 00:04:33,600
也就是对应的蒸馏

147
00:04:33,650 --> 00:04:36,200
dipstick V3 里面用 R1 的模型

148
00:04:36,200 --> 00:04:38,250
把它蒸馏到千问和喇嘛里面

149
00:04:38,250 --> 00:04:39,600
发现效果挺好的

150
00:04:39,766 --> 00:04:40,000
于是

151
00:04:40,000 --> 00:04:42,333
最近大家很多人去关心增流问题

152
00:04:42,333 --> 00:04:45,000
因此我们会在这里面 downstream result

153
00:04:45,000 --> 00:04:46,650
也就是下游任务的结果里面

154
00:04:46,650 --> 00:04:49,250
看看增流有哪些不一样的方案

155
00:04:49,250 --> 00:04:50,400
那最后第 5 个

156
00:04:50,400 --> 00:04:52,400
就工程性的手段

157
00:04:52,400 --> 00:04:56,133
当时候 space 抢搜本使用的是 tensophone

158
00:04:56,133 --> 00:04:58,250
那现在的工程性的数据并行

159
00:04:58,250 --> 00:04:58,850
模型并行

160
00:04:58,850 --> 00:05:00,133
还有专家并行

161
00:05:00,450 --> 00:05:00,933
更多的

162
00:05:00,933 --> 00:05:03,600
可能是用拍 touch 里面的 Micron 和 closer

163
00:05:03,600 --> 00:05:06,400
还有华为升腾自己的 minor speed 啦

164
00:05:06,450 --> 00:05:07,766
所以我们现在来看到

165
00:05:07,766 --> 00:05:08,333
第五个内容

166
00:05:08,333 --> 00:05:10,133
可能也不是我们重点关心的

167
00:05:10,133 --> 00:05:12,450
也不跟 MV 架构不强相关

168
00:05:12,450 --> 00:05:14,850
我们更多的是在分布式并行里面

169
00:05:14,850 --> 00:05:17,450
跟大家去解读相关的内容

170
00:05:17,450 --> 00:05:19,600
终点后面也会重点的去看看

171
00:05:19,600 --> 00:05:22,133
专家并行怎么去实现

172
00:05:22,283 --> 00:05:25,400
这篇文章我们更关心的是第二部分

173
00:05:25,450 --> 00:05:27,683
那我们现在马上进入正式的

174
00:05:27,683 --> 00:05:28,683
后面的内容

175
00:05:29,966 --> 00:05:32,733
在一开始就说到了大规模的训练

176
00:05:32,733 --> 00:05:35,850
现在是提升模型效果最好的办法了

177
00:05:35,850 --> 00:05:38,000
于是在整个 switch 选说门里面

178
00:05:38,000 --> 00:05:40,683
一开头就给我们丢出了一个核弹

179
00:05:40,683 --> 00:05:44,683
我们的稀疏模型的参数量越来越多

180
00:05:44,733 --> 00:05:47,000
实际测试的 loss 会越来越低

181
00:05:47,000 --> 00:05:50,083
也就是模型的效果会越来越好

182
00:05:50,083 --> 00:05:53,483
特别是对应负指数的 PPL 的实现

183
00:05:53,483 --> 00:05:55,600
也是我们的模型效果越大

184
00:05:55,600 --> 00:05:57,800
或者我们的模型参数量越大

185
00:05:57,933 --> 00:05:59,333
模型效果越好

186
00:05:59,333 --> 00:06:01,850
那一开始就讲了 scaling 的问题

187
00:06:01,933 --> 00:06:04,166
对应的我们的这篇文章的开头

188
00:06:04,166 --> 00:06:06,650
scaling 也是完全一致的

189
00:06:06,650 --> 00:06:07,650
那我们接下来

190
00:06:07,650 --> 00:06:09,650
看一下下面的一些内容

191
00:06:09,650 --> 00:06:10,683
下面就说到

192
00:06:10,683 --> 00:06:11,850
在 introduction 里面

193
00:06:11,850 --> 00:06:14,650
我们其实做了很多的贡献

194
00:06:14,650 --> 00:06:15,333
那第一个就

195
00:06:15,333 --> 00:06:18,250
是 swims 穿梭码的一个架构

196
00:06:18,366 --> 00:06:19,800
简化了和提升了

197
00:06:19,800 --> 00:06:21,450
MOE 的一个相关的内容

198
00:06:21,450 --> 00:06:22,133
怎么提升

199
00:06:22,133 --> 00:06:24,250
我们将后面会展开的

200
00:06:24,566 --> 00:06:27,450
第二个就是即使计算资源有限了

201
00:06:27,450 --> 00:06:29,283
但是也可以用很少的专家

202
00:06:29,283 --> 00:06:32,766
两个专家来实现比较好的一个效果

203
00:06:32,766 --> 00:06:34,883
而且成功的将预训练的模型

204
00:06:34,883 --> 00:06:36,200
跟微调的模型

205
00:06:36,483 --> 00:06:38,000
提炼成为一个稠密的模型

206
00:06:38,000 --> 00:06:40,250
说白了也就是做微调和蒸馏的

207
00:06:40,250 --> 00:06:41,400
作用也是中米

208
00:06:41,400 --> 00:06:43,200
后面欢迎跟大家一起去讲讲

209
00:06:43,200 --> 00:06:44,800
蒸馏怎么去实现的

210
00:06:44,800 --> 00:06:47,400
在这篇文章里面大家也比较关心蒸馏

211
00:06:47,400 --> 00:06:49,650
还有就工程化的能力了

212
00:06:49,650 --> 00:06:51,533
用了标 16 的低精度的格式

213
00:06:51,533 --> 00:06:54,050
来去进行混合精度训练

214
00:06:54,050 --> 00:06:56,333
还有用了很多的专家

215
00:06:56,333 --> 00:06:58,483
把我们的模型规模扩大了

216
00:06:58,533 --> 00:07:00,166
允许到更多的专家数量

217
00:07:00,166 --> 00:07:00,533
而且

218
00:07:00,533 --> 00:07:04,000
增加了专家的一个正则化 regulation

219
00:07:04,083 --> 00:07:05,200
相关的内容

220
00:07:05,200 --> 00:07:07,050
那下面就没有其他的

221
00:07:07,050 --> 00:07:08,200
反正就我们的数据了

222
00:07:08,200 --> 00:07:10,650
也很多模型的效果也很好

223
00:07:10,800 --> 00:07:12,250
下面我们正式来到了

224
00:07:12,250 --> 00:07:14,683
可能综艺觉得大家最关心的内容了

225
00:07:14,683 --> 00:07:15,850
就是 switch 传说嘛

226
00:07:15,850 --> 00:07:16,566
这篇文章

227
00:07:16,566 --> 00:07:18,450
到底是怎么去实现的

228
00:07:18,450 --> 00:07:20,133
特别是他在陆游方面

229
00:07:20,133 --> 00:07:22,800
做了哪些比较重要的工作

230
00:07:23,366 --> 00:07:24,566
那这里面就说到了

231
00:07:24,566 --> 00:07:27,450
其实前面的工作者或者研究

232
00:07:27,450 --> 00:07:28,800
有很多相关的工作

233
00:07:28,800 --> 00:07:31,000
不过我们这篇文章

234
00:07:31,000 --> 00:07:33,133
研究了第四个轴

235
00:07:33,133 --> 00:07:33,966
第四个点

236
00:07:33,966 --> 00:07:35,283
因为前面有 123 的模型

237
00:07:35,283 --> 00:07:37,000
规模变大什么什么的

238
00:07:37,050 --> 00:07:38,133
然后第四个点

239
00:07:38,133 --> 00:07:41,083
就是增加参数的数量的时候

240
00:07:41,083 --> 00:07:43,083
同时保持每个势力

241
00:07:43,083 --> 00:07:45,450
或者每个 sample 的一个浮点的操作不变

242
00:07:45,733 --> 00:07:48,333
那这个说白了就是做了一个 hypothesy

243
00:07:48,333 --> 00:07:49,650
一个具体的假

244
00:07:49,650 --> 00:07:51,966
释这个假释就是参数的计数

245
00:07:51,966 --> 00:07:53,050
就参数的量

246
00:07:53,083 --> 00:07:55,200
跟我们的总的计算量没有关系

247
00:07:55,200 --> 00:07:58,333
是一个单独重要的缩放的因子

248
00:07:58,333 --> 00:07:59,200
或者变量

249
00:07:59,283 --> 00:08:00,000
那说白了

250
00:08:00,000 --> 00:08:01,600
中以提炼成为一句话

251
00:08:01,600 --> 00:08:03,483
就是总的计算量不变

252
00:08:03,766 --> 00:08:06,250
我们的模型的参数量规模越大

253
00:08:06,250 --> 00:08:07,650
也能够提升效果

254
00:08:07,650 --> 00:08:10,200
也就是通过 MOE 的这个架构

255
00:08:10,200 --> 00:08:12,683
我们可以把模型的规模做大

256
00:08:12,683 --> 00:08:15,450
但是因为 MOE 是个稀疏的 MOE 结构

257
00:08:15,450 --> 00:08:18,333
我们不是所有专家都同一时间激活

258
00:08:18,333 --> 00:08:19,966
所以我们的总的计算量

259
00:08:19,966 --> 00:08:22,200
可能跟稠密的模型不变

260
00:08:22,250 --> 00:08:22,683
但是

261
00:08:22,683 --> 00:08:26,000
这种方式也能够提升大模型的效果

262
00:08:26,000 --> 00:08:28,933
这也是他说到的第四个维度的问题

263
00:08:28,933 --> 00:08:30,533
也就是现在要换方

264
00:08:30,600 --> 00:08:31,650
坚持走 m

265
00:08:31,650 --> 00:08:34,000
e 这个架构的路线的一个原因

266
00:08:34,366 --> 00:08:37,683
那下面我们来看一下这一个图

267
00:08:37,683 --> 00:08:38,766
蛮有意思的

268
00:08:38,883 --> 00:08:39,366
这个图

269
00:08:39,366 --> 00:08:41,883
就是整个 switch 传送码的一个 incorder

270
00:08:41,883 --> 00:08:42,733
的一个 box

271
00:08:42,733 --> 00:08:44,166
所谓的 incorder

272
00:08:44,166 --> 00:08:45,800
因为在传送码里面

273
00:08:45,800 --> 00:08:47,083
有 incorder 跟 decoder

274
00:08:47,083 --> 00:08:48,850
decorder 其实也是类似的

275
00:08:48,850 --> 00:08:50,483
那因为 speech 传送码

276
00:08:50,483 --> 00:08:52,733
主要是针对传统 Transformer

277
00:08:52,733 --> 00:08:55,766
incorder 编码跟解码进行实现

278
00:08:55,933 --> 00:08:56,450
所以这里面

279
00:08:56,450 --> 00:08:59,133
就专门说到它是一个 incorder 的内容

280
00:08:59,133 --> 00:09:00,683
那我们看一下左边的这个

281
00:09:00,683 --> 00:09:02,366
就是简单的小说放

282
00:09:02,366 --> 00:09:03,883
我们输入一个 x

283
00:09:04,050 --> 00:09:05,966
然后经过一个 self attention 层

284
00:09:05,966 --> 00:09:08,733
也就是自注意力层加一个 agnormalization

285
00:09:08,733 --> 00:09:11,400
然后把以前的 f 分层

286
00:09:11,450 --> 00:09:13,166
替换成为 switching

287
00:09:13,333 --> 00:09:15,766
FNFN 的一个 moe 的架构了

288
00:09:15,766 --> 00:09:18,933
再加上一个 add layer low 再进行输出

289
00:09:19,133 --> 00:09:19,650
那下面

290
00:09:19,650 --> 00:09:22,050
我们重点来打开一下 switch and FFN

291
00:09:22,050 --> 00:09:23,283
就是 moe 这个架构的

292
00:09:23,283 --> 00:09:24,533
层了蛮有意思的

293
00:09:24,533 --> 00:09:27,133
就是我输入一个具体的 TOKEN

294
00:09:27,366 --> 00:09:27,883
这个 took

295
00:09:27,883 --> 00:09:30,800
会经过我们的一个 self attention LA lob

296
00:09:30,800 --> 00:09:32,650
然后给到这个 FFN 层

297
00:09:32,766 --> 00:09:34,483
FFFN 层里面有一个路由

298
00:09:34,483 --> 00:09:35,733
我们叫做 water

299
00:09:36,250 --> 00:09:37,600
或者我们的叫做 gate

300
00:09:37,733 --> 00:09:40,166
可能在之前的文章里面叫做 gate

301
00:09:40,166 --> 00:09:41,250
现在叫做 what

302
00:09:41,333 --> 00:09:42,600
更多的是路由了

303
00:09:42,600 --> 00:09:43,333
那最终

304
00:09:43,333 --> 00:09:45,650
这里面只说了我有很多个专家

305
00:09:45,650 --> 00:09:48,133
但是我最终只选择一个专家

306
00:09:48,133 --> 00:09:49,366
那激活一个专家

307
00:09:49,366 --> 00:09:50,400
然后激活一个专家

308
00:09:50,400 --> 00:09:51,400
有一个概率

309
00:09:51,400 --> 00:09:52,933
这个 p 通过这个 p

310
00:09:52,933 --> 00:09:55,566
跟我们的这个 FN 进行一个相乘

311
00:09:55,850 --> 00:09:57,650
最后得到的答案进行输出

312
00:09:57,650 --> 00:09:58,650
每一个 TOKEN

313
00:09:58,650 --> 00:10:01,800
单独的过一次路由这种方式去实现

314
00:10:01,800 --> 00:10:03,400
我的 switch 全说嘛

315
00:10:03,650 --> 00:10:05,933
那我们继续往下看一下 switch 全说嘛

316
00:10:05,933 --> 00:10:07,600
里面最核心的就是

317
00:10:07,600 --> 00:10:11,000
怎么去简化我的一个稀疏路由的

318
00:10:11,000 --> 00:10:13,083
这里面就讲了 h 场

319
00:10:13,083 --> 00:10:14,366
就是 Moe

320
00:10:14,366 --> 00:10:16,683
就是相关的陆游了学校

321
00:10:16,683 --> 00:10:18,533
也就是自己之前的那篇文章

322
00:10:18,533 --> 00:10:20,850
其实提出了很多相关的算法

323
00:10:20,850 --> 00:10:23,200
基本上以前的算法就是讲这了

324
00:10:23,200 --> 00:10:24,166
就讲的这个类型

325
00:10:24,166 --> 00:10:26,283
朱米在之前的文章也讲过了

326
00:10:26,483 --> 00:10:28,933
然后再选择一个 topk 的门

327
00:10:28,966 --> 00:10:30,483
然后去进行约束

328
00:10:30,483 --> 00:10:32,933
这个 topk 的门就变成这里面的 p i

329
00:10:32,966 --> 00:10:35,333
可能在之前的文章里面

330
00:10:35,333 --> 00:10:37,400
会用 g i 来去表示

331
00:10:37,400 --> 00:10:39,683
i 就是我们 d i 个 ASP

332
00:10:39,766 --> 00:10:42,850
那 e i 就是我们 ASP 的一个输出

333
00:10:43,166 --> 00:10:44,083
通过这种方式

334
00:10:44,083 --> 00:10:47,333
是最 naive 最原始的 moe 的一个计算

335
00:10:47,333 --> 00:10:50,050
所以这里面只是罗列那更核心的就

336
00:10:50,050 --> 00:10:53,133
是这里面提出了我们的 switch to voting

337
00:10:53,400 --> 00:10:55,366
以前的文章都是讲 getting 的

338
00:10:55,366 --> 00:10:57,366
这里面就改了个名字

339
00:10:57,366 --> 00:10:59,000
叫做 walting 路由

340
00:10:59,000 --> 00:11:01,450
那路由跟门控其实是一样的

341
00:11:01,450 --> 00:11:02,766
那其实做的更好

342
00:11:02,766 --> 00:11:04,683
总理觉得可能后面叫路由

343
00:11:04,683 --> 00:11:06,083
也是更加的灵活

344
00:11:06,083 --> 00:11:06,883
因为门控

345
00:11:06,883 --> 00:11:09,766
更多的是指开关门与非门那种方式

346
00:11:09,766 --> 00:11:10,450
而现在

347
00:11:10,450 --> 00:11:12,200
基本上都是通过路由的方式

348
00:11:12,200 --> 00:11:13,366
来去实现的

349
00:11:13,366 --> 00:11:15,166
这里面学校就说了

350
00:11:15,450 --> 00:11:16,683
刷笑就说了

351
00:11:16,683 --> 00:11:19,650
其实在 2017 年包括之前的文章里面

352
00:11:19,650 --> 00:11:21,283
就为了使得路由器

353
00:11:21,283 --> 00:11:22,933
或者我们的给点函数

354
00:11:22,933 --> 00:11:24,366
有一个比较好的梯队

355
00:11:24,366 --> 00:11:25,133
还有学习

356
00:11:25,133 --> 00:11:27,166
一般来说路由都 k

357
00:11:27,450 --> 00:11:29,883
是我们的专家数量多于大于一的

358
00:11:29,883 --> 00:11:31,050
凭直观来说

359
00:11:31,050 --> 00:11:33,166
可能多个专家会更好

360
00:11:33,166 --> 00:11:35,566
所以至少需要两个专家

361
00:11:35,566 --> 00:11:36,050
于是

362
00:11:36,050 --> 00:11:38,733
我们就进一步的研究了 top k 的策略

363
00:11:38,733 --> 00:11:41,533
其实发现模型的一个中低层的

364
00:11:41,533 --> 00:11:43,050
比较高的 k 的值

365
00:11:43,166 --> 00:11:46,050
对于很多陆游层都有非常多的作用的

366
00:11:46,050 --> 00:11:49,050
也就是我们的陆游的专家数量

367
00:11:49,250 --> 00:11:50,400
必须要多

368
00:11:50,566 --> 00:11:51,933
但是这篇文章

369
00:11:51,933 --> 00:11:53,133
我们 switch 传说版

370
00:11:53,133 --> 00:11:56,766
就提出了一个新的 idea contractory

371
00:11:56,766 --> 00:11:59,400
那于是我们选择步骤

372
00:11:59,400 --> 00:12:02,733
常规路线只录录到一个专家

373
00:12:02,733 --> 00:12:06,333
single asper 所以我们叫做 Swish layer

374
00:12:06,366 --> 00:12:09,133
通过这种方式我们做了大量的实验

375
00:12:09,133 --> 00:12:11,966
证明我们这个方法特别的好

376
00:12:12,200 --> 00:12:13,650
那为什么会特别好

377
00:12:13,650 --> 00:12:14,650
主要得益于

378
00:12:14,650 --> 00:12:17,800
我们 Swish layer 一共三个特点

379
00:12:17,800 --> 00:12:18,166
第一个

380
00:12:18,166 --> 00:12:20,883
就是减少了我们路由器的计算

381
00:12:20,933 --> 00:12:22,400
只有一个专家

382
00:12:22,400 --> 00:12:24,200
因为我们只将对应的 TOKEN

383
00:12:24,200 --> 00:12:26,400
路由到单个专家里面

384
00:12:26,400 --> 00:12:28,083
那第二点就是每个

385
00:12:28,083 --> 00:12:29,166
专家的 bitch

386
00:12:29,166 --> 00:12:31,800
就是处理的数据可以减少一半的

387
00:12:31,800 --> 00:12:32,933
因为每个 TOKEN

388
00:12:32,933 --> 00:12:35,566
仅仅被路由到单个专家里面

389
00:12:35,850 --> 00:12:36,483
至少两个

390
00:12:36,483 --> 00:12:37,600
我现在只剩一个

391
00:12:37,600 --> 00:12:38,333
那第三个

392
00:12:38,333 --> 00:12:40,733
就简化了通讯的成本

393
00:12:41,733 --> 00:12:43,566
刚才了解完整个模型加构嘛

394
00:12:43,566 --> 00:12:46,200
我们现在来看一下下面的这个图

395
00:12:46,200 --> 00:12:48,883
这个图就讲了我怎么去做动态

396
00:12:48,883 --> 00:12:50,483
陆游的没有意思的

397
00:12:50,483 --> 00:12:52,483
我们在整个 page size 里面

398
00:12:52,483 --> 00:12:53,933
有很多个 TOKEN

399
00:12:54,083 --> 00:12:55,966
假设我整一个 devices 里面

400
00:12:55,966 --> 00:12:57,483
现在上面两个图

401
00:12:57,483 --> 00:12:59,283
一个图还有另外一个图

402
00:12:59,333 --> 00:13:02,766
最大的区别就是 capacity factor

403
00:13:02,966 --> 00:13:05,333
就是我们的一个专家容量

404
00:13:05,366 --> 00:13:07,133
专家容量到底有多少

405
00:13:07,250 --> 00:13:09,166
那专家容量我们默认是一的时候

406
00:13:09,166 --> 00:13:10,483
我们看一下他怎么走的

407
00:13:10,483 --> 00:13:11,166
那首先

408
00:13:11,166 --> 00:13:13,050
我们处理第一个 TOKEN 的时候

409
00:13:13,050 --> 00:13:15,133
会走到第二个专家进行处理

410
00:13:15,133 --> 00:13:16,366
而不是跨专家

411
00:13:16,366 --> 00:13:19,133
只是处理第二个专家处理这一个 TOKEN

412
00:13:19,166 --> 00:13:20,566
当然针对下一个 TOKEN

413
00:13:20,566 --> 00:13:23,850
我们可能会给一个专家进行处理

414
00:13:23,850 --> 00:13:26,133
但是如果我们的专家一

415
00:13:26,133 --> 00:13:28,283
超过我们的 cable factor 的时候

416
00:13:28,283 --> 00:13:29,650
我们就遇到冲突了

417
00:13:29,650 --> 00:13:30,800
就没办法去处理了

418
00:13:30,800 --> 00:13:32,166
而是作为残差

419
00:13:32,166 --> 00:13:34,883
传给下一层的专家进行处理

420
00:13:34,883 --> 00:13:35,883
通过这种方式

421
00:13:35,883 --> 00:13:36,250
当然

422
00:13:36,250 --> 00:13:39,450
如果我们希望整个处理的一些内容

423
00:13:39,450 --> 00:13:41,366
更多专家能够处理的头坎更多

424
00:13:41,366 --> 00:13:43,050
我们可以增加 Kable fact

425
00:13:43,083 --> 00:13:45,850
也就是专家的容量来去实现的

426
00:13:45,850 --> 00:13:47,250
所以这里面的 Kable fact

427
00:13:47,250 --> 00:13:50,966
更多了就变成我们的超餐去调整的

428
00:13:51,000 --> 00:13:51,850
从这个图里面

429
00:13:51,850 --> 00:13:54,166
我们可以看到刚才讲到的 3 个点

430
00:13:54,166 --> 00:13:55,366
都已经满足了

431
00:13:55,366 --> 00:13:56,600
这就是 switch 小说嘛

432
00:13:56,600 --> 00:13:59,250
提供的最核心的内容了了

433
00:13:59,250 --> 00:14:00,883
解完相关的实现的原理

434
00:14:00,883 --> 00:14:02,000
实现原理特别简单的

435
00:14:02,000 --> 00:14:03,450
反正就选择一个路由嘛

436
00:14:03,450 --> 00:14:05,083
然后加上一个乘

437
00:14:05,166 --> 00:14:07,166
那这里面我们继续往下看一看

438
00:14:07,166 --> 00:14:09,166
就说到的高效的悉数路由

439
00:14:09,166 --> 00:14:09,683
这里面

440
00:14:09,683 --> 00:14:12,250
就使用了一个 match tensorfold 库

441
00:14:12,250 --> 00:14:14,166
那这个库我们就先不管了

442
00:14:14,166 --> 00:14:15,600
后我们往下看一看

443
00:14:16,000 --> 00:14:16,333
这里面

444
00:14:16,333 --> 00:14:18,366
就说到了几个可能重要的特性

445
00:14:18,366 --> 00:14:18,683
第一个

446
00:14:18,683 --> 00:14:22,483
就是分布式的交换的一个实现的方案

447
00:14:22,533 --> 00:14:23,200
那因为

448
00:14:23,200 --> 00:14:24,000
现在来说

449
00:14:24,000 --> 00:14:26,733
所有的张量都是在变硬的时候

450
00:14:26,733 --> 00:14:27,566
静态的确认的

451
00:14:27,566 --> 00:14:29,933
因为他们实现的是 tensorfood 的这些框架

452
00:14:29,933 --> 00:14:31,366
还有谷歌的 TPU

453
00:14:31,450 --> 00:14:34,200
貌似就是软硬件的一个结合

454
00:14:34,483 --> 00:14:35,733
希望 mattersport 加商藤

455
00:14:35,733 --> 00:14:38,250
也能够提出比较有意思的一些特性

456
00:14:38,250 --> 00:14:40,250
那这面就重复了一下

457
00:14:40,250 --> 00:14:41,800
我们所有的张量的 ship

458
00:14:41,800 --> 00:14:43,250
都是静态的

459
00:14:43,250 --> 00:14:44,850
在兵役的时候静态的确定的

460
00:14:44,850 --> 00:14:47,733
但是在训练和推理的过程当中

461
00:14:47,733 --> 00:14:49,800
我们的陆游的决策和计算

462
00:14:49,800 --> 00:14:51,450
是动态的 dynamic 的

463
00:14:51,450 --> 00:14:54,533
因此怎么去设计一个专家的能力

464
00:14:54,533 --> 00:14:57,800
是一个非常重要的一个关键节点

465
00:14:58,050 --> 00:15:00,483
那下面我们看一下所谓的专家容量

466
00:15:00,600 --> 00:15:03,683
也就是专家的容量 asper capacity

467
00:15:03,683 --> 00:15:04,483
专家的容量

468
00:15:04,483 --> 00:15:05,533
就是 token

469
00:15:05,533 --> 00:15:06,483
每 batch size

470
00:15:06,483 --> 00:15:08,733
去除以我们的一个专家的数量

471
00:15:08,733 --> 00:15:11,450
再乘以 capacity 的一个 factor

472
00:15:11,450 --> 00:15:13,966
也就是说我们的专家容量的系数

473
00:15:13,966 --> 00:15:15,250
通过这种方式

474
00:15:15,250 --> 00:15:18,333
去进一步扩展我们的一个容量因子

475
00:15:19,483 --> 00:15:20,766
因为我们是静态的

476
00:15:20,766 --> 00:15:22,400
所以大于一的一个容量因子

477
00:15:22,400 --> 00:15:25,133
我们会单独的创建呃 deshin buffer

478
00:15:25,133 --> 00:15:27,683
也就额外的缓冲区来去适应

479
00:15:27,683 --> 00:15:30,133
专家之间 TOKEN 没有完美的去进

480
00:15:30,133 --> 00:15:31,766
行处理和均衡的

481
00:15:31,800 --> 00:15:33,333
那聊到均衡这个问题

482
00:15:33,333 --> 00:15:34,850
下面都是我们怎么去实现

483
00:15:34,850 --> 00:15:35,933
具体的细节了

484
00:15:36,166 --> 00:15:36,933
那蛮有意思

485
00:15:36,933 --> 00:15:39,800
就是可微分的一个均衡负载

486
00:15:39,800 --> 00:15:42,000
所谓的可微分的均衡负载

487
00:15:42,000 --> 00:15:44,083
因为有些均衡负载的算法

488
00:15:44,083 --> 00:15:45,766
我们必须要加到 lost 里面去

489
00:15:45,766 --> 00:15:47,800
进行一个可微分的操作

490
00:15:47,800 --> 00:15:50,850
也就是给我们的大模型进行训练的

491
00:15:51,000 --> 00:15:52,283
但是有一些

492
00:15:52,283 --> 00:15:54,566
设计出来的一个均衡负载的算法

493
00:15:54,566 --> 00:15:55,766
不一定能够微分

494
00:15:55,800 --> 00:15:56,533
所以这里面

495
00:15:56,533 --> 00:15:58,683
就说了我设计了一个比较简单的

496
00:15:58,683 --> 00:16:01,766
比较特别的一个均衡负载的算法

497
00:16:01,766 --> 00:16:04,166
那这个 lost 是 x low lost

498
00:16:04,166 --> 00:16:05,533
也就是辅助的

499
00:16:05,533 --> 00:16:06,600
lost 里面

500
00:16:06,600 --> 00:16:08,366
这个阿尔法是超餐

501
00:16:08,533 --> 00:16:11,250
n 是我们的专家的数量

502
00:16:11,400 --> 00:16:12,733
FI 乘以 PI

503
00:16:12,766 --> 00:16:14,933
那我们看一下 FI 到底是什么

504
00:16:15,050 --> 00:16:16,083
FI 主要是指

505
00:16:16,083 --> 00:16:19,200
被分配到第 i 个 as per 的一个 TOKEN 数

506
00:16:19,450 --> 00:16:20,333
对应的 PI

507
00:16:20,333 --> 00:16:21,333
就是整个败取

508
00:16:21,333 --> 00:16:22,166
每个 TOKEN

509
00:16:22,200 --> 00:16:25,600
分配给第 i 个 as per 的一个概率的总和

510
00:16:25,966 --> 00:16:27,450
通过概率的总和

511
00:16:27,450 --> 00:16:29,483
乘以每个 ASP 的一个脱口数

512
00:16:29,483 --> 00:16:31,566
然后再乘以所有的专家

513
00:16:31,566 --> 00:16:34,733
我们就得到我们的辅助损失函数了

514
00:16:34,933 --> 00:16:36,166
至于为什么要再乘以 n

515
00:16:36,166 --> 00:16:37,283
这里面就解释了

516
00:16:37,283 --> 00:16:38,483
我们有 n 个专家

517
00:16:38,483 --> 00:16:39,800
我们希望所有的专家

518
00:16:39,800 --> 00:16:42,200
都有一个一分之 n 的这种效果

519
00:16:42,200 --> 00:16:44,333
也就每个专家都尽可能的平衡

520
00:16:44,333 --> 00:16:46,933
而不是每一次只选定一个专家

521
00:16:46,933 --> 00:16:48,600
所以我们希望每个专家的

522
00:16:48,600 --> 00:16:50,800
还是 value of 1 除以 n

523
00:16:50,800 --> 00:16:52,400
因为我们在计算的时候

524
00:16:52,400 --> 00:16:53,133
为了方便

525
00:16:53,133 --> 00:16:54,400
这里面我们可以看一下

526
00:16:54,566 --> 00:16:55,000
通过这条

527
00:16:55,000 --> 00:16:55,366
公式

528
00:16:55,366 --> 00:16:57,483
我们发现其实再乘以一个 n

529
00:16:57,483 --> 00:17:00,050
是加快我们的一个计算的效果的

530
00:17:00,050 --> 00:17:02,533
所以就乘了以一个 n

531
00:17:02,683 --> 00:17:05,283
那最后就是 2.3 的 switch 全数目

532
00:17:05,283 --> 00:17:06,400
里面讲的一些内容

533
00:17:06,400 --> 00:17:08,800
还有 improve training and fighting technology

534
00:17:08,800 --> 00:17:10,083
这里面就有一个表达

535
00:17:10,083 --> 00:17:12,600
说不同的 capacity 的一个 factor

536
00:17:12,966 --> 00:17:13,966
是容量因子

537
00:17:13,966 --> 00:17:16,250
对整个模型的效果的影响

538
00:17:16,250 --> 00:17:17,600
可以看到我们的容量因子

539
00:17:17,600 --> 00:17:19,450
从 1 到 1.25 到 2

540
00:17:19,450 --> 00:17:21,200
会发现模型的效果

541
00:17:21,200 --> 00:17:22,250
没有说好

542
00:17:22,250 --> 00:17:23,000
非常的多

543
00:17:23,000 --> 00:17:25,333
但是会影响模型的性能

544
00:17:25,333 --> 00:17:26,166
也就是 speed

545
00:17:26,200 --> 00:17:27,050
那一般来说

546
00:17:27,050 --> 00:17:28,650
容量因子是唯一

547
00:17:28,650 --> 00:17:31,166
可能是比较综合的一个选择

548
00:17:31,166 --> 00:17:32,683
我们的模型的效果也好

549
00:17:32,683 --> 00:17:35,333
我们的模型的性能也相对比较好

550
00:17:35,733 --> 00:17:36,400
那基本上

551
00:17:36,400 --> 00:17:39,250
这篇文章最核心的内容就讲完了

552
00:17:39,250 --> 00:17:39,766
那下面

553
00:17:39,766 --> 00:17:42,000
就有了一些什么 BF 16 的计算

554
00:17:42,000 --> 00:17:43,333
用了 B16 非常好

555
00:17:43,333 --> 00:17:44,733
然后我们怎么去稳定

556
00:17:44,733 --> 00:17:47,450
我们训练的时候的一个稳定性

557
00:17:47,450 --> 00:17:48,533
所以我们一开始

558
00:17:48,533 --> 00:17:50,333
需要给一个比较小的

559
00:17:50,333 --> 00:17:52,133
一个初始化的超餐

560
00:17:52,133 --> 00:17:53,933
而不是随机一个超餐值

561
00:17:53,933 --> 00:17:56,133
所以说在训练的时候也蛮重要的

562
00:17:56,133 --> 00:17:58,283
还有对于我们的一个稀疏的模型

563
00:17:58,283 --> 00:18:01,200
里面需要做一些正则化的内容

564
00:18:01,200 --> 00:18:03,133
包括加一个 drop out

565
00:18:03,333 --> 00:18:04,050
加上做 out 了

566
00:18:04,050 --> 00:18:07,283
可能会使得我们的模型的效果更好

567
00:18:07,283 --> 00:18:09,650
那还有一些其他的特性

568
00:18:09,883 --> 00:18:10,883
凋零的特性

569
00:18:10,883 --> 00:18:13,450
怎么在固定训练的赛下面

570
00:18:13,483 --> 00:18:15,800
增大我们的专家的数量了

571
00:18:15,800 --> 00:18:18,450
使得我们的模型效果越来越好

572
00:18:18,450 --> 00:18:21,600
那往下了基本上就是对应的实验了

573
00:18:21,600 --> 00:18:24,050
那这些实验都是讲 scaling

574
00:18:24,050 --> 00:18:26,400
就是我们的大模型的规则

575
00:18:26,850 --> 00:18:27,200
那么有意

576
00:18:27,200 --> 00:18:29,483
思的就是这里面就讲到了一个点

577
00:18:29,483 --> 00:18:30,533
在第四点里面

578
00:18:30,533 --> 00:18:32,250
distillation 也就是增流

579
00:18:32,333 --> 00:18:33,283
现在的增流

580
00:18:33,283 --> 00:18:35,883
我们看到大部分都是指把一笔数据

581
00:18:35,883 --> 00:18:38,566
或者把 ie 的数据拿过来做一个 SFT

582
00:18:38,800 --> 00:18:40,283
但是实际的蒸馏

583
00:18:40,283 --> 00:18:42,083
能够提升比较好的效果

584
00:18:42,200 --> 00:18:44,283
更多的是我们把专家

585
00:18:44,283 --> 00:18:46,166
把我们的大模型作为老师

586
00:18:46,166 --> 00:18:48,083
来提升我们的一个能力

587
00:18:48,200 --> 00:18:51,566
然后用 75%的光触的 label 来去增流

588
00:18:51,566 --> 00:18:52,533
那这种方式

589
00:18:52,533 --> 00:18:54,000
能够提升 30%

590
00:18:54,000 --> 00:18:55,933
非常多的一个模型的

591
00:18:55,933 --> 00:18:58,400
也就是其他的模型的效果

592
00:18:58,400 --> 00:19:00,600
这就是增流所带来的收益

593
00:19:00,600 --> 00:19:03,000
也是我们真正的增流的方案

594
00:19:03,050 --> 00:19:03,600
到后面

595
00:19:03,600 --> 00:19:06,650
就是更多的一些 experiences 实验

596
00:19:06,650 --> 00:19:08,533
那我们往下看一下这里面这个图案

597
00:19:08,533 --> 00:19:09,883
君米觉得蛮有意思的

598
00:19:09,883 --> 00:19:12,733
这个是 2011 年到 22 年的时候

599
00:19:12,733 --> 00:19:15,933
钟米会经常用来介绍分布式并行的

600
00:19:16,050 --> 00:19:18,250
但是现在分布式并行

601
00:19:18,250 --> 00:19:18,683
你会发现

602
00:19:18,683 --> 00:19:20,933
我们已经出了非常多详细的内容

603
00:19:20,933 --> 00:19:22,083
所以这篇文章

604
00:19:22,083 --> 00:19:24,683
对于分布式并行相关的 d 的并行

605
00:19:24,683 --> 00:19:25,566
模型并行

606
00:19:25,566 --> 00:19:26,283
战略并行

607
00:19:26,283 --> 00:19:27,166
专家并行

608
00:19:27,166 --> 00:19:28,133
相对落后

609
00:19:28,800 --> 00:19:30,250
而在分布式并行

610
00:19:30,250 --> 00:19:33,766
ZOMI 其实在网上分享了两次

611
00:19:33,766 --> 00:19:35,733
第一次会比较粗浅的理解

612
00:19:35,733 --> 00:19:38,166
第二次就真正的去打开 disp 了

613
00:19:38,200 --> 00:19:38,933
Mac 闯

614
00:19:38,933 --> 00:19:41,850
里面的相关的分布式并行的细节

615
00:19:41,850 --> 00:19:42,733
有兴趣了解

616
00:19:42,733 --> 00:19:43,733
也可以上这里面

617
00:19:43,733 --> 00:19:45,083
不是看这篇文章

618
00:19:45,650 --> 00:19:47,483
最后还是回到这篇文章

619
00:19:47,483 --> 00:19:49,250
简读的翻完这篇文章

620
00:19:49,250 --> 00:19:51,250
后面更多的是一些 experiences

621
00:19:51,250 --> 00:19:53,683
还有那个呃没有其他了诶

622
00:19:54,250 --> 00:19:55,733
一直都没过到一个图

623
00:19:55,733 --> 00:19:57,166
那这个图很有意思

624
00:19:57,200 --> 00:19:58,450
它里面这篇文章

625
00:19:58,450 --> 00:20:00,000
其实做了两个工作

626
00:20:00,000 --> 00:20:03,683
那第一个是是 which layer 放在 FFN 层

627
00:20:03,683 --> 00:20:06,683
第二个是 which layer 放在 attention 层

628
00:20:06,683 --> 00:20:08,733
通过 attention 这种方式做陆游

629
00:20:08,733 --> 00:20:10,966
但是发现这种效果没有太好

630
00:20:10,966 --> 00:20:12,166
没有非常的明显

631
00:20:12,166 --> 00:20:14,650
反而增加我们的计算的复杂度

632
00:20:14,650 --> 00:20:15,800
所以说最后

633
00:20:15,800 --> 00:20:18,133
我们是采用了这个图

634
00:20:18,133 --> 00:20:18,800
这种方案

635
00:20:18,800 --> 00:20:22,333
来去实现我们的一个 speech FFN 层

636
00:20:22,333 --> 00:20:24,733
也就是 speech 全缩码的一个来源了

637
00:20:24,733 --> 00:20:26,650
所以这篇文章做了很多的探索

638
00:20:26,650 --> 00:20:28,450
也希望有兴趣的小伙伴们

639
00:20:28,450 --> 00:20:30,966
拿这篇文章再去深入的研读一下

640
00:20:32,283 --> 00:20:35,650
我们现在不妨来个总结和思考的

641
00:20:35,650 --> 00:20:37,766
那在整个 switch 全称网里面

642
00:20:37,766 --> 00:20:40,250
谷歌在 2021 年 1 月份的时候

643
00:20:40,250 --> 00:20:41,600
就发表了这篇文章

644
00:20:41,650 --> 00:20:42,450
主要的特点

645
00:20:42,450 --> 00:20:44,450
就是提出了 switch 专家

646
00:20:44,450 --> 00:20:46,283
陆游的一个具体的方案

647
00:20:46,600 --> 00:20:49,600
主要就选择单个专家进行激活的

648
00:20:49,600 --> 00:20:52,283
那具体是通过一个稀疏的门控机制

649
00:20:52,283 --> 00:20:53,766
也就稀疏的 wooting

650
00:20:53,766 --> 00:20:55,883
还有 asper 的一个容量限制

651
00:20:55,883 --> 00:20:58,366
也就是 capacity 的方案

652
00:20:58,366 --> 00:21:00,400
就实现了我们的均衡负载

653
00:21:00,400 --> 00:21:02,883
把模型的参数量和规模做大

654
00:21:03,200 --> 00:21:04,250
展示了 MV 架构

655
00:21:04,250 --> 00:21:05,650
在大模型的一个潜力

656
00:21:05,650 --> 00:21:07,566
就是他的斯高林诺的法则

657
00:21:07,566 --> 00:21:09,766
而且对斯高林诺还有蒸馏

658
00:21:09,766 --> 00:21:11,683
做了很多相关的探索

659
00:21:11,733 --> 00:21:13,566
那今天的内容就先到这

660
00:21:14,083 --> 00:21:15,000
有兴趣的小伙伴

661
00:21:15,000 --> 00:21:17,533
可以升为下面的链接进行了解

662
00:21:18,050 --> 00:21:18,733
卷的不行了

663
00:21:18,733 --> 00:21:19,683
卷的不行了

664
00:21:19,683 --> 00:21:21,366
AI 系统的全套知识都在这里

665
00:21:21,366 --> 00:21:23,000
欢迎跟着目录进行学习

666
00:21:23,400 --> 00:21:24,333
给我一键三连

667
00:21:24,333 --> 00:21:25,533
给我一键三连

668
00:21:26,000 --> 00:21:26,683
谢谢各位

669
00:21:26,683 --> 00:21:27,733
拜了个拜

