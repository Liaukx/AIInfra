1
00:00:00,116 --> 00:00:01,783
内容/录制/字幕:Z0MI 酱，视频剪辑  :梁嘉铭

2
00:00:01,916 --> 00:00:03,133
哈喽大家好

3
00:00:03,133 --> 00:00:04,333
我是 ZOMI

4
00:00:04,333 --> 00:00:05,966
在整个 MO1 系列里面

5
00:00:05,966 --> 00:00:09,400
我们现在来到了 MO1 加 in 时代的

6
00:00:09,400 --> 00:00:11,883
一个最经典的论文的走读

7
00:00:12,250 --> 00:00:15,166
这篇文章是在整个 2017 年的时候

8
00:00:15,166 --> 00:00:17,483
谷歌在 ICLR 里面去发表的

9
00:00:17,600 --> 00:00:18,966
里面这个标题蛮有意思的

10
00:00:18,966 --> 00:00:20,133
叫做奥特威

11
00:00:20,133 --> 00:00:24,050
就是里 large nature never 就非常大规模

12
00:00:24,083 --> 00:00:26,283
也是谷歌在 2017 年的时候

13
00:00:26,283 --> 00:00:28,333
基于传统的 in 时代

14
00:00:28,333 --> 00:00:30,566
去发表了超大规模的大模型

15
00:00:30,566 --> 00:00:33,050
那时候我们还没有迎来全 Soma 架构

16
00:00:33,050 --> 00:00:34,883
也没有大模型出现错

17
00:00:34,883 --> 00:00:36,366
Q06 当时还不存在

18
00:00:36,366 --> 00:00:37,933
不过了阅读这篇文章

19
00:00:37,933 --> 00:00:39,366
总比觉得谷歌这个思想

20
00:00:39,366 --> 00:00:40,966
还是非常的超前的

21
00:00:41,000 --> 00:00:41,850
那我们今天

22
00:00:41,850 --> 00:00:44,133
就重点去分析这篇文章

23
00:00:44,333 --> 00:00:47,050
那回到我们整个系列目录里面

24
00:00:47,050 --> 00:00:47,883
其实我们一开始

25
00:00:47,883 --> 00:00:50,333
就介绍了 Moe 的一个基本的内容

26
00:00:50,333 --> 00:00:51,966
然后在上一个视频里面

27
00:00:51,966 --> 00:00:53,966
我们讲了一个 M1 的前世今生

28
00:00:53,966 --> 00:00:56,200
捋的捋 MV 架构的整体的发展

29
00:00:56,200 --> 00:00:57,650
和相关的演变的方式

30
00:00:57,650 --> 00:00:59,400
和最重要最核心的内容

31
00:00:59,483 --> 00:00:59,850
接着

32
00:00:59,850 --> 00:01:02,200
我们在这上下两三个视频里面

33
00:01:02,200 --> 00:01:04,450
重点去每一篇论文了

34
00:01:04,566 --> 00:01:05,766
跟大家一起去重点

35
00:01:05,766 --> 00:01:07,733
走读相关核心论文

36
00:01:08,133 --> 00:01:10,600
最后如果大家喜欢这个内容的话

37
00:01:10,600 --> 00:01:13,400
你们可以去这个链接去下载相关的 PPT

38
00:01:13,400 --> 00:01:15,650
也可以去到花科链接里面去下载

39
00:01:15,683 --> 00:01:17,850
说明主读过的论文的相关的内容

40
00:01:18,850 --> 00:01:19,483
那我们现在

41
00:01:19,483 --> 00:01:21,083
马上进入到这个视频里面

42
00:01:21,083 --> 00:01:21,883
最核心的内容

43
00:01:21,883 --> 00:01:25,283
看一下这一篇文章具体讲了哪些事情

44
00:01:25,283 --> 00:01:26,733
给我们做了哪些启发

45
00:01:26,733 --> 00:01:28,683
old wage 里 large nation never

46
00:01:28,800 --> 00:01:29,883
那我的英文不标准

47
00:01:29,883 --> 00:01:31,366
没关系欢迎吐槽

48
00:01:32,250 --> 00:01:33,000
打开文章

49
00:01:33,000 --> 00:01:34,083
打开文章

50
00:01:34,083 --> 00:01:35,800
那我们现在打开了这篇文章之后

51
00:01:35,800 --> 00:01:37,966
我们看到整篇的论文

52
00:01:38,000 --> 00:01:41,200
是在 2017 年 ICLR 上面去发表的

53
00:01:41,250 --> 00:01:41,766
蛮有意思的

54
00:01:41,766 --> 00:01:43,766
就是我们首先要看看它的标题

55
00:01:43,800 --> 00:01:44,366
说了

56
00:01:44,366 --> 00:01:47,083
它是一个超大规模的一个神经网络

57
00:01:47,283 --> 00:01:48,133
那这个大规模的

58
00:01:48,133 --> 00:01:49,800
更多是用 M1 来堆上去的

59
00:01:49,800 --> 00:01:52,200
我们之前说到的千亿稠密

60
00:01:52,200 --> 00:01:53,200
万亿稀疏

61
00:01:53,200 --> 00:01:54,133
那这个稠密

62
00:01:54,133 --> 00:01:56,250
更多是指我们现在全色码架构

63
00:01:56,250 --> 00:01:57,283
那这个稀疏

64
00:01:57,283 --> 00:01:59,000
主要是指全色码架构

65
00:01:59,000 --> 00:02:00,733
加上 MO1 的这些架构

66
00:02:00,733 --> 00:02:03,083
能够把我们的模型规模做得越来越大

67
00:02:03,250 --> 00:02:04,000
那这篇文章

68
00:02:04,000 --> 00:02:06,050
也是用了 Moe 之后

69
00:02:06,050 --> 00:02:07,766
把以前传统的 in 的模型

70
00:02:07,766 --> 00:02:09,933
规模做得超级的大

71
00:02:09,933 --> 00:02:12,000
那这里面文章的标题冒意思的

72
00:02:12,000 --> 00:02:16,650
后面的冒号就是 SPASLY gated Moe 的层

73
00:02:16,650 --> 00:02:18,450
最核心的就是 SPASLY

74
00:02:18,450 --> 00:02:21,000
就是我们的稀疏门控 Moe

75
00:02:21,366 --> 00:02:22,766
最重要的提出了这个内容

76
00:02:22,766 --> 00:02:25,650
那我们看一下作者还是要有点意思的

77
00:02:25,650 --> 00:02:28,800
作者里面的虚写重要的

78
00:02:28,800 --> 00:02:31,533
它后面有很多相关的 M1 的架构

79
00:02:31,533 --> 00:02:32,400
的眼镜

80
00:02:32,400 --> 00:02:34,766
都是由这位大神来去实现的

81
00:02:34,766 --> 00:02:36,933
或者他发表了很多相关 M1 的文章

82
00:02:37,366 --> 00:02:39,800
那他的合作作者还有 HITAN 跟 Jeff d 嘛

83
00:02:39,800 --> 00:02:42,133
所以说阵容非常的豪华

84
00:02:42,133 --> 00:02:44,200
那我们看一下整体的 f 圈

85
00:02:44,333 --> 00:02:45,250
f 圈蛮有意思的

86
00:02:45,250 --> 00:02:46,333
你其实发现

87
00:02:46,400 --> 00:02:49,366
在整个 M1 加固里面的 condition computation

88
00:02:49,366 --> 00:02:50,933
也就是有条件的计算

89
00:02:50,933 --> 00:02:52,283
很重要那但是

90
00:02:52,283 --> 00:02:53,083
有条件的计算

91
00:02:53,083 --> 00:02:54,850
当我们的模型规模上去之后

92
00:02:55,050 --> 00:02:56,083
模型的效率很差

93
00:02:56,083 --> 00:02:57,733
然后模型的效果也不好

94
00:02:57,733 --> 00:02:58,283
所以说

95
00:02:58,283 --> 00:02:59,050
这篇文章

96
00:02:59,050 --> 00:03:01,850
就引入了一个稀疏的门控 MO1

97
00:03:01,850 --> 00:03:03,800
所以叫 spasti gate it

98
00:03:03,800 --> 00:03:04,850
MO1 的架构

99
00:03:04,850 --> 00:03:07,450
提升了 1,000 倍的一个具体的性能

100
00:03:07,450 --> 00:03:08,400
就执行性能

101
00:03:08,400 --> 00:03:09,766
而且这个文章

102
00:03:09,766 --> 00:03:10,683
就为 PERCENT

103
00:03:10,683 --> 00:03:11,766
model Architecture

104
00:03:11,883 --> 00:03:12,766
为 Moe

105
00:03:12,766 --> 00:03:14,683
去到了 137 币

106
00:03:14,883 --> 00:03:15,850
在 2017 年的时候

107
00:03:15,850 --> 00:03:17,450
这个网络模型的仓储规模

108
00:03:17,450 --> 00:03:19,283
是非常的夸张快乐

109
00:03:19,333 --> 00:03:20,600
非常的惊人的

110
00:03:21,133 --> 00:03:22,566
那我们继续来往下看一下

111
00:03:22,566 --> 00:03:24,333
相关的 relative 工作

112
00:03:24,333 --> 00:03:24,766
说实话

113
00:03:24,766 --> 00:03:26,883
relative 工作中也觉得可能有很多

114
00:03:26,883 --> 00:03:28,450
但是大家可能不必要关心

115
00:03:28,450 --> 00:03:29,933
因为整个 in 的加构

116
00:03:29,933 --> 00:03:31,283
已经没有人用了

117
00:03:31,283 --> 00:03:33,966
所以相关工作可能并不那么的重要

118
00:03:33,966 --> 00:03:36,000
这里讲的更多的是 condition computation

119
00:03:36,000 --> 00:03:38,283
也就是条件性计算的一个相关的内容

120
00:03:38,333 --> 00:03:40,483
也就是各种各样的 get it 的网络

121
00:03:40,566 --> 00:03:42,600
那这篇文章我们先不看这个

122
00:03:43,133 --> 00:03:44,966
我们要先看看相关的挑战

123
00:03:44,966 --> 00:03:46,333
的挑战还是蛮多的

124
00:03:46,366 --> 00:03:47,966
了解这篇文章的挑战之后

125
00:03:47,966 --> 00:03:49,450
就知道这篇文章

126
00:03:49,450 --> 00:03:52,333
主要是解决哪些任务问题的

127
00:03:52,333 --> 00:03:53,050
那首先

128
00:03:53,050 --> 00:03:55,400
第一个就是我用 GPU 来进行计算

129
00:03:55,400 --> 00:03:56,600
那用 GPU 进行计算

130
00:03:56,600 --> 00:03:57,533
更重要的是分布

131
00:03:57,533 --> 00:03:59,683
是并行或者并行的内容

132
00:04:00,283 --> 00:04:01,333
GPU 擅长做并行

133
00:04:01,333 --> 00:04:02,483
不擅长于做串形

134
00:04:02,483 --> 00:04:03,450
f l s 的所以

135
00:04:03,450 --> 00:04:04,450
我们要充分利用好

136
00:04:04,450 --> 00:04:06,050
GPU 的一个计算结构

137
00:04:06,050 --> 00:04:07,933
所以说 gated 网络的计算

138
00:04:07,933 --> 00:04:10,766
或者 gated 的一个函数的设计很重要

139
00:04:10,766 --> 00:04:11,133
第二个

140
00:04:11,133 --> 00:04:12,533
就是大大 batch size

141
00:04:12,533 --> 00:04:14,800
说实话 batch size 因为 moe

142
00:04:14,800 --> 00:04:17,166
会分散我们的 bitch size 的数量

143
00:04:17,166 --> 00:04:19,283
所以真正到每个专家的计算

144
00:04:19,283 --> 00:04:20,566
会变得越来越少

145
00:04:20,600 --> 00:04:22,050
还有网络的贷款

146
00:04:22,133 --> 00:04:24,250
真正在 GPU 计算的时候

147
00:04:24,250 --> 00:04:25,966
计算会比网络贷款

148
00:04:25,966 --> 00:04:27,283
快了很多很多

149
00:04:27,283 --> 00:04:28,400
所以怎么去提升网络

150
00:04:28,400 --> 00:04:29,966
贷款是个很大的问题

151
00:04:30,166 --> 00:04:32,450
还有面向我们的损失函数

152
00:04:32,483 --> 00:04:35,000
怎么去重新的定义损失函数

153
00:04:35,000 --> 00:04:36,450
因为现在 MA 多了

154
00:04:36,450 --> 00:04:37,000
专家多了

155
00:04:37,000 --> 00:04:38,166
怎么做负载均衡

156
00:04:38,166 --> 00:04:40,083
都涉及到我们的损失函数

157
00:04:40,083 --> 00:04:41,650
或者损失的问题

158
00:04:41,650 --> 00:04:44,050
当然了我们还有模型的一个容量

159
00:04:44,050 --> 00:04:45,000
后面就讲到

160
00:04:45,000 --> 00:04:46,166
我们的数据量越大

161
00:04:46,166 --> 00:04:47,450
我们的模型规模越大

162
00:04:47,450 --> 00:04:48,850
可能模型效果就越好

163
00:04:48,850 --> 00:04:50,850
这个 model capacity

164
00:04:50,850 --> 00:04:52,333
哎英文太不好了

165
00:04:52,800 --> 00:04:54,200
其实叫 capacity

166
00:04:54,200 --> 00:04:55,000
不管怎么样

167
00:04:55,000 --> 00:04:56,166
大家能听得懂就行了

168
00:04:56,166 --> 00:04:59,000
反正印度英文大家能都能听得懂

169
00:04:59,050 --> 00:05:00,883
何况中国英文

170
00:05:00,883 --> 00:05:02,283
对不对

171
00:05:12,966 --> 00:05:15,050
那我们现在回到这个 capacity

172
00:05:15,083 --> 00:05:16,483
就是模型的规模

173
00:05:16,483 --> 00:05:18,933
这个是完全符合 scarlet 的规则

174
00:05:18,933 --> 00:05:20,766
反正就是 with large data

175
00:05:20,766 --> 00:05:21,600
数据量越多

176
00:05:21,600 --> 00:05:22,766
模型规模越大

177
00:05:23,166 --> 00:05:24,366
效果就越好

178
00:05:24,400 --> 00:05:25,250
所以说这个

179
00:05:25,250 --> 00:05:27,450
就做了一些相关实验性的尝试

180
00:05:27,450 --> 00:05:27,850
那这个

181
00:05:27,850 --> 00:05:29,966
可能会在 experience 里面看的更多

182
00:05:30,000 --> 00:05:32,800
那最核心的就是我们的 loss 怎么去训

183
00:05:32,800 --> 00:05:34,166
怎么做负载均衡

184
00:05:34,166 --> 00:05:36,400
还有我们去提升 GPU 的一个能力

185
00:05:36,400 --> 00:05:38,166
那我们接下来往后看一看了

186
00:05:38,166 --> 00:05:39,166
这篇文章

187
00:05:39,166 --> 00:05:40,200
现在的 1.2 了

188
00:05:40,200 --> 00:05:41,000
就是最核心的

189
00:05:41,000 --> 00:05:43,283
就提出了我们自己的神经网络模型

190
00:05:43,600 --> 00:05:44,283
那这个网络模型

191
00:05:44,283 --> 00:05:46,200
就现在来到了这个图了

192
00:05:46,200 --> 00:05:47,000
那可以看到

193
00:05:47,000 --> 00:05:47,566
这一层

194
00:05:47,566 --> 00:05:49,883
是一个 RNN 自循环的

195
00:05:49,883 --> 00:05:50,533
但是 RNA

196
00:05:50,533 --> 00:05:52,166
它现在变成两层的 RN

197
00:05:52,250 --> 00:05:54,050
当然 RN 可以很多层

198
00:05:54,083 --> 00:05:54,933
那中间

199
00:05:54,933 --> 00:05:56,283
在 RN 跟 RN 之间

200
00:05:56,283 --> 00:05:58,166
加了一个 moe 的层这

201
00:05:58,166 --> 00:05:59,850
个 moe 也是循环的

202
00:05:59,850 --> 00:06:03,133
所以真正这篇文章叫做 recurrent moe

203
00:06:03,450 --> 00:06:05,333
跟我们的 RNN

204
00:06:05,566 --> 00:06:07,166
或者那个循环神经网络里面

205
00:06:07,166 --> 00:06:08,483
是接近一致的

206
00:06:08,566 --> 00:06:09,650
M1 之间

207
00:06:09,683 --> 00:06:12,400
就这两个 m 之间的是全中共享的

208
00:06:12,400 --> 00:06:14,366
所以跟我们现在全送的架构的理解

209
00:06:14,366 --> 00:06:15,283
完全不一样

210
00:06:15,333 --> 00:06:17,283
有兴趣的小伙伴或者不太清楚的

211
00:06:17,283 --> 00:06:19,850
可以看回 RNN 的一个具体的架构

212
00:06:19,850 --> 00:06:21,850
那我们看一下在这里能不能画

213
00:06:22,083 --> 00:06:23,166
首先我们可以看到

214
00:06:23,166 --> 00:06:24,800
这里面真正的 RN 里面

215
00:06:24,800 --> 00:06:26,766
这里有一个权重 w

216
00:06:26,933 --> 00:06:28,333
这里也有一个权重 k

217
00:06:28,333 --> 00:06:29,333
也称重 v

218
00:06:29,333 --> 00:06:30,450
好多个权重

219
00:06:30,450 --> 00:06:30,883
但是

220
00:06:30,883 --> 00:06:33,733
真正的 in 的一个具体自己的结构

221
00:06:33,733 --> 00:06:35,483
是这么一个字更新的

222
00:06:35,566 --> 00:06:37,283
所以我们把这个序列展开

223
00:06:37,283 --> 00:06:38,250
就变成左边的

224
00:06:38,250 --> 00:06:40,483
这个就变成一个 c 款

225
00:06:40,483 --> 00:06:42,250
一个 c 款的这么去过来

226
00:06:42,283 --> 00:06:43,000
但实际上

227
00:06:43,000 --> 00:06:44,533
权重是字更新的

228
00:06:44,850 --> 00:06:46,683
那在这个权重自更新之上

229
00:06:46,683 --> 00:06:47,683
我们做一个抽象

230
00:06:47,683 --> 00:06:50,650
就是中间这一层加了个 MOE

231
00:06:50,650 --> 00:06:51,733
然后上面

232
00:06:51,733 --> 00:06:54,133
其实还是我们的权重的自更新

233
00:06:54,133 --> 00:06:55,333
而 MOE 的权重

234
00:06:55,333 --> 00:06:57,400
也是经过这种方式来自更新的

235
00:06:57,800 --> 00:06:59,000
VF 大家理解一下

236
00:06:59,000 --> 00:07:00,133
vcoint level

237
00:07:00,133 --> 00:07:02,133
循环神经网络的循环

238
00:07:02,166 --> 00:07:04,050
就是把循环展开

239
00:07:04,533 --> 00:07:07,083
two thousand years later

240
00:07:07,366 --> 00:07:08,200
橡皮擦擦完之后

241
00:07:08,200 --> 00:07:08,800
我们看到

242
00:07:08,800 --> 00:07:10,366
整个循环神经网络展开

243
00:07:10,366 --> 00:07:11,650
就变成这种形式

244
00:07:11,650 --> 00:07:12,133
实际上

245
00:07:12,133 --> 00:07:15,366
它的 m e layers 之间是共享的 viper

246
00:07:15,366 --> 00:07:16,850
我们现在简单的了解一下

247
00:07:16,850 --> 00:07:17,600
这篇文章就行了

248
00:07:17,600 --> 00:07:20,166
反正我们又不用实现这篇文章

249
00:07:20,166 --> 00:07:20,966
就说我们提出了

250
00:07:20,966 --> 00:07:23,733
刚才的那个 LSTM 的一个 GSTM

251
00:07:23,733 --> 00:07:25,133
提出我们的 MV 的架构

252
00:07:25,133 --> 00:07:28,050
所以说最核心的就是 Moe

253
00:07:28,050 --> 00:07:29,483
加 RNN

254
00:07:29,566 --> 00:07:31,000
那相关的 Moe 的工作

255
00:07:31,000 --> 00:07:31,966
我们基本上这里面

256
00:07:31,966 --> 00:07:33,683
就不再详细的展述了

257
00:07:33,683 --> 00:07:35,000
因为它没什么内容

258
00:07:35,000 --> 00:07:36,883
最核心内容我们来了第二个

259
00:07:36,883 --> 00:07:38,533
the structure of the Moe

260
00:07:38,533 --> 00:07:40,883
这个模型的架构到底是怎么去实现的

261
00:07:41,483 --> 00:07:43,000
首先下面这个公式

262
00:07:43,000 --> 00:07:45,966
就是经典的 Moe 架构的计算公式

263
00:07:45,966 --> 00:07:50,050
你们 y 等于 g s 乘以 e x 的一个求和

264
00:07:50,133 --> 00:07:53,366
那 e s 就是第 i 个 ASP 的一个虚出

265
00:07:53,566 --> 00:07:56,166
g x 就是第 i 个 ASP 的权重

266
00:07:56,166 --> 00:07:57,800
我们的 g 就是 get it

267
00:07:58,166 --> 00:08:00,333
我们的网络现在都叫 voted 了

268
00:08:00,333 --> 00:08:01,366
叫做陆游了

269
00:08:01,366 --> 00:08:04,450
不过就不同的文章不同的定义

270
00:08:04,800 --> 00:08:07,250
为了去解决我们的 Esper 过多

271
00:08:07,250 --> 00:08:09,400
所以我们需要有一个门控的 GX

272
00:08:09,400 --> 00:08:11,400
来去控制我们的计算量

273
00:08:11,400 --> 00:08:13,483
去解决我们的 spasity 稀疏性

274
00:08:13,483 --> 00:08:15,200
带来的一个性能的问题

275
00:08:15,483 --> 00:08:16,883
当 g s 等于 0 的时候

276
00:08:16,883 --> 00:08:18,283
就不需要去计算了

277
00:08:18,283 --> 00:08:18,483
所以

278
00:08:18,483 --> 00:08:20,733
这篇文章的最重要的内容的 SPA

279
00:08:20,733 --> 00:08:21,966
是来源于这里面

280
00:08:21,966 --> 00:08:23,800
我们的网络模型

281
00:08:23,800 --> 00:08:25,250
或者我们的门控

282
00:08:25,333 --> 00:08:27,966
是让我们的整个专家变得稀疏

283
00:08:27,966 --> 00:08:29,483
有些专家是不计算的

284
00:08:29,483 --> 00:08:31,083
通过这种方式来去实现的

285
00:08:31,733 --> 00:08:32,650
那非常好的

286
00:08:32,650 --> 00:08:33,083
就是

287
00:08:33,083 --> 00:08:36,283
对应到上面的这个论文的图里面

288
00:08:36,283 --> 00:08:37,166
某些 ASP

289
00:08:37,166 --> 00:08:38,850
它是没有去计算的

290
00:08:38,850 --> 00:08:42,000
真正的计算的可能是 ASP2 跟 ASP n 减一

291
00:08:42,000 --> 00:08:43,800
通过我们的 getting level

292
00:08:43,800 --> 00:08:45,850
进入门控的网络模型

293
00:08:45,850 --> 00:08:46,483
当然门控

294
00:08:46,483 --> 00:08:48,650
也是一个网络模型的结构

295
00:08:48,650 --> 00:08:49,600
那接着我们往下

296
00:08:49,600 --> 00:08:52,733
看一下最核心的就是 2.1getting 的我

297
00:08:52,733 --> 00:08:53,050
当然了

298
00:08:53,050 --> 00:08:55,366
我们刚才提到了所有的悉数

299
00:08:55,366 --> 00:08:57,400
都是这个 getting 的我所引起的时候

300
00:08:57,400 --> 00:08:58,733
我们现在关心这个内容

301
00:08:58,733 --> 00:09:00,483
一般来说在之前的论文

302
00:09:00,483 --> 00:09:02,166
都已经追溯到 94 年了

303
00:09:02,166 --> 00:09:03,533
非常的夸张哈

304
00:09:03,600 --> 00:09:05,600
那因为这篇论文已经是 2017 年的时候

305
00:09:05,600 --> 00:09:06,400
所以看到

306
00:09:06,400 --> 00:09:08,533
整个深度学习发展的非常的漫长

307
00:09:08,600 --> 00:09:09,483
所以我们现在看到

308
00:09:09,483 --> 00:09:11,450
所有的人都在研究 RNN

309
00:09:11,450 --> 00:09:12,483
研究大模型

310
00:09:12,483 --> 00:09:13,200
但中间

311
00:09:13,200 --> 00:09:14,883
走了很多的技术的弯路

312
00:09:14,883 --> 00:09:16,850
和相关技术的探索

313
00:09:17,133 --> 00:09:18,650
那回到我们这篇论文里面

314
00:09:18,650 --> 00:09:20,733
我们看一下所谓的 soft getting

315
00:09:20,733 --> 00:09:22,800
就是一开始的一个最原始的

316
00:09:22,800 --> 00:09:25,283
我们的门控网络是怎么设计的

317
00:09:25,283 --> 00:09:27,766
就是输入 x 乘以权重 w

318
00:09:27,766 --> 00:09:28,850
加个 Softmax

319
00:09:28,850 --> 00:09:30,850
就是我们的门控的输出了

320
00:09:31,083 --> 00:09:32,933
这种方式是筹密的 moe

321
00:09:32,933 --> 00:09:33,800
每个专家

322
00:09:33,800 --> 00:09:36,450
都会有自己的一个门控的一个标准值

323
00:09:36,566 --> 00:09:39,250
于是这篇文章就加了两个事情

324
00:09:39,250 --> 00:09:41,133
叫做 noise talk kick getting

325
00:09:41,366 --> 00:09:43,683
是他提出来的一个新的算法

326
00:09:43,683 --> 00:09:45,883
最核心的就是融合了两个东西

327
00:09:45,883 --> 00:09:47,850
在我们的 Softmax gating

328
00:09:47,850 --> 00:09:50,200
那么里面也就是基于传统的

329
00:09:50,366 --> 00:09:51,366
加了点东西

330
00:09:51,366 --> 00:09:54,600
就变成我们下面的这条公式了

331
00:09:54,650 --> 00:09:55,966
那引入了两个东西

332
00:09:55,966 --> 00:09:57,283
我们看一下这条公式

333
00:09:57,283 --> 00:09:59,850
就是提出来的 noise top k gating

334
00:09:59,883 --> 00:10:01,766
第一个就是 keep top k

335
00:10:02,133 --> 00:10:03,333
也就是引入了稀疏性

336
00:10:03,333 --> 00:10:05,283
只选择 top k 个专家

337
00:10:05,450 --> 00:10:07,450
另外就是 HX

338
00:10:07,450 --> 00:10:10,533
把我们的数的 x 加了一系列的噪声

339
00:10:10,566 --> 00:10:11,250
那这个噪声

340
00:10:11,250 --> 00:10:13,766
我们会在后面的 bandix 里面去展开的

341
00:10:13,766 --> 00:10:14,333
这里面

342
00:10:14,333 --> 00:10:16,566
真正的论文里面没展开详细

343
00:10:16,566 --> 00:10:18,800
所以说真正敢这么去写论文的人

344
00:10:18,800 --> 00:10:19,733
真的有料

345
00:10:19,733 --> 00:10:20,250
一般来说

346
00:10:20,250 --> 00:10:21,366
我写论文怎么写

347
00:10:21,366 --> 00:10:22,683
就把这个继续在这里面

348
00:10:22,683 --> 00:10:24,283
2.32.4 这么去展开

349
00:10:24,483 --> 00:10:25,483
然后变成 3 的

350
00:10:25,850 --> 00:10:27,283
反正就一系列的这种

351
00:10:27,283 --> 00:10:28,083
那蛮有意思的

352
00:10:28,083 --> 00:10:29,050
就是他就没了

353
00:10:29,050 --> 00:10:30,483
说我这篇文章最核心的

354
00:10:30,483 --> 00:10:32,333
就是提出了这个算法

355
00:10:32,450 --> 00:10:33,250
感觉很简单

356
00:10:33,250 --> 00:10:34,400
但实际上很牛逼

357
00:10:34,400 --> 00:10:36,366
特别是看到的 spirit

358
00:10:36,366 --> 00:10:37,850
那这里面的 top k 也蛮有意思的

359
00:10:37,850 --> 00:10:39,733
就比较简单和粗暴

360
00:10:39,733 --> 00:10:40,966
直接选用了 top k

361
00:10:40,966 --> 00:10:42,450
保留最大的 k 个值

362
00:10:42,450 --> 00:10:43,250
那其他

363
00:10:43,250 --> 00:10:44,883
因为经过 softmas 的计算

364
00:10:44,883 --> 00:10:47,133
复值就直接设置负无从

365
00:10:47,133 --> 00:10:48,566
或者算出来就是负有重

366
00:10:48,566 --> 00:10:49,333
就报名了

367
00:10:49,333 --> 00:10:50,966
我们的 SPAS 的 M1 的加工

368
00:10:50,966 --> 00:10:53,083
只选择部分的 x Pro 来进行计算

369
00:10:53,083 --> 00:10:55,283
实现了它的一个稀疏性

370
00:10:55,283 --> 00:10:56,650
而这个噪声

371
00:10:56,650 --> 00:10:57,850
为什么要加噪声

372
00:10:58,333 --> 00:10:59,166
噪声在哪

373
00:10:59,166 --> 00:10:59,966
为什么要加

374
00:10:59,966 --> 00:11:02,333
好像对我们的稀疏性没什么作用

375
00:11:02,483 --> 00:11:02,850
哎哎

376
00:11:02,850 --> 00:11:03,683
这个蛮有意思的

377
00:11:03,683 --> 00:11:04,600
就是噪声

378
00:11:04,600 --> 00:11:06,050
其实是更好的去解决

379
00:11:06,050 --> 00:11:08,000
我们 M 周一的专家越来越多

380
00:11:08,133 --> 00:11:09,933
网络模型的规模越来越大

381
00:11:09,933 --> 00:11:12,533
也就对应到我们的奥特为指示力

382
00:11:13,283 --> 00:11:14,650
超大规模的时候

383
00:11:14,650 --> 00:11:16,050
去使用的

384
00:11:16,050 --> 00:11:17,000
那使用

385
00:11:17,000 --> 00:11:17,766
用来干嘛

386
00:11:17,766 --> 00:11:20,366
主要是用来解决专家越来越多

387
00:11:20,366 --> 00:11:22,800
计算均衡负载的问题

388
00:11:23,166 --> 00:11:24,133
下面第三步

389
00:11:24,133 --> 00:11:26,683
综米觉得可能并不是大家所关心的

390
00:11:26,683 --> 00:11:29,450
也就是所谓的性能提升的一个内容

391
00:11:29,450 --> 00:11:31,366
或者那个性能的确认准则

392
00:11:31,366 --> 00:11:32,450
反正这里面

393
00:11:32,450 --> 00:11:33,483
就说了几个点

394
00:11:33,483 --> 00:11:35,200
在视频或者 QQ 上面

395
00:11:35,200 --> 00:11:36,083
大的 Betch size

396
00:11:36,083 --> 00:11:37,050
对于计算的效率

397
00:11:37,050 --> 00:11:38,000
非常的有效了

398
00:11:38,000 --> 00:11:39,483
以分摊参数的加载

399
00:11:39,483 --> 00:11:42,483
和参数的更新等相关的开销说

400
00:11:42,483 --> 00:11:44,933
我们尽可能的希望参数量越大越好

401
00:11:45,050 --> 00:11:45,850
但是差数量大了

402
00:11:45,850 --> 00:11:46,800
就会引起一个问题

403
00:11:46,800 --> 00:11:49,533
我们的 Moe 的专家怎么去分布

404
00:11:49,533 --> 00:11:51,133
特别是在分布式并行里面

405
00:11:51,133 --> 00:11:52,450
于是下面就说了

406
00:11:52,450 --> 00:11:55,733
哎我有一个 mix data plus and model plus

407
00:11:55,733 --> 00:11:58,283
就引入了数据并行跟模型并行

408
00:11:58,600 --> 00:11:58,850
不过

409
00:11:58,850 --> 00:12:01,000
为什么我们中米不想在这里面解读

410
00:12:01,000 --> 00:12:03,400
因为现在的数据并行跟模型并行

411
00:12:03,450 --> 00:12:04,966
现在用的是全松母的架构

412
00:12:04,966 --> 00:12:06,483
跟以前 RN 的架构

413
00:12:06,483 --> 00:12:07,566
就完全不一样了

414
00:12:07,566 --> 00:12:09,683
所以大家没有必要去了解一些显能

415
00:12:09,683 --> 00:12:10,933
相对落后

416
00:12:10,933 --> 00:12:12,400
过时的一些相关的技术

417
00:12:12,400 --> 00:12:14,733
指捞现在反正新的 MV 的架构

418
00:12:14,733 --> 00:12:16,766
都用了数据并行跟模型并行

419
00:12:17,800 --> 00:12:19,566
当然了我们刚才讲到的 MV 架构

420
00:12:19,566 --> 00:12:21,566
实际上是一个 Vicome 的 Moe

421
00:12:21,566 --> 00:12:23,800
所以说它增大 bitch size 的方式

422
00:12:23,800 --> 00:12:25,450
跟我们现在的也不一样了

423
00:12:25,450 --> 00:12:28,650
最核心的我们来到了第四个内容

424
00:12:28,650 --> 00:12:30,533
第四个才是相对比较核心的

425
00:12:30,533 --> 00:12:31,566
就是均衡负载

426
00:12:31,566 --> 00:12:31,933
因为

427
00:12:31,933 --> 00:12:34,600
我要实现这么大规模的模型的训练

428
00:12:34,600 --> 00:12:36,083
我均衡负载很重要

429
00:12:36,083 --> 00:12:37,483
如果负载不均衡

430
00:12:37,483 --> 00:12:40,083
某些专家就有可能像这里面说到的

431
00:12:40,333 --> 00:12:42,450
某些专家他分配的计算率越多

432
00:12:42,450 --> 00:12:43,166
声谱越大

433
00:12:43,166 --> 00:12:43,933
它效果越好

434
00:12:43,933 --> 00:12:45,200
某些专家就崴脚了

435
00:12:45,200 --> 00:12:48,000
不行了所以说均衡负载很重要

436
00:12:48,000 --> 00:12:49,733
说了我们刚才讲到的 spasity

437
00:12:49,766 --> 00:12:50,850
下面我们重点

438
00:12:50,850 --> 00:12:54,450
去聊一下那个 balance ASPO 的一个 utilization

439
00:12:54,450 --> 00:12:56,166
当然了均衡负载它不叫均衡负载

440
00:12:56,166 --> 00:12:58,766
而是平衡专家的一个利用率

441
00:12:59,166 --> 00:13:00,600
那在这篇文章之前

442
00:13:00,600 --> 00:13:02,133
其实大家大部分都用的

443
00:13:02,133 --> 00:13:03,600
都是 heart can string

444
00:13:03,600 --> 00:13:06,650
也就是强约束的方式去避免某些

445
00:13:06,650 --> 00:13:08,566
专家效率特别的低

446
00:13:08,650 --> 00:13:10,400
未进去的数据特别的少

447
00:13:10,850 --> 00:13:12,400
所以说现在这篇文章

448
00:13:12,400 --> 00:13:14,883
就使用了一个新的方法叫做 soft control

449
00:13:15,050 --> 00:13:17,483
也是我们的软约束的方式

450
00:13:17,483 --> 00:13:18,883
那这个方式很重要的

451
00:13:18,883 --> 00:13:22,450
就是我们在所有的 getting

452
00:13:22,450 --> 00:13:24,966
就是那个悉数 getting 的上面介绍了

453
00:13:24,966 --> 00:13:26,966
那上面的这个就是 spasity

454
00:13:27,000 --> 00:13:30,683
然后加了一个 importance 的一个参数

455
00:13:30,683 --> 00:13:32,966
或者重要的指标

456
00:13:32,966 --> 00:13:34,366
你要通过这个指标来

457
00:13:34,366 --> 00:13:36,166
在我们的 lost 函数里面

458
00:13:36,166 --> 00:13:37,450
那这个是 x

459
00:13:37,683 --> 00:13:40,650
那个 lost 也就是所谓的辅助函数

460
00:13:40,650 --> 00:13:42,083
重要性的辅助函数

461
00:13:42,083 --> 00:13:45,766
加了一个 w importance 的相关的一个参数

462
00:13:46,333 --> 00:13:49,400
这种文叫做网络模型的超参去实现的

463
00:13:49,400 --> 00:13:50,966
那至于这个 CV

464
00:13:50,966 --> 00:13:51,966
这个 importance x

465
00:13:51,966 --> 00:13:54,450
就对我们的 x 进行一个输入

466
00:13:54,450 --> 00:13:56,766
加了一个特别 importance 的一个参数

467
00:13:56,766 --> 00:14:00,533
然后通过一个个 conference of variance 相关的

468
00:14:00,533 --> 00:14:02,533
就是我们这里面的 CV

469
00:14:02,850 --> 00:14:04,966
来去实现我们的均衡负载

470
00:14:04,966 --> 00:14:05,683
所以这里面

471
00:14:05,683 --> 00:14:06,450
最核心的就是说

472
00:14:06,450 --> 00:14:08,683
我多了一个辅助损失函数

473
00:14:08,683 --> 00:14:10,850
至于这个辅助损失函数怎么实现的

474
00:14:10,850 --> 00:14:12,933
也在 appendix 里面去实现的

475
00:14:12,933 --> 00:14:15,000
就是我们的后面重点的去讲

476
00:14:15,000 --> 00:14:17,850
那我们现在快速的过完这篇文章

477
00:14:17,850 --> 00:14:19,400
那这篇文章就说到了

478
00:14:19,400 --> 00:14:20,650
虽然刚才讲到的

479
00:14:20,650 --> 00:14:22,933
这个 l importance 的一个损失函数

480
00:14:22,933 --> 00:14:25,050
可以保证受专家的公平性

481
00:14:25,050 --> 00:14:27,083
但是专家有可能还是收到一个

482
00:14:27,083 --> 00:14:29,283
非常不同的数据的量

483
00:14:29,933 --> 00:14:31,600
我们仔细的看一下这段话

484
00:14:31,600 --> 00:14:32,850
它主要是表达

485
00:14:32,850 --> 00:14:34,283
虽然我们现在均衡负载

486
00:14:34,283 --> 00:14:35,200
可以使用

487
00:14:35,200 --> 00:14:38,966
推导出 l importance 较小这么一个结论

488
00:14:38,966 --> 00:14:39,800
但是 l importance

489
00:14:39,800 --> 00:14:42,333
较小了却不能保障均衡负载

490
00:14:42,333 --> 00:14:43,766
也就是说 l importance 较小

491
00:14:43,766 --> 00:14:46,850
只是均衡负载一个必要不充分的条件

492
00:14:46,933 --> 00:14:48,650
为了解决这个问题

493
00:14:48,650 --> 00:14:51,083
就引入了一个额外的损 10 函数

494
00:14:51,083 --> 00:14:52,600
叫做 l load

495
00:14:52,800 --> 00:14:53,333
这里面

496
00:14:53,333 --> 00:14:54,000
就用到了

497
00:14:54,000 --> 00:14:57,366
添加在每个专家上面的一个随机噪声

498
00:14:57,366 --> 00:14:58,200
通过这种方式

499
00:14:58,200 --> 00:14:59,083
放在 PANDAX1

500
00:14:59,083 --> 00:15:01,200
也就是他的复录一里面去介绍的

501
00:15:01,200 --> 00:15:03,533
whatever 甭管我们现在其他东西

502
00:15:03,533 --> 00:15:06,366
所以这篇文章的最核心的算法两个

503
00:15:06,683 --> 00:15:09,333
那第一个就是这里面的 getting

504
00:15:09,333 --> 00:15:10,483
我们加了噪声

505
00:15:10,483 --> 00:15:10,933
第二个

506
00:15:10,933 --> 00:15:12,250
加了稀疏性

507
00:15:12,250 --> 00:15:14,333
然后在我们做均衡负载的时候

508
00:15:14,333 --> 00:15:16,483
加了一个辅助的损失函数

509
00:15:16,483 --> 00:15:18,133
去使得我们的每个专家

510
00:15:18,133 --> 00:15:19,366
就更加的平衡

511
00:15:19,366 --> 00:15:21,366
那这里面的辅助的损伤数

512
00:15:21,366 --> 00:15:22,933
有两个一个是 out important

513
00:15:23,166 --> 00:15:24,683
一个是 outload

514
00:15:24,933 --> 00:15:27,333
那我们接下来往回看一看所有的东西

515
00:15:27,333 --> 00:15:28,800
我们再看一下它的 BANDIX

516
00:15:28,850 --> 00:15:29,683
因为看论文嘛

517
00:15:29,683 --> 00:15:31,000
肯定是简单的走读

518
00:15:31,000 --> 00:15:31,883
那这里面就说了

519
00:15:31,883 --> 00:15:33,166
我做了大量的实验

520
00:15:33,166 --> 00:15:34,083
各种各样的实验

521
00:15:34,083 --> 00:15:35,133
那这个实验蛮有意思

522
00:15:35,133 --> 00:15:37,250
就是指标是用的是 LPP

523
00:15:37,250 --> 00:15:37,483
no

524
00:15:37,483 --> 00:15:38,250
no no

525
00:15:38,250 --> 00:15:39,650
是 PPL

526
00:15:39,766 --> 00:15:40,600
perpacity

527
00:15:40,600 --> 00:15:41,366
宽阔度

528
00:15:41,366 --> 00:15:42,883
那这个宽阔度越小

529
00:15:42,883 --> 00:15:44,800
整个 PW 就越大

530
00:15:44,800 --> 00:15:46,766
一句话就是我们期望 sentence

531
00:15:46,766 --> 00:15:48,733
就 NLP 里面出现的概率

532
00:15:48,733 --> 00:15:49,766
就会越高

533
00:15:49,850 --> 00:15:50,966
所以说 PPL

534
00:15:50,966 --> 00:15:52,250
就越小越好

535
00:15:52,400 --> 00:15:53,450
那整个时间里面

536
00:15:53,450 --> 00:15:54,933
我们先看左边

537
00:15:54,933 --> 00:15:56,883
左边就是为了验证模型的容量

538
00:15:56,883 --> 00:15:58,366
提升带来的收益

539
00:15:58,366 --> 00:16:00,533
这里面就训练了 43225

540
00:16:00,533 --> 00:16:03,050
六个 exper 的一个 fat model

541
00:16:03,050 --> 00:16:04,283
也就是这个 fat model

542
00:16:04,283 --> 00:16:06,800
我们训练了很多的一个比较胖

543
00:16:06,800 --> 00:16:08,050
的一些网络模型

544
00:16:08,050 --> 00:16:09,200
还有包含二 56

545
00:16:09,200 --> 00:16:11,566
一零二 4 四零 9 六个 exper

546
00:16:11,566 --> 00:16:12,733
还有 Rekit model

547
00:16:12,733 --> 00:16:14,283
因为网络模型规模越大

548
00:16:14,450 --> 00:16:17,650
是要分开 highwacket 多层来去实现的

549
00:16:17,650 --> 00:16:18,483
而每层

550
00:16:18,483 --> 00:16:20,333
fat 里面激活四个专家

551
00:16:20,333 --> 00:16:23,283
highwacket 里面就激活两个专家

552
00:16:23,283 --> 00:16:24,533
从这个方面可以看到

553
00:16:24,533 --> 00:16:26,883
随着我们的网络模型规模扩大

554
00:16:27,366 --> 00:16:29,733
第一个现象就是我们可能需要引入了

555
00:16:29,733 --> 00:16:31,766
哈 lucky 就是分层的 M1 了

556
00:16:31,933 --> 00:16:32,850
那另外一个就

557
00:16:32,850 --> 00:16:35,050
随着我们的网络模型变大

558
00:16:35,050 --> 00:16:36,683
我们的模型的效果

559
00:16:36,683 --> 00:16:38,333
PPL 是越来越好的

560
00:16:38,333 --> 00:16:40,566
PPL 越低我们的效果是越好的

561
00:16:40,566 --> 00:16:42,366
这是其中一个概念

562
00:16:42,366 --> 00:16:42,800
那第二

563
00:16:42,800 --> 00:16:44,333
我们看一下右边

564
00:16:44,333 --> 00:16:46,166
传统的一个 OSTM 的模型

565
00:16:46,166 --> 00:16:48,133
跟我们现在加了 M1 的模型

566
00:16:48,250 --> 00:16:49,883
随着我们的模型规模变大

567
00:16:49,883 --> 00:16:52,166
我们的 PPL 也是越来越好的

568
00:16:52,166 --> 00:16:54,000
也就是模型规模增大

569
00:16:54,000 --> 00:16:56,683
我们的模型的效果会变好

570
00:16:56,883 --> 00:16:57,400
那这个

571
00:16:57,400 --> 00:16:58,966
是完全的在 17 年的时候

572
00:16:58,966 --> 00:17:02,283
符合我们的斯凋零落的这种法则

573
00:17:02,966 --> 00:17:05,000
那我们现在还看一下下面的内容

574
00:17:05,000 --> 00:17:07,133
基本上跟我刚才讲的差不多

575
00:17:07,166 --> 00:17:09,566
还有 competition efficiently

576
00:17:09,566 --> 00:17:11,533
就怎么去提升性能的

577
00:17:11,566 --> 00:17:12,650
没有太多的内容

578
00:17:12,650 --> 00:17:14,800
那接下来我们看一下 5.2

579
00:17:14,883 --> 00:17:16,000
100 billion 的

580
00:17:16,000 --> 00:17:20,650
谷歌的新闻的 copper 啦相关的内容

581
00:17:20,650 --> 00:17:21,850
那这里面蛮有意思的

582
00:17:21,850 --> 00:17:23,000
就是指标

583
00:17:23,000 --> 00:17:24,883
都是用 PPL 宽阔度

584
00:17:24,883 --> 00:17:27,483
但是随着我们的网络模型的增大

585
00:17:27,483 --> 00:17:29,200
那这面有两条曲线

586
00:17:29,200 --> 00:17:30,133
蓝色的这条

587
00:17:30,133 --> 00:17:33,600
就是用了 10B 脸的一个语料进行训练的

588
00:17:33,600 --> 00:17:34,283
下面这个

589
00:17:34,283 --> 00:17:36,566
是用了 100B 脸的语料进行训练的

590
00:17:36,566 --> 00:17:37,166
可以看到

591
00:17:37,166 --> 00:17:39,000
随着我们的数据量越大

592
00:17:39,333 --> 00:17:40,283
我们的模型

593
00:17:40,283 --> 00:17:42,000
的一个效果是越好的

594
00:17:42,000 --> 00:17:43,283
也是在 17 年的时候

595
00:17:43,283 --> 00:17:46,966
谷歌已经感觉有这个斯高林洛的方案

596
00:17:46,966 --> 00:17:48,333
所以谷歌也在做

597
00:17:48,333 --> 00:17:49,650
不过没有 open AI

598
00:17:49,650 --> 00:17:51,283
一根筋做到底而已

599
00:17:51,283 --> 00:17:53,800
或者直接用了那个 decorder 的架构

600
00:17:53,850 --> 00:17:56,400
那我们现在还是在 in 时代 VF

601
00:17:56,400 --> 00:17:59,133
没关系我们现在来往下看一下

602
00:17:59,200 --> 00:18:02,133
接着就是对应的一些具体的校任务

603
00:18:02,133 --> 00:18:03,400
我们新 translation

604
00:18:03,400 --> 00:18:06,650
我们的机器翻译相关的一个性能

605
00:18:06,650 --> 00:18:07,883
还有 Multi language

606
00:18:07,883 --> 00:18:10,133
多语言的一个翻译的性能了

607
00:18:10,166 --> 00:18:10,966
重点讲了很多

608
00:18:10,966 --> 00:18:12,366
反正我的模型效果很好

609
00:18:12,366 --> 00:18:15,133
用的这个 moe 的架构

610
00:18:15,133 --> 00:18:16,566
就反正这个形态

611
00:18:17,483 --> 00:18:18,400
那接下来的内容

612
00:18:18,400 --> 00:18:19,933
可能会相对复杂一点

613
00:18:19,933 --> 00:18:23,250
我们简单的去跟大家组读一下 appendix

614
00:18:23,250 --> 00:18:24,600
它的一个副录

615
00:18:24,766 --> 00:18:26,733
对应的一个均衡负载

616
00:18:26,733 --> 00:18:28,483
的一个具体的计算喽

617
00:18:28,600 --> 00:18:30,800
那我们现在还是看回它的副录

618
00:18:30,800 --> 00:18:31,450
副录里面

619
00:18:31,450 --> 00:18:33,850
讲的最核心的就是 low balance

620
00:18:33,850 --> 00:18:34,650
一个 lost

621
00:18:34,650 --> 00:18:35,766
一个新的 lost

622
00:18:36,000 --> 00:18:37,166
就好像我们刚才讲到的

623
00:18:37,166 --> 00:18:38,333
在 CS4 里面

624
00:18:38,333 --> 00:18:40,883
为了去解决我们的负载均衡

625
00:18:40,883 --> 00:18:41,400
所以现在

626
00:18:41,400 --> 00:18:43,366
要定义一个额外的损失函数

627
00:18:43,366 --> 00:18:44,650
来鼓励专家

628
00:18:44,650 --> 00:18:47,800
去接受大致一致的相关的数据的样本

629
00:18:47,800 --> 00:18:48,333
不过

630
00:18:48,333 --> 00:18:50,450
因为专家收到的样本数量是离散的

631
00:18:50,450 --> 00:18:53,083
所以不能用于一个反向传播

632
00:18:53,083 --> 00:18:54,566
所以说 unfortunately 了

633
00:18:54,766 --> 00:18:56,166
因此这篇文章

634
00:18:56,166 --> 00:18:58,933
设计了一个平滑的 estimate 估计器

635
00:18:58,933 --> 00:19:01,166
然后叫做 l load

636
00:19:01,166 --> 00:19:03,050
一个 s 就是 load x 了

637
00:19:03,050 --> 00:19:05,283
它输入的是一个具体的 betch size

638
00:19:05,283 --> 00:19:07,566
然后分配给到每一个专家

639
00:19:07,566 --> 00:19:08,883
也就是为了这个 l load

640
00:19:08,883 --> 00:19:11,766
主要是帮助我们的数据去平衡的

641
00:19:11,766 --> 00:19:13,083
分配下面

642
00:19:13,083 --> 00:19:16,166
其实最核心的就是呃这一条公式

643
00:19:16,166 --> 00:19:18,800
那这一条公式感觉想的非常的多

644
00:19:18,883 --> 00:19:21,050
具体它代表的是什么

645
00:19:21,200 --> 00:19:23,400
我们回顾一下前面的 MV 的设计

646
00:19:23,400 --> 00:19:24,400
设计了 HS

647
00:19:24,400 --> 00:19:26,850
为 keep talk k 的一个函数的输入

648
00:19:26,850 --> 00:19:29,283
也就是前面的这一条了

649
00:19:29,283 --> 00:19:31,650
这个是 keep talk k 的一个具体的输入

650
00:19:32,083 --> 00:19:34,166
那这里面新定义了一个东西

651
00:19:34,166 --> 00:19:37,050
叫做 KTH 的一个 SQT

652
00:19:37,050 --> 00:19:37,850
这个 SQT

653
00:19:37,850 --> 00:19:42,133
表示在去掉 SHX 里面第 i 个分量之后

654
00:19:42,133 --> 00:19:43,800
就第 i 个 s 破之后

655
00:19:44,166 --> 00:19:46,166
排在第 k 大的一个值

656
00:19:46,166 --> 00:19:47,000
那据这个

657
00:19:47,000 --> 00:19:50,000
在定义下面的这个所谓的第 i 个专家

658
00:19:50,000 --> 00:19:52,650
处理输入输出的一个具体的概率率

659
00:19:52,650 --> 00:19:54,650
我们的 PXI

660
00:19:55,000 --> 00:19:55,683
呃整体来说

661
00:19:55,683 --> 00:19:56,766
就是固定其他分量

662
00:19:56,766 --> 00:19:58,533
已经选取好的一个 noise

663
00:19:58,533 --> 00:19:59,450
就是噪声

664
00:19:59,650 --> 00:20:02,800
重新的给第 i 个分量添加一次噪声

665
00:20:02,883 --> 00:20:06,050
结果比 KTH excuting 大的概率

666
00:20:06,050 --> 00:20:08,483
所以整体公式就变成这个了

667
00:20:08,600 --> 00:20:10,283
那我们现在通过这个 noise

668
00:20:10,283 --> 00:20:11,683
可以把第二个专家

669
00:20:11,683 --> 00:20:12,883
是否去处理

670
00:20:12,883 --> 00:20:14,400
我们输入的 TOKEN 的离散值

671
00:20:14,400 --> 00:20:15,933
变成了第二个专家

672
00:20:15,933 --> 00:20:17,366
处理这个输入的概率

673
00:20:17,366 --> 00:20:19,450
这么一个平滑的估计

674
00:20:20,450 --> 00:20:21,366
因此 PSI

675
00:20:21,366 --> 00:20:23,566
实际上代表的是这么一个概率

676
00:20:23,566 --> 00:20:24,483
那所以这些概率

677
00:20:24,483 --> 00:20:26,733
就简化成为下面的这条公式了

678
00:20:27,083 --> 00:20:29,083
而这里面有一个参数

679
00:20:29,083 --> 00:20:29,733
这个参数

680
00:20:29,733 --> 00:20:31,766
其实就是标准正在分布的 CDR

681
00:20:32,000 --> 00:20:32,533
那接下来

682
00:20:32,533 --> 00:20:35,333
我们去把第 i 个专家的一个负载

683
00:20:35,333 --> 00:20:37,683
就可以直接定义为 load XI

684
00:20:37,683 --> 00:20:39,483
i 就是我们的第 i 个专家

685
00:20:39,483 --> 00:20:42,850
里面的负载就是 PXI 这么一条公式了

686
00:20:43,366 --> 00:20:43,883
那我们看到

687
00:20:43,883 --> 00:20:45,600
实际上在真正执行的时候

688
00:20:45,600 --> 00:20:47,566
是执行下面这个公式的

689
00:20:47,566 --> 00:20:49,166
w 就是一个超餐了

690
00:20:49,166 --> 00:20:51,400
那 CV 就是一个 coefficient

691
00:20:51,400 --> 00:20:52,083
是不是 why

692
00:20:52,083 --> 00:20:53,166
就是离散系数

693
00:20:53,400 --> 00:20:54,400
就使用概率分布的

694
00:20:54,400 --> 00:20:56,450
一个离散化的一个具体的度量

695
00:20:56,450 --> 00:20:57,600
不过这些概率分布

696
00:20:57,600 --> 00:20:59,450
其实简单的可以加可能不加

697
00:20:59,450 --> 00:21:00,883
没有太多的影响

698
00:21:00,966 --> 00:21:02,766
最终这里面就讲到了

699
00:21:02,766 --> 00:21:05,566
我用了两个 loss 或两个超仓

700
00:21:05,566 --> 00:21:06,650
一个是 w important

701
00:21:06,800 --> 00:21:07,600
一个是 w

702
00:21:07,883 --> 00:21:10,000
然后最终的实验的 PPL

703
00:21:10,000 --> 00:21:11,050
比没有使用

704
00:21:11,050 --> 00:21:12,333
因为志玲就没有使用嘛

705
00:21:12,333 --> 00:21:14,766
就这一条没有使用是 39.8

706
00:21:15,083 --> 00:21:16,683
使用了或者两个都使用了

707
00:21:16,683 --> 00:21:18,450
我们的 loss 能降的更低

708
00:21:18,450 --> 00:21:20,483
我们的 PPL 能够实现的更好

709
00:21:20,733 --> 00:21:22,650
就是是用了这两个巨囊负载

710
00:21:22,650 --> 00:21:24,133
使得我们的模型的效果

711
00:21:24,133 --> 00:21:25,850
就进一步的提升

712
00:21:26,600 --> 00:21:27,050
那当然了

713
00:21:27,050 --> 00:21:27,800
下面的呃

714
00:21:27,800 --> 00:21:30,400
pandas b 就是讲到多级的 m

715
00:21:30,400 --> 00:21:31,450
e 需怎么设计

716
00:21:31,450 --> 00:21:32,250
所以这篇文章

717
00:21:32,250 --> 00:21:35,083
还是 pandas 做了非常多有用的贡献的

718
00:21:35,083 --> 00:21:36,733
不过二级或者二层的

719
00:21:36,733 --> 00:21:39,933
我觉得大家也可以不用太多的去关心

720
00:21:40,283 --> 00:21:41,283
那下面的实验

721
00:21:41,283 --> 00:21:44,766
其实中米觉得比较有意思的就是 TABLE8

722
00:21:44,850 --> 00:21:46,083
TABLE8 这个就说了

723
00:21:46,083 --> 00:21:48,133
我们的模型规模更大

724
00:21:48,133 --> 00:21:49,850
那这里面我们可以看出来

725
00:21:49,850 --> 00:21:51,850
在专家数量不多的时候

726
00:21:51,850 --> 00:21:53,083
提升专家的数量

727
00:21:53,083 --> 00:21:55,883
我们的模型效果是有效的去提升的

728
00:21:56,050 --> 00:21:58,566
但是收益会慢慢的降低

729
00:21:58,566 --> 00:22:01,166
甚至出现了专家数量越来越多

730
00:22:01,166 --> 00:22:03,133
我们的模型效果反而不好

731
00:22:03,133 --> 00:22:04,966
所以我们要控制好专家的数量

732
00:22:04,966 --> 00:22:05,650
你说都

733
00:22:05,650 --> 00:22:09,250
到了个十百千万十万十万个专家

734
00:22:09,250 --> 00:22:11,133
说实话有必要搞那么多专家吗

735
00:22:11,133 --> 00:22:12,733
所以我们看到最近特别火的

736
00:22:12,733 --> 00:22:15,133
Dipstick 的一个 V3 MV 的架构

737
00:22:15,133 --> 00:22:17,250
只有 256 有个专家

738
00:22:17,250 --> 00:22:19,283
那未来可能会越来越多的专家

739
00:22:19,283 --> 00:22:20,400
模型效果越来越好

740
00:22:20,400 --> 00:22:21,250
为什么这么猜测

741
00:22:21,250 --> 00:22:23,600
因为在 17 年的时候这个技术已经有了

742
00:22:23,600 --> 00:22:26,133
距今已经到 7 年多了

743
00:22:26,200 --> 00:22:27,283
蛮有意思的

744
00:22:27,533 --> 00:22:28,450
那这篇文章最后

745
00:22:28,450 --> 00:22:30,650
还有一些简单的小实验

746
00:22:30,650 --> 00:22:31,533
也就是验证一下

747
00:22:31,533 --> 00:22:33,250
每个专家或者不同的专家

748
00:22:33,366 --> 00:22:34,883
到底是不是不同的专家

749
00:22:34,883 --> 00:22:36,333
学到不同的指认物

750
00:22:36,333 --> 00:22:36,800
实际上

751
00:22:36,800 --> 00:22:38,166
确实也是非常合理的

752
00:22:38,166 --> 00:22:40,050
反正当时候的 in

753
00:22:40,050 --> 00:22:41,566
相对比现在的全 SOMO

754
00:22:41,566 --> 00:22:42,800
更好的去验证

755
00:22:42,800 --> 00:22:44,800
每个专家处理的不同的 TOKEN

756
00:22:44,800 --> 00:22:46,600
用我们自己把 TOKEN 丢给我们的

757
00:22:46,850 --> 00:22:48,566
in 的循环神经网络模型

758
00:22:48,566 --> 00:22:51,250
每一个 r 或者每一个循环神器的

759
00:22:51,250 --> 00:22:52,850
就处理其中的一个透感

760
00:22:52,850 --> 00:22:55,200
所以当时做这种验照的相对比较简单

761
00:22:55,200 --> 00:22:57,200
现在都变成全收买价架构了

762
00:22:57,200 --> 00:22:58,766
所以去验证这种黑盒子

763
00:22:58,766 --> 00:23:00,366
反而不那么的容易

764
00:23:00,366 --> 00:23:01,766
不过这个是后话了

765
00:23:01,766 --> 00:23:02,566
那下面的东西

766
00:23:02,566 --> 00:23:04,366
中美觉得可能并不那么的重要

767
00:23:04,366 --> 00:23:05,333
我们今天

768
00:23:05,333 --> 00:23:07,483
主要是跟大家分享了

769
00:23:07,483 --> 00:23:09,000
或者走读了这篇文章

770
00:23:09,000 --> 00:23:10,733
auto agent nature level

771
00:23:10,766 --> 00:23:14,966
一个 SPAS 的一个稀疏门控的 MOE 的架构

772
00:23:15,933 --> 00:23:17,200
之所以要分享这篇文章

773
00:23:17,200 --> 00:23:18,166
是因为这篇文章

774
00:23:18,166 --> 00:23:20,450
是 2017 年的时候

775
00:23:20,450 --> 00:23:22,933
最大的参数规模的大模型

776
00:23:22,933 --> 00:23:25,083
而且认为是 Moe

777
00:23:25,083 --> 00:23:26,800
在大元模型里面的非常重要的

778
00:23:26,800 --> 00:23:28,083
里程碑的一个模型

779
00:23:28,083 --> 00:23:29,650
加固和相关的论文

780
00:23:29,650 --> 00:23:32,366
那今天的内容就先到这里了

781
00:23:32,366 --> 00:23:33,333
谢谢各位

782
00:23:33,333 --> 00:23:33,966
拜了个拜

783
00:23:33,966 --> 00:23:35,483
有兴趣的可以在这里面

784
00:23:35,483 --> 00:23:37,966
去下载相关你想了解的资料

