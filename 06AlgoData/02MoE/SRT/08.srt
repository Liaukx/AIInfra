1
00:00:00,000 --> 00:00:02,633
内容/录制:Z0MI 酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,633 --> 00:00:03,366
哈喽大家好

3
00:00:03,400 --> 00:00:06,566
我是个在不吃和少吃之间

4
00:00:06,600 --> 00:00:08,766
选择了不少吃的 ZOMI

5
00:00:08,766 --> 00:00:10,133
所以现在越来越胖了

6
00:00:11,466 --> 00:00:11,900
今天

7
00:00:11,933 --> 00:00:14,600
我们来到了 Moe 相关的一个论文

8
00:00:14,600 --> 00:00:16,233
主读里面的最后一个内容了

9
00:00:16,233 --> 00:00:17,933
我们看一下 Deepseak 的 Moe

10
00:00:18,066 --> 00:00:19,733
这篇对应的论文

11
00:00:19,766 --> 00:00:21,633
ZOMI 在非常划时代的

12
00:00:21,633 --> 00:00:23,266
 还是 GPT 时代里面

13
00:00:23,533 --> 00:00:24,733
里面的幻方量化

14
00:00:24,733 --> 00:00:26,566
在 2024 年年初

15
00:00:26,966 --> 00:00:28,533
或者在 2023 年年底

16
00:00:28,533 --> 00:00:31,266
发表的 Deepseek MOE 相关的这篇论文

17
00:00:31,333 --> 00:00:33,366
蛮有意思就是首先

18
00:00:33,366 --> 00:00:36,733
我们还回顾整个 MOE 系列的一个大纲

19
00:00:36,733 --> 00:00:39,066
我们现在还是在 moe 的一个核心工作

20
00:00:39,066 --> 00:00:41,733
论文里面最后几个内容了

21
00:00:41,766 --> 00:00:43,333
就是 Deepseek 的 Moe

22
00:00:43,333 --> 00:00:43,933
这里面

23
00:00:43,933 --> 00:00:45,400
也会跟我们个大模型

24
00:00:45,400 --> 00:00:47,400
遇上 moe 比较相关

25
00:00:47,400 --> 00:00:48,600
或者比较重合

26
00:00:48,800 --> 00:00:49,000
当然了

27
00:00:49,000 --> 00:00:51,133
ZOMI 觉得如果大家对什么都不懂

28
00:00:51,133 --> 00:00:52,933
或者对 MOE 架构都不了解

29
00:00:53,000 --> 00:00:56,400
还是重点看一下 MOE 的一个架构原理

30
00:00:56,400 --> 00:00:57,600
我们将会在下一节

31
00:00:57,600 --> 00:01:00,633
跟大家简单的可视化的剖析 MOE

32
00:01:00,633 --> 00:01:03,166
还有一个 MOE 的基础的 gShare

33
00:01:03,266 --> 00:01:03,966
现在

34
00:01:04,000 --> 00:01:06,366
我们马上来到了第一个内容

35
00:01:06,366 --> 00:01:08,933
去翻一翻 Deepseek Moe 这一篇论文

36
00:01:08,933 --> 00:01:10,866
也就是 Deepseek 发表的第一篇

37
00:01:10,866 --> 00:01:13,266
关于大模型 Moe 专家架构的

38
00:01:13,266 --> 00:01:13,966
这篇论文

39
00:01:14,000 --> 00:01:14,666
对业界

40
00:01:14,666 --> 00:01:17,500
实际上有一个非常大的作用

41
00:01:18,366 --> 00:01:20,000
我们简单的还是看一下

42
00:01:20,000 --> 00:01:22,200
整个 Deepseek MOE 的一个摘要

43
00:01:22,266 --> 00:01:24,633
首先 Deepseek MOE 这篇论文

44
00:01:24,633 --> 00:01:27,266
是采用了一个 moe 的架构的

45
00:01:27,266 --> 00:01:30,033
最核心的就提出了一个 Share expert

46
00:01:30,033 --> 00:01:31,933
也就是所谓的共享专家

47
00:01:31,966 --> 00:01:33,400
通过一个共享专家

48
00:01:33,400 --> 00:01:35,400
加上其他我们的路由专家

49
00:01:35,400 --> 00:01:38,933
去组合提升我们专家的优化能力

50
00:01:38,933 --> 00:01:40,633
和专家的专业程度

51
00:01:40,766 --> 00:01:44,333
接着就讲了很多的一些相关的内容

52
00:01:44,333 --> 00:01:46,400
就是我们的规模的一些挑战

53
00:01:46,666 --> 00:01:47,233
Scaling 的时候

54
00:01:47,233 --> 00:01:49,300
怎么去解决我们的一个计算开销

55
00:01:49,333 --> 00:01:52,466
还有提出了一个专家的一个负载均衡

56
00:01:52,466 --> 00:01:53,566
相关的内容

57
00:01:53,600 --> 00:01:56,433
接着有了一个共享专家的提出

58
00:01:56,433 --> 00:01:58,833
还有相关的一些优化的算法

59
00:01:59,033 --> 00:02:00,366
基本上就这么多

60
00:02:00,400 --> 00:02:01,800
训练的过程当中

61
00:02:01,800 --> 00:02:04,266
也提出了三阶段的训练

62
00:02:04,266 --> 00:02:06,100
从简单的训练到专家的强化

63
00:02:06,133 --> 00:02:07,466
到协同的训练

64
00:02:07,466 --> 00:02:09,566
而且训练效率的提升

65
00:02:09,833 --> 00:02:11,066
就说了我们采用了一个

66
00:02:11,066 --> 00:02:13,033
更低精度的数据的格式

67
00:02:13,033 --> 00:02:15,100
去减少我们的计算的耗时

68
00:02:15,133 --> 00:02:16,033
整体来说

69
00:02:16,033 --> 00:02:17,466
我们现在重点的

70
00:02:17,466 --> 00:02:20,700
去打开 Deepseek Moe 相关的文章

71
00:02:20,733 --> 00:02:23,000
跟大家一起去重点的解读一下的

72
00:02:23,333 --> 00:02:26,466
现在我们打开 Deepseek Moe 的第一篇文章

73
00:02:26,466 --> 00:02:30,066
也是在 2024 年 1 月份的时候去发布的

74
00:02:30,066 --> 00:02:30,733
说实话

75
00:02:30,766 --> 00:02:32,366
ZOMI 其实现在有点迷茫

76
00:02:32,366 --> 00:02:34,466
我到底解读这些论文的好还是不好

77
00:02:34,466 --> 00:02:34,700
其实

78
00:02:34,733 --> 00:02:36,766
也没有太多人能关注我的公众号

79
00:02:36,800 --> 00:02:38,266
然后也没有公众号了

80
00:02:38,266 --> 00:02:40,333
没有太多人关注我的一个相关的视频

81
00:02:40,366 --> 00:02:41,666
也没有给我太多的反馈

82
00:02:41,733 --> 00:02:43,333
反正我就傻傻的自己录着

83
00:02:43,333 --> 00:02:44,200
录着录着吧

84
00:02:44,533 --> 00:02:46,166
哎呀也不知道什么时候是到个头

85
00:02:46,166 --> 00:02:47,233
也不知道录了为啥

86
00:02:47,233 --> 00:02:48,666
为为了是干啥

87
00:02:50,600 --> 00:02:51,066
没关系

88
00:02:51,066 --> 00:02:53,833
我们现在来到了整个 Deepseek Moe 的文章

89
00:02:53,833 --> 00:02:55,266
Deepseek 这篇文章

90
00:02:55,266 --> 00:02:56,433
ZOMI 还是比较关心的

91
00:02:56,433 --> 00:02:58,133
就是它的一个标题

92
00:02:58,166 --> 00:02:58,966
这个标题

93
00:02:58,966 --> 00:02:59,366
蛮有意思

94
00:02:59,366 --> 00:03:02,633
就是 Ultimate Expert Specialization

95
00:03:03,000 --> 00:03:03,400
蛮有意思的

96
00:03:03,400 --> 00:03:05,866
中文我就比较难去翻译

97
00:03:05,866 --> 00:03:07,533
在混合专家模型

98
00:03:07,566 --> 00:03:10,733
迈向终极的专家专业形态

99
00:03:11,400 --> 00:03:12,166
好像是这样的解

100
00:03:12,166 --> 00:03:13,366
释但没关系

101
00:03:13,366 --> 00:03:15,433
反正就是整个 Expert

102
00:03:15,433 --> 00:03:16,866
更加 Specialization

103
00:03:16,866 --> 00:03:17,966
就更加专业化了

104
00:03:18,000 --> 00:03:19,033
怎么去专业化

105
00:03:19,033 --> 00:03:20,666
我们就体现下面的内容

106
00:03:20,766 --> 00:03:22,233
ZOMI 还是第一个关心的

107
00:03:22,233 --> 00:03:23,033
就是他的一个作者

108
00:03:23,033 --> 00:03:23,966
真行蛮有意思

109
00:03:24,000 --> 00:03:25,466
就大家可能会比较关心

110
00:03:25,466 --> 00:03:27,333
我这里面的标环的一个人

111
00:03:27,366 --> 00:03:30,600
他就是幻方量化的 CEO

112
00:03:30,866 --> 00:03:31,533
梁文峰

113
00:03:31,566 --> 00:03:32,533
这里面的这篇文章

114
00:03:32,533 --> 00:03:33,200
他也是挂了

115
00:03:33,200 --> 00:03:34,366
自己是作者的

116
00:03:34,366 --> 00:03:34,800
蛮有意思

117
00:03:34,800 --> 00:03:36,166
就是这篇文章

118
00:03:36,166 --> 00:03:36,866
跟北京大学

119
00:03:36,866 --> 00:03:38,900
清华大学还有南京大学进行合作的

120
00:03:38,933 --> 00:03:40,466
当然了离不开 Deepseek

121
00:03:40,466 --> 00:03:41,933
一个 AI 研究院

122
00:03:42,433 --> 00:03:44,233
我们接下来来看一下整个 Abstract

123
00:03:44,233 --> 00:03:46,133
也就是相关的一个摘要

124
00:03:46,166 --> 00:03:47,200
这里面就说了

125
00:03:47,200 --> 00:03:49,233
其实在 LLM 时代

126
00:03:49,233 --> 00:03:51,433
MOE 的专家是非常有前途的

127
00:03:51,433 --> 00:03:51,900
只不过

128
00:03:51,933 --> 00:03:53,566
我们之前没有

129
00:03:53,766 --> 00:03:54,833
就是第一个

130
00:03:54,833 --> 00:03:55,966
就遇到很多问题

131
00:03:56,000 --> 00:03:58,533
就是不能够很好的稳定的收敛了

132
00:03:58,533 --> 00:03:59,000
第二个

133
00:03:59,000 --> 00:04:01,066
就是每个专家的获取的知识

134
00:04:01,066 --> 00:04:02,166
都不太一样

135
00:04:02,200 --> 00:04:03,866
所以为了解决这一系列的问题

136
00:04:03,866 --> 00:04:05,666
我们就提出了一个 Deepseek

137
00:04:05,666 --> 00:04:07,333
Moe 的网络模型架构

138
00:04:07,466 --> 00:04:09,033
这个网络模型架构

139
00:04:09,033 --> 00:04:11,166
主要有两个重点

140
00:04:11,200 --> 00:04:14,400
就两个 principal 来两个重点了

141
00:04:14,400 --> 00:04:15,000
第一个

142
00:04:15,000 --> 00:04:16,566
就是将专家细分

143
00:04:16,566 --> 00:04:19,966
成为很多不同的一个小专家

144
00:04:19,966 --> 00:04:20,833
并且允许

145
00:04:20,833 --> 00:04:22,900
可以对我们专家进行一个灵活的组合

146
00:04:22,933 --> 00:04:24,333
和激活的方式的

147
00:04:24,533 --> 00:04:25,166
第二个

148
00:04:25,166 --> 00:04:28,033
就是将共享专家隔离出来

149
00:04:28,033 --> 00:04:30,433
作为 isolate 一个独立的专家

150
00:04:30,433 --> 00:04:31,966
叫做共享专家

151
00:04:32,000 --> 00:04:33,233
通过这种共享专家

152
00:04:33,233 --> 00:04:35,866
来获得更多共同的一个知识

153
00:04:35,866 --> 00:04:38,233
减少我们的路由专家的一个冗余

154
00:04:38,600 --> 00:04:39,733
蛮有意思的就是一开

155
00:04:39,733 --> 00:04:40,833
始我可以发现

156
00:04:40,833 --> 00:04:43,366
在整个网络模型的探索的过程当中

157
00:04:43,400 --> 00:04:47,133
从两 b 到 16B 到后面的 145B

158
00:04:47,400 --> 00:04:49,833
这么一个网站模型规模变大的

159
00:04:49,833 --> 00:04:50,866
进行一个探索

160
00:04:50,866 --> 00:04:51,933
每一次探索

161
00:04:51,966 --> 00:04:53,133
都对比不同的文章

162
00:04:53,133 --> 00:04:53,633
两边

163
00:04:53,633 --> 00:04:55,866
就是对比 gShare 的一个具体的性能

164
00:04:56,166 --> 00:04:56,733
16B

165
00:04:56,733 --> 00:04:59,966
就对比 Llmma27B 的一个网络模型性能

166
00:05:00,033 --> 00:05:02,733
165B 就对比一个 gShare 的结构

167
00:05:02,766 --> 00:05:03,966
还有个 Deepseek 自己的

168
00:05:03,966 --> 00:05:05,600
一个 67B 的一个性能

169
00:05:05,600 --> 00:05:06,600
所以说基本上了

170
00:05:06,600 --> 00:05:09,033
他就网络模型规模就越做越大

171
00:05:09,033 --> 00:05:11,900
然后根据上面提出来的两个重点

172
00:05:12,933 --> 00:05:14,166
在整篇文章里面

173
00:05:14,166 --> 00:05:15,133
我们现在来到了

174
00:05:15,133 --> 00:05:17,066
一个 introduction 的一个内容哎

175
00:05:17,133 --> 00:05:18,766
也就是对应的简介了

176
00:05:18,766 --> 00:05:20,166
这里面就说到了

177
00:05:20,166 --> 00:05:22,400
其实有很多 recent research

178
00:05:22,400 --> 00:05:25,366
最近的一些相关的论文研究

179
00:05:25,366 --> 00:05:27,833
就表明只要有足够的数据

180
00:05:27,833 --> 00:05:28,633
sufficient

181
00:05:28,633 --> 00:05:29,466
training data

182
00:05:29,466 --> 00:05:31,266
然后我们的模型的算力越大

183
00:05:31,466 --> 00:05:32,566
网络模型规模越大

184
00:05:32,600 --> 00:05:33,600
我们的模型架构

185
00:05:33,600 --> 00:05:35,566
或者我们的模型效果就越好

186
00:05:35,633 --> 00:05:37,466
所以这个就表明了

187
00:05:37,466 --> 00:05:40,633
反正往 scaling Law 的方式做做行了

188
00:05:40,633 --> 00:05:42,833
因为我们现在的算力成本很大

189
00:05:42,833 --> 00:05:44,066
所以说业界了

190
00:05:44,066 --> 00:05:46,700
就为了解决算力成本的问题

191
00:05:46,733 --> 00:05:48,800
就提出了 moe 的专家

192
00:05:48,833 --> 00:05:50,566
有了这个 moe 的专家之后

193
00:05:50,600 --> 00:05:51,566
你就会发现

194
00:05:51,566 --> 00:05:54,066
我们的网络模型的规模增大的时候

195
00:05:54,066 --> 00:05:55,900
我们的算力成本没有增大

196
00:05:55,966 --> 00:05:58,466
所以这里面就说到 MOE 专家

197
00:05:58,466 --> 00:05:59,533
是很有前途的

198
00:05:59,566 --> 00:06:02,600
但是尽管 MOE 专家很有前途

199
00:06:02,600 --> 00:06:05,200
但是 MOE 专家也遇到几个问题

200
00:06:05,200 --> 00:06:06,466
这里面就罗列了

201
00:06:06,466 --> 00:06:07,066
第一个问题

202
00:06:07,066 --> 00:06:09,300
就是我们的知识的混合

203
00:06:09,300 --> 00:06:11,333
因为我们的专家很多

204
00:06:11,333 --> 00:06:12,600
知识怎么去学习

205
00:06:12,600 --> 00:06:14,366
就变成了一个很难的问题了

206
00:06:14,400 --> 00:06:14,966
第二个

207
00:06:14,966 --> 00:06:16,866
就是知识的冗余问题

208
00:06:16,866 --> 00:06:18,433
knowledge reductant

209
00:06:18,433 --> 00:06:19,733
知识的冗余分配

210
00:06:19,766 --> 00:06:21,033
给不同专家的 TOKEN

211
00:06:21,033 --> 00:06:22,966
可能需要学习共同的知识

212
00:06:23,000 --> 00:06:24,866
以前的专家越来越多

213
00:06:24,866 --> 00:06:25,866
所以说你会发现

214
00:06:25,866 --> 00:06:28,566
每个专家到底学到什么是不清楚的

215
00:06:28,600 --> 00:06:29,866
所以这篇论文里面

216
00:06:29,866 --> 00:06:32,266
就刚才讲到的 Deepseek moe

217
00:06:32,466 --> 00:06:32,966
这篇文章

218
00:06:33,000 --> 00:06:36,033
就主要提出了两个主要的策略

219
00:06:36,033 --> 00:06:36,900
to principle

220
00:06:36,933 --> 00:06:40,066
第一个就是 Fine-Grained Expert Segmentation

221
00:06:40,066 --> 00:06:43,666
也就是细腻度的专家的分割

222
00:06:44,133 --> 00:06:45,833
所谓的细腻度专家分割

223
00:06:45,833 --> 00:06:48,633
其实比较有意思或者比较简单的

224
00:06:48,633 --> 00:06:50,833
就是我们的以前一个 FFN

225
00:06:50,833 --> 00:06:52,500
就是一个简单的专家嘛

226
00:06:52,533 --> 00:06:55,033
我们现在把 FFN 再切细

227
00:06:55,133 --> 00:06:58,033
把 FFN 的参数的规模减少

228
00:06:58,033 --> 00:07:00,533
但是把 FFN 的数量的增加

229
00:07:00,566 --> 00:07:01,233
这里面

230
00:07:01,233 --> 00:07:03,033
我们可能有一个专业的术语

231
00:07:03,033 --> 00:07:05,166
叫做小专家模型

232
00:07:05,833 --> 00:07:07,100
小专家的模型

233
00:07:07,133 --> 00:07:08,933
小是指我们的专家模型

234
00:07:08,933 --> 00:07:10,066
参数量的规模少

235
00:07:10,133 --> 00:07:12,833
但是我们变成了一个多专家的模式

236
00:07:12,866 --> 00:07:14,700
就是专家的参数量变少

237
00:07:14,733 --> 00:07:16,833
但是专家的数量变多

238
00:07:16,833 --> 00:07:17,700
通过这种方式

239
00:07:17,733 --> 00:07:21,066
我们要做 fine-Grained 的细腻度的专家分割

240
00:07:21,133 --> 00:07:21,733
第二个

241
00:07:21,733 --> 00:07:23,533
就提出了一个共享专家

242
00:07:23,533 --> 00:07:25,366
独立个共享专家出来的好处

243
00:07:25,366 --> 00:07:26,733
就是可以对我们的数据

244
00:07:26,733 --> 00:07:28,000
或者对我们的知识

245
00:07:28,033 --> 00:07:30,500
更好的去学习和压缩的

246
00:07:30,533 --> 00:07:32,533
有了这一个内容之后

247
00:07:32,533 --> 00:07:34,033
这里面下面就讲到了

248
00:07:34,466 --> 00:07:36,333
我在 2B16B

249
00:07:36,366 --> 00:07:38,733
还有对应的 145B 里面

250
00:07:38,966 --> 00:07:41,166
就实现了一个比较好的效果

251
00:07:41,166 --> 00:07:41,933
这个效果

252
00:07:41,933 --> 00:07:42,833
我们就看一下

253
00:07:43,133 --> 00:07:44,433
整个图我们可以看到

254
00:07:44,433 --> 00:07:47,333
纵轴是我们的网络模型的参数量

255
00:07:47,366 --> 00:07:49,833
然后这个是我们的网络模型的性能

256
00:07:49,833 --> 00:07:51,300
所谓的真正的参数量

257
00:07:51,333 --> 00:07:52,800
是指激活的参数量

258
00:07:52,800 --> 00:07:54,966
因为 MOE 专家这个 16B

259
00:07:54,966 --> 00:07:56,966
不代表它是真正激活的

260
00:07:57,200 --> 00:07:59,333
16B 是指他所有的参数量

261
00:07:59,333 --> 00:08:01,966
真正激活的可能只有 2 点多 b

262
00:08:01,966 --> 00:08:03,533
接近 3B 然后但是

263
00:08:03,533 --> 00:08:05,366
我的网络模型的一个规模

264
00:08:05,366 --> 00:08:07,766
或者网络模型的效果取得非常好

265
00:08:07,766 --> 00:08:09,333
所以说 Deepseek MOE

266
00:08:09,333 --> 00:08:10,433
就是独树一帜

267
00:08:10,433 --> 00:08:11,566
反正大家都在这条

268
00:08:11,600 --> 00:08:13,200
个回归线上面徘徊

269
00:08:13,200 --> 00:08:16,000
但是我 Deepseek 这篇文章就完全

270
00:08:16,000 --> 00:08:18,766
不同我是一个参数量又少

271
00:08:18,766 --> 00:08:21,466
但是模型又好的一个具体的效果

272
00:08:23,666 --> 00:08:26,233
了解完整个 introduction 之后

273
00:08:26,233 --> 00:08:28,966
我们看一下这篇文章的一个 contribution

274
00:08:29,000 --> 00:08:29,433
蛮有意思

275
00:08:29,433 --> 00:08:31,233
就是还是我们刚才讲到的

276
00:08:31,233 --> 00:08:32,300
一个架构的创新

277
00:08:32,333 --> 00:08:34,033
就提出了一个混合专家

278
00:08:34,033 --> 00:08:35,633
细腻度专家的问题

279
00:08:35,633 --> 00:08:38,700
然后做了一个非常多的一些消融实验

280
00:08:38,766 --> 00:08:41,800
还有实现了我们的一个训练的稳定性

281
00:08:41,866 --> 00:08:42,766
基本上

282
00:08:42,800 --> 00:08:44,000
我觉得最核心的

283
00:08:44,000 --> 00:08:45,666
还是网络模型的一个参数

284
00:08:45,666 --> 00:08:48,100
规模或者网络模型的架构

285
00:08:48,833 --> 00:08:50,433
刚才了解完一切之后

286
00:08:50,433 --> 00:08:51,300
我们 ZOMI

287
00:08:51,333 --> 00:08:53,633
还是希望跟大家一起去聊一聊

288
00:08:53,633 --> 00:08:54,666
最核心的内容

289
00:08:54,800 --> 00:08:56,200
现在我们来看一下

290
00:08:56,200 --> 00:08:58,066
整篇文章的第二部分

291
00:08:58,066 --> 00:08:59,066
准备工作

292
00:08:59,066 --> 00:09:00,066
MOE 专家

293
00:09:00,066 --> 00:09:02,133
在 transformar 架构里人的一个准备工作

294
00:09:02,166 --> 00:09:03,666
下面的这些公式

295
00:09:03,666 --> 00:09:06,466
说白了都是整个 self Attention 层

296
00:09:06,466 --> 00:09:08,466
里面的一个相关的计算

297
00:09:08,733 --> 00:09:08,933
当然了

298
00:09:08,933 --> 00:09:11,166
不同的公式或者不同的文章

299
00:09:11,166 --> 00:09:13,966
里面的计算的方法会不太一样

300
00:09:13,966 --> 00:09:15,666
不过怎么样都好

301
00:09:15,666 --> 00:09:16,700
我们基本上

302
00:09:16,733 --> 00:09:18,966
在整个 Transformer decorder 的架构

303
00:09:18,966 --> 00:09:21,033
也就是刚才讲到了 Glam

304
00:09:21,033 --> 00:09:21,866
这篇文章

305
00:09:22,233 --> 00:09:24,033
就是把我们的一个 transformer

306
00:09:24,033 --> 00:09:25,866
再加上 FFN

307
00:09:25,866 --> 00:09:27,300
把 FFN 里面的

308
00:09:27,433 --> 00:09:29,666
换成我们的一个具体的 Expert

309
00:09:29,866 --> 00:09:31,666
然后通过一个 TOP-K 的选择

310
00:09:31,666 --> 00:09:33,566
然后再给到我们 softmas 的计算

311
00:09:33,633 --> 00:09:35,300
所以说整个网络模型

312
00:09:35,333 --> 00:09:38,200
就基本上没有太多的一个新的地方

313
00:09:38,200 --> 00:09:40,333
如果大家想了解或深度看一下

314
00:09:40,333 --> 00:09:42,933
这个网络模型到底长什么样子

315
00:09:42,933 --> 00:09:45,233
我们可以去到 ZOMI 的下一个视频

316
00:09:45,233 --> 00:09:47,733
跟大家重点详细的展开的

317
00:09:47,766 --> 00:09:51,000
现在我们回到整篇文章里面

318
00:09:51,166 --> 00:09:52,033
整篇文章

319
00:09:52,033 --> 00:09:54,133
最核心的就是第三个内容了

320
00:09:54,166 --> 00:09:55,933
当然我们不要一个个的过

321
00:09:55,933 --> 00:09:57,733
或者遇到一个图就去看一个图

322
00:09:57,733 --> 00:09:58,466
更多的 ZOMI

323
00:09:58,466 --> 00:09:59,366
看文章的时候

324
00:09:59,400 --> 00:10:02,433
就会去看一下他对应的一些顺序

325
00:10:02,433 --> 00:10:03,566
看到对应的顺序

326
00:10:03,600 --> 00:10:05,133
引用到这个图的时候

327
00:10:05,133 --> 00:10:06,266
我去看这个图

328
00:10:06,266 --> 00:10:07,666
这样会会比较好

329
00:10:07,666 --> 00:10:10,133
Deepseek MOE 这里面就提到了

330
00:10:10,166 --> 00:10:11,766
其实你可以看到个 section 2

331
00:10:11,766 --> 00:10:12,800
其实有很多问题

332
00:10:12,800 --> 00:10:14,233
我们针对这个 section 2

333
00:10:14,233 --> 00:10:17,033
就是我们的普通的 MOE 加 decode

334
00:10:17,033 --> 00:10:18,533
这种 transformar 的类型

335
00:10:18,566 --> 00:10:20,633
我们提出了两个东西

336
00:10:20,633 --> 00:10:21,766
第一个我们刚才讲到的

337
00:10:21,800 --> 00:10:23,733
一个细腻度的专家的分割

338
00:10:23,733 --> 00:10:24,133
第二个

339
00:10:24,133 --> 00:10:25,833
就是共享专家

340
00:10:25,833 --> 00:10:27,733
细腻度的专家的分割

341
00:10:27,766 --> 00:10:30,566
就是把我们之前的一个 FFN 层

342
00:10:30,566 --> 00:10:33,033
把 FFN 的一个参数量减半

343
00:10:33,033 --> 00:10:35,333
然后把我们的一个网络模型

344
00:10:35,366 --> 00:10:37,333
就是专家的数量增加

345
00:10:37,333 --> 00:10:38,366
增加一倍

346
00:10:38,366 --> 00:10:39,033
所以这里面

347
00:10:39,033 --> 00:10:42,466
就多了一个 MNMK 相关的内容了

348
00:10:42,466 --> 00:10:44,433
就是我们的专家的数量

349
00:10:44,566 --> 00:10:45,600
变多了

350
00:10:45,933 --> 00:10:48,566
第二个就是所谓的共享专家

351
00:10:48,566 --> 00:10:50,733
这面就有一大段话

352
00:10:50,800 --> 00:10:51,266
这段话

353
00:10:51,266 --> 00:10:53,333
还是很值得大家一起去读一读的

354
00:10:53,366 --> 00:10:55,766
ZOMI 就简单的做一个翻译

355
00:10:55,766 --> 00:10:57,566
就使用传统的路由的时候

356
00:10:57,566 --> 00:10:59,966
因为我们分配给不同的专家

357
00:10:59,966 --> 00:11:01,466
会有不同的 TOKEN

358
00:11:01,466 --> 00:11:04,066
这些 TOKEN 都带着一些不同的知识

359
00:11:04,066 --> 00:11:05,500
最终训练起来的时候

360
00:11:05,533 --> 00:11:06,166
你就会发现

361
00:11:06,166 --> 00:11:07,000
多个专家

362
00:11:07,000 --> 00:11:10,000
可能会集中的获取一些具体的知识

363
00:11:10,000 --> 00:11:12,033
导致专家之间的参数

364
00:11:12,033 --> 00:11:13,333
就有些冗余了

365
00:11:13,366 --> 00:11:14,400
有些专家学的好

366
00:11:14,400 --> 00:11:15,666
有些专家学的不好

367
00:11:15,666 --> 00:11:17,166
为了解决这个问题

368
00:11:17,200 --> 00:11:19,533
就提出了一个共享专家的问题

369
00:11:19,566 --> 00:11:22,366
提出了就单独多了一个 FFN 出来

370
00:11:22,366 --> 00:11:24,600
可以看到这里面 ZOMI 就框出来了

371
00:11:24,600 --> 00:11:26,000
在整个路由的选择

372
00:11:26,000 --> 00:11:27,066
TOP-K 的选择

373
00:11:27,133 --> 00:11:29,800
这个 SK 就独立的加一

374
00:11:29,800 --> 00:11:31,600
然后去减去我们额外的

375
00:11:31,600 --> 00:11:33,333
这个多出来的共享的专家

376
00:11:33,333 --> 00:11:34,600
再给我们的 Softmax

377
00:11:34,666 --> 00:11:35,466
基本上

378
00:11:35,466 --> 00:11:37,733
数学公式可能相对来说

379
00:11:37,766 --> 00:11:39,433
如果认真看比较好理解

380
00:11:39,433 --> 00:11:41,066
我们现在还是看看图

381
00:11:41,600 --> 00:11:42,033
以前

382
00:11:42,033 --> 00:11:44,066
我们会选择两个路由

383
00:11:44,066 --> 00:11:45,766
就 top-k，k 是 2

384
00:11:45,800 --> 00:11:47,400
所以我们基本上就选择

385
00:11:47,400 --> 00:11:49,000
第一个专家或者第 n 个专家

386
00:11:49,000 --> 00:11:50,000
选择两个专家

387
00:11:50,000 --> 00:11:51,166
然后进行 concat

388
00:11:51,333 --> 00:11:53,400
这倒丢给下一层的输入

389
00:11:53,433 --> 00:11:55,300
然后发现这个效果不好

390
00:11:55,333 --> 00:11:56,466
之后 MOE

391
00:11:56,466 --> 00:11:58,733
就整个 Deepseek 的 MOE 了

392
00:11:58,766 --> 00:12:02,466
DS 我们先来假称 DS M O E

393
00:12:02,466 --> 00:12:03,233
这篇文章

394
00:12:03,233 --> 00:12:06,533
就提出了我们下面的第二个内容了

395
00:12:06,566 --> 00:12:07,766
就是 Fine-Grained 

396
00:12:07,966 --> 00:12:09,733
也就是细腻度的优化

397
00:12:09,766 --> 00:12:11,133
所以细腻度的优化

398
00:12:11,133 --> 00:12:12,166
以前我们左边的这

399
00:12:12,166 --> 00:12:13,533
个 N 我们现在

400
00:12:13,533 --> 00:12:16,466
把专家的数量拓宽成为 2N

401
00:12:16,533 --> 00:12:18,833
也就是专家的一个数量更多了

402
00:12:18,833 --> 00:12:22,500
但是每一个专家 123456789

403
00:12:22,533 --> 00:12:25,066
这些专家的参数量就更小了

404
00:12:25,066 --> 00:12:25,833
所以可以看到

405
00:12:25,833 --> 00:12:27,033
他的一个宽度

406
00:12:27,033 --> 00:12:28,333
没有之前宽了

407
00:12:28,366 --> 00:12:29,966
所以说我们的专家的数量

408
00:12:29,966 --> 00:12:33,266
或者专家的容量就变得更少了

409
00:12:33,333 --> 00:12:34,800
这样

410
00:12:34,800 --> 00:12:35,800
还不解决问题

411
00:12:35,800 --> 00:12:36,233
于是

412
00:12:36,233 --> 00:12:39,033
又提出了一个新的一个共享专家

413
00:12:39,033 --> 00:12:41,466
单独的有一个共享专家一出来

414
00:12:41,466 --> 00:12:43,833
一这个专家是每一次必选的

415
00:12:43,833 --> 00:12:46,666
这个专家是学习所有的共同的参数

416
00:12:46,666 --> 00:12:48,866
因此假设我们以前是 TOP-K

417
00:12:49,266 --> 00:12:50,366
top k，k 等于 2

418
00:12:50,400 --> 00:12:50,933
现在

419
00:12:50,933 --> 00:12:53,133
k 就变成必须等于 3

420
00:12:53,133 --> 00:12:55,433
因为我们多了一个绿色的共享专家

421
00:12:55,433 --> 00:12:56,500
通过这种方式

422
00:12:56,533 --> 00:12:59,033
来去实现整个 MOE 的架构的

423
00:12:59,033 --> 00:13:00,500
一个性能的稳定性

424
00:13:00,533 --> 00:13:03,333
和学习到更多的冗余的知识

425
00:13:03,333 --> 00:13:05,066
所以说这篇文章就对应了

426
00:13:05,066 --> 00:13:06,633
隔离的一个共享专家出来

427
00:13:06,633 --> 00:13:09,900
多了一个 FFN 层单独做累积的

428
00:13:10,433 --> 00:13:11,366
后面的内容

429
00:13:11,400 --> 00:13:13,766
就是我们的一个负载均衡的问题

430
00:13:13,766 --> 00:13:16,566
怎么去做我们的负载均衡的一个选择

431
00:13:16,566 --> 00:13:19,200
还有一个 balance lost 的一个计算

432
00:13:19,666 --> 00:13:21,666
下面的内容还是非常重要的

433
00:13:21,666 --> 00:13:22,666
就是我们选择

434
00:13:22,666 --> 00:13:24,666
或者我们在真正做计算的时候

435
00:13:24,666 --> 00:13:26,300
怎么去做均衡复杂

436
00:13:26,333 --> 00:13:28,866
因为我们现在多了一个混合专家出来

437
00:13:28,866 --> 00:13:29,733
嘛这里面

438
00:13:29,766 --> 00:13:31,133
更多是数学的问题

439
00:13:31,133 --> 00:13:32,833
就是我们的专家

440
00:13:32,833 --> 00:13:33,900
怎么做均衡复杂

441
00:13:33,933 --> 00:13:36,000
又提出了一条新的公式

442
00:13:36,000 --> 00:13:38,400
还有我们的一个 device 的

443
00:13:38,400 --> 00:13:39,633
一个 level 的专家

444
00:13:39,633 --> 00:13:40,833
怎么做均衡复查

445
00:13:40,833 --> 00:13:42,166
所谓的 device 就

446
00:13:42,200 --> 00:13:44,200
是因为我们在训练的过程当中

447
00:13:44,200 --> 00:13:46,633
会把不同的专家分配到不同的卡

448
00:13:46,633 --> 00:13:48,233
所以我们下面就说到了

449
00:13:48,233 --> 00:13:49,633
我们设置了一个小小的

450
00:13:49,633 --> 00:13:51,166
专家的平衡因子

451
00:13:51,200 --> 00:13:53,000
去平衡我们的一个

452
00:13:53,000 --> 00:13:55,566
或者降低整个路由变坏的一个风险的

453
00:13:55,566 --> 00:13:55,933
同时

454
00:13:55,933 --> 00:13:57,666
据设置一个比较大的一个设备数额

455
00:13:57,666 --> 00:13:58,766
来去平衡我们的专家

456
00:13:58,800 --> 00:13:59,933
均衡负载的内容了

457
00:13:59,933 --> 00:14:01,033
基本上你会发现

458
00:14:01,033 --> 00:14:03,466
这条公式还是相对比较简单的

459
00:14:03,466 --> 00:14:06,133
计算起来其实加了更多这种求和呀

460
00:14:06,166 --> 00:14:07,933
还有我们些超参

461
00:14:08,233 --> 00:14:10,100
接着就是实验部分的

462
00:14:10,133 --> 00:14:12,600
实验就部分就不跟大家去展开了

463
00:14:12,600 --> 00:14:13,966
基本上你可以发现

464
00:14:13,966 --> 00:14:15,733
反正每篇文章

465
00:14:15,733 --> 00:14:17,066
都做了大量的矩阵

466
00:14:17,066 --> 00:14:19,633
在不同的一个网络模型

467
00:14:19,966 --> 00:14:20,933
然后进行对比

468
00:14:20,933 --> 00:14:22,533
这个是网络模型的参数

469
00:14:22,533 --> 00:14:25,033
然后在不同的一个验证集上面

470
00:14:25,033 --> 00:14:26,533
就说明了我的 Deepseek

471
00:14:26,566 --> 00:14:28,600
Moe 的效果是非常的好的

472
00:14:28,600 --> 00:14:31,333
还有一些其他额外的实验

473
00:14:31,333 --> 00:14:31,600
当然

474
00:14:31,600 --> 00:14:34,166
里面还有很多的图，相关的内容

475
00:14:34,166 --> 00:14:35,733
包括我们的训练稳定性

476
00:14:35,733 --> 00:14:38,033
还有我们的 Moe 专家加上之后

477
00:14:38,033 --> 00:14:40,766
不同的参数规模所实现的内容

478
00:14:41,166 --> 00:14:42,333
今天的内容

479
00:14:42,333 --> 00:14:45,000
Moe 的混合专家就这么多了

480
00:14:45,000 --> 00:14:46,800
我们简单的回顾一下

481
00:14:46,800 --> 00:14:48,033
整个第四个 Moe

482
00:14:48,033 --> 00:14:48,933
主要是两个内容

483
00:14:48,966 --> 00:14:51,066
第一个就是我们的变成小专家

484
00:14:51,066 --> 00:14:51,533
第二个

485
00:14:51,566 --> 00:14:54,466
变成我们的专有专家这么一个内容了

486
00:14:54,466 --> 00:14:56,066
今天的内容先到这里为止

487
00:14:56,066 --> 00:14:56,666
谢谢各位

488
00:14:56,666 --> 00:14:57,500
拜了个拜

