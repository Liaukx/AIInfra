1
00:00:00,000 --> 00:00:02,033
内容/录制/字幕:Z0MI 酱，视频剪辑:梁嘉铭

2
00:00:02,200 --> 00:00:03,966
大家好我是 ZOMI

3
00:00:03,966 --> 00:00:07,06=6
现在来到了 MoE 的混合专家里面

4
00:00:07,066 --> 00:00:08,966
整个系列的前世今生

5
00:00:09,033 --> 00:00:09,966
也就是了解一下

6
00:00:10,000 --> 00:00:12,633
MoE 架构的一个相关的历史的内容

7
00:00:12,766 --> 00:00:14,600
今天的视频可能会有点长

8
00:00:14,600 --> 00:00:15,466
那接下来

9
00:00:15,466 --> 00:00:16,100
如果太长

10
00:00:16,133 --> 00:00:19,033
就会分开两个视频跟大家去分享

11
00:00:23,533 --> 00:00:25,200
那首先看一下这一期视频

12
00:00:25,200 --> 00:00:26,433
或者今天的视频

13
00:00:26,433 --> 00:00:28,133
跟大家分享哪些事情

14
00:00:28,200 --> 00:00:29,533
第一个看一下综述

15
00:00:29,533 --> 00:00:31,800
就是 MoE 架构的整体的总览

16
00:00:31,800 --> 00:00:34,266
去了解一下 MoE 架构的历史发展

17
00:00:34,266 --> 00:00:35,333
到底是怎么样

18
00:00:35,733 --> 00:00:38,466
接着就看一下历史所发生

19
00:00:38,466 --> 00:00:40,100
MoE 的重要的事件

20
00:00:40,133 --> 00:00:41,933
就从第二到第五

21
00:00:41,933 --> 00:00:43,466
首先就是奠基的工作

22
00:00:43,466 --> 00:00:46,133
90 年代初期 MoE 相关的文章

23
00:00:46,166 --> 00:00:48,633
接着可能到了架构的形成

24
00:00:48,633 --> 00:00:50,633
RNN 时代里面

25
00:00:50,633 --> 00:00:53,533
MoE 跟 RNN 有哪些相关的合作

26
00:00:53,566 --> 00:00:55,800
那接着就是提升效果

27
00:00:55,800 --> 00:00:56,966
在 Transformer 的时代

28
00:00:56,966 --> 00:00:58,466
就 Transformer 来临的时候

29
00:00:58,566 --> 00:01:02,166
MoE 跟 Transformer 的 encord decoder 怎么去结合

30
00:01:02,166 --> 00:01:03,800
到了最近的智能涌现

31
00:01:03,800 --> 00:01:04,833
有了大模型时代

32
00:01:04,833 --> 00:01:08,166
看一下 GPT 类的 Transformer 架构

33
00:01:08,200 --> 00:01:10,833
跟 MoE 怎么做更好的结合

34
00:01:10,866 --> 00:01:12,533
最后来一个可视化

35
00:01:12,566 --> 00:01:15,466
了解一下 mixtral 7x8B 为例子

36
00:01:15,533 --> 00:01:17,866
MoE 架构里面到底学什么

37
00:01:17,866 --> 00:01:19,166
怎么去运作

38
00:01:21,600 --> 00:01:22,133
今天的内容

39
00:01:22,133 --> 00:01:24,966
在整一个 MoE 混合专家的系列里面

40
00:01:24,966 --> 00:01:27,000
来到了第二个部分

41
00:01:27,133 --> 00:01:28,200
MoE 的前世今生

42
00:01:28,200 --> 00:01:30,566
也就是 MoE 架构的相关的历史

43
00:01:32,433 --> 00:01:34,033
马上来到了第一个内容

44
00:01:34,033 --> 00:01:34,700
更重要的是

45
00:01:34,733 --> 00:01:37,466
看一个 summery of MoE 的相关的文章

46
00:01:37,466 --> 00:01:39,433
那现在看一下整体的历史

47
00:01:39,433 --> 00:01:40,333
对 MoE 架构

48
00:01:40,366 --> 00:01:41,266
最重要的文献

49
00:01:41,266 --> 00:01:42,833
其实在上一期视频

50
00:01:42,833 --> 00:01:44,366
跟大家简单的讲过

51
00:01:44,400 --> 00:01:47,033
从 1991 年到了 2024 年

52
00:01:47,033 --> 00:01:48,533
整个 MoE 架构

53
00:01:48,566 --> 00:01:50,433
经历了好几轮的迭代

54
00:01:50,433 --> 00:01:52,533
也就是对应的目录

55
00:01:52,566 --> 00:01:53,766
从奠基的工作

56
00:01:53,766 --> 00:01:55,866
到现在的智能涌现的大模型

57
00:01:55,866 --> 00:01:57,166
发展的非常的多

58
00:01:57,200 --> 00:01:59,166
已经经过了三次到四次

59
00:01:59,166 --> 00:02:01,333
一个重要的技术的改变

60
00:02:01,333 --> 00:02:03,166
和相关的架构的演进

61
00:02:04,400 --> 00:02:05,933
那现在来看一下

62
00:02:05,933 --> 00:02:08,966
整体的近期发布的 MoE 大模型

63
00:02:08,966 --> 00:02:10,600
从 2023 年

64
00:02:10,733 --> 00:02:11,433
大家都说

65
00:02:11,433 --> 00:02:14,433
GP4 可能是一个万亿规模的大模型

66
00:02:14,433 --> 00:02:16,266
或者万亿规模的 MoE 以后

67
00:02:16,266 --> 00:02:18,900
整个 MoE 的加速度是越来越快

68
00:02:18,933 --> 00:02:20,000
到了现在

69
00:02:20,000 --> 00:02:22,400
最近也就是 2025 年的 1 月份

70
00:02:22,400 --> 00:02:24,533
你可以看到 DeepSeek 的 V3

71
00:02:24,533 --> 00:02:26,133
那 DeepSeek 不是第一个版本

72
00:02:26,133 --> 00:02:27,733
从 V1 V2 V3

73
00:02:27,800 --> 00:02:31,633
到现在的一个最近的 Qwen 2.5 的 Max 开始

74
00:02:31,833 --> 00:02:32,966
整体的 MoE

75
00:02:33,000 --> 00:02:35,800
或者整体的大模型用 MoE 的架构

76
00:02:35,800 --> 00:02:37,066
就越来越多

77
00:02:37,866 --> 00:02:39,233
所以今天重点

78
00:02:39,233 --> 00:02:43,133
看一下 survey in MoE 或者 survey on MoE

79
00:02:43,233 --> 00:02:43,866
那蛮有意思

80
00:02:43,866 --> 00:02:45,366
就是很重要的事件

81
00:02:45,400 --> 00:02:47,600
就是从 2017 年开始

82
00:02:47,833 --> 00:02:49,433
现在整体的 MoE

83
00:02:49,433 --> 00:02:51,866
在多个领域都在不断的发展

84
00:02:51,866 --> 00:02:54,533
那这里面看到很多不同的颜色

85
00:02:54,566 --> 00:02:55,400
首先来说

86
00:02:55,400 --> 00:02:56,366
自然语言处理

87
00:02:56,366 --> 00:02:58,066
也就是经常讲到

88
00:02:58,066 --> 00:02:59,533
大模型的 MoE

89
00:02:59,566 --> 00:03:01,400
是现在的绿色

90
00:03:01,400 --> 00:03:03,000
当然了 MoE 这个架构

91
00:03:03,000 --> 00:03:06,766
不仅仅在 NLP LLM 领域在发展

92
00:03:06,800 --> 00:03:09,600
它在计算机视觉也在发展

93
00:03:09,600 --> 00:03:10,933
所以现在看到

94
00:03:10,933 --> 00:03:13,766
从 vMoEs wincMoE 到 softMoE

95
00:03:13,833 --> 00:03:15,533
这种黄色的框

96
00:03:15,566 --> 00:03:17,466
也是的一个 MoE

97
00:03:17,466 --> 00:03:19,866
当然除了视觉以外还有

98
00:03:19,866 --> 00:03:20,666
多模态

99
00:03:20,766 --> 00:03:23,866
那粉色的就是的一些多模态

100
00:03:23,866 --> 00:03:25,033
MoE 的出现

101
00:03:25,033 --> 00:03:27,700
当然了大模型除了 NLP 视觉

102
00:03:27,733 --> 00:03:28,266
还有多模态

103
00:03:28,266 --> 00:03:31,266
有推荐系统和推荐领域相关

104
00:03:31,400 --> 00:03:32,433
推荐的就是

105
00:03:32,433 --> 00:03:34,066
对应的蓝色推荐

106
00:03:34,066 --> 00:03:36,133
也引入了 MoE 的架构

107
00:03:36,166 --> 00:03:38,266
所以可以看到 MoE 的架构

108
00:03:38,266 --> 00:03:39,666
越演越多

109
00:03:39,666 --> 00:03:40,633
越来越复杂

110
00:03:40,633 --> 00:03:42,266
现在来了解一下

111
00:03:42,266 --> 00:03:44,500
MoE 的整体的一个具体的概述

112
00:03:44,566 --> 00:03:45,233
那首先

113
00:03:45,233 --> 00:03:46,533
每一个 MoE 层

114
00:03:46,800 --> 00:03:48,733
以这个图为例子

115
00:03:48,833 --> 00:03:51,633
每一个这个是一个主要的 MoE 一层

116
00:03:51,633 --> 00:03:54,133
主要有一组或者很多个专家

117
00:03:54,166 --> 00:03:57,466
那这里面的 FFN 就是对应的专家

118
00:03:57,466 --> 00:04:00,433
所谓的 MoE mixture of Expert

119
00:04:00,600 --> 00:04:03,000
每一个 FFN 对应的是一个 Expert

120
00:04:03,000 --> 00:04:04,366
对应的是一个专家

121
00:04:04,366 --> 00:04:05,966
另外的话还有一个 gate

122
00:04:05,966 --> 00:04:07,766
也就是所谓的门控网络

123
00:04:07,766 --> 00:04:09,233
G 来去组成

124
00:04:09,233 --> 00:04:10,133
所以 MoE 里面

125
00:04:10,166 --> 00:04:12,600
最核心的就是第一个的专家

126
00:04:12,600 --> 00:04:15,133
第二个的门控两个内容

127
00:04:15,133 --> 00:04:16,166
那门控网络

128
00:04:16,166 --> 00:04:17,200
通常会采用

129
00:04:17,200 --> 00:04:18,800
softmax 相关的一个函数

130
00:04:18,800 --> 00:04:19,766
来去实现

131
00:04:19,833 --> 00:04:20,900
那最重要的作用

132
00:04:20,933 --> 00:04:22,733
就是将输入的 TOKEN

133
00:04:22,733 --> 00:04:25,666
因为输入的 x1 或者 x2

134
00:04:25,666 --> 00:04:28,900
引导到合适的专家里面去计算

135
00:04:28,933 --> 00:04:29,466
那所以说

136
00:04:29,466 --> 00:04:32,933
现在来看到 MoE 两个核心的内容

137
00:04:33,133 --> 00:04:35,333
后面会详细的去展开

138
00:04:35,866 --> 00:04:37,333
哎 ZOMI 老师你好

139
00:04:37,466 --> 00:04:39,866
为什么在 Transformer 架构

140
00:04:40,200 --> 00:04:42,066
MoE 层的位置选择

141
00:04:42,066 --> 00:04:45,466
一般都在 Transformer 架构的一个 FFN 层

142
00:04:45,466 --> 00:04:46,633
进行替换

143
00:04:47,233 --> 00:04:49,766
哎小新问的这个问题蛮有意思的说

144
00:04:49,800 --> 00:04:52,266
为什么在 Transformer 架构里面

145
00:04:52,266 --> 00:04:54,700
不去代替一个 self attention

146
00:04:54,733 --> 00:04:57,000
反而是代替 FFN 层

147
00:04:57,200 --> 00:04:58,466
把 FFN 层换

148
00:04:58,466 --> 00:04:59,633
成很多个专家

149
00:04:59,633 --> 00:05:01,666
因为随着模型的扩大

150
00:05:01,666 --> 00:05:03,433
也就是现在所谓的大模型

151
00:05:03,566 --> 00:05:04,866
FFN 层的计算量

152
00:05:04,866 --> 00:05:06,533
在整个 Transformer 架构里面

153
00:05:06,566 --> 00:05:07,800
是越来越大

154
00:05:07,800 --> 00:05:09,266
所以代替 FFN 层

155
00:05:09,266 --> 00:05:11,233
能够去节省的算力

156
00:05:11,233 --> 00:05:13,233
从而提高模型的一个性能

157
00:05:13,333 --> 00:05:16,866
这个就是为什么大家会选择在 FFN 层

158
00:05:16,933 --> 00:05:17,733
去做一个 MoE

159
00:05:17,733 --> 00:05:19,200
架构的替换

160
00:05:20,200 --> 00:05:22,000
那回到的 MoE 架构里面

161
00:05:22,000 --> 00:05:23,966
实际上 MoE 架构里面

162
00:05:23,966 --> 00:05:25,166
虽然 MoE

163
00:05:25,166 --> 00:05:27,233
对比起的一个 LLM 

164
00:05:27,233 --> 00:05:27,966
里面的稠密

165
00:05:28,000 --> 00:05:28,766
LLM 

166
00:05:28,766 --> 00:05:29,966
它属于一个稀疏

167
00:05:30,266 --> 00:05:32,500
但是在 MoE 架构里面

168
00:05:32,533 --> 00:05:35,600
它自己还要分稀疏和稠密

169
00:05:35,600 --> 00:05:38,400
那重点看一下下面的这两个图

170
00:05:38,466 --> 00:05:40,066
所谓的稠密的 MoE

171
00:05:40,066 --> 00:05:41,933
就对于输入的 x

172
00:05:41,966 --> 00:05:43,133
输入的 TOKEN

173
00:05:43,133 --> 00:05:45,933
通过一个 gate 网络及门控网络

174
00:05:45,933 --> 00:05:47,866
来选择所有的专家

175
00:05:47,866 --> 00:05:50,566
每一个专家都有可能去激活

176
00:05:50,866 --> 00:05:53,500
那另外的话还有一个稀疏的 MoE

177
00:05:53,633 --> 00:05:55,500
也就是输的 TOKEN 之后

178
00:05:55,600 --> 00:05:58,266
选择 top K 专家来去执行

179
00:05:58,266 --> 00:05:59,633
有条件的去执行

180
00:05:59,633 --> 00:06:01,500
有部分专家是不执行

181
00:06:01,533 --> 00:06:02,000
这种

182
00:06:02,000 --> 00:06:05,933
就是稠密的 MoE 跟稀疏的 MoE 区别

183
00:06:05,933 --> 00:06:09,200
那至少现在来看到稠密的混合专家

184
00:06:09,200 --> 00:06:11,766
也就是左边的这个图里面

185
00:06:11,800 --> 00:06:12,633
所用到

186
00:06:12,633 --> 00:06:14,433
架构其实现在的应用

187
00:06:14,433 --> 00:06:15,766
非常的广泛

188
00:06:15,800 --> 00:06:17,166
基本上通过一个门控网络

189
00:06:17,166 --> 00:06:19,566
来来去选择对应的专家

190
00:06:19,566 --> 00:06:21,133
每个专家都会激活值

191
00:06:21,133 --> 00:06:24,333
激活的量或者激活的份额比不一样

192
00:06:24,600 --> 00:06:25,833
那现在来看一下

193
00:06:25,833 --> 00:06:28,633
刚才讲到了 MoE 很重要的两个点

194
00:06:28,733 --> 00:06:29,966
第一个点就是

195
00:06:29,966 --> 00:06:31,166
专家的数量

196
00:06:31,166 --> 00:06:33,366
第二就是的门控网络

197
00:06:33,366 --> 00:06:34,633
那这里面的门控网络

198
00:06:34,633 --> 00:06:36,766
重点就得是最核心

199
00:06:36,800 --> 00:06:37,566
最重要的作用

200
00:06:37,566 --> 00:06:40,433
就是协调专家的每一个 FFN 层

201
00:06:40,800 --> 00:06:41,566
参与多少计算

202
00:06:41,566 --> 00:06:42,566
怎么去参与计算

203
00:06:42,566 --> 00:06:45,433
还有它对应的各自的输出的组合

204
00:06:45,566 --> 00:06:47,633
那根据每一个输入的 TOKEN

205
00:06:47,633 --> 00:06:48,733
一个处理的方式

206
00:06:48,766 --> 00:06:50,800
不同整个门控的机制

207
00:06:50,800 --> 00:06:52,833
分开三种不同的类型

208
00:06:52,833 --> 00:06:54,466
第一种就是稀疏的门控

209
00:06:54,466 --> 00:06:55,300
所以稀疏门控

210
00:06:55,333 --> 00:06:58,000
就是我只激活部分的专家

211
00:06:58,000 --> 00:06:58,633
所以说

212
00:06:58,633 --> 00:07:00,966
大家说那个 MoE 是一个稀疏的架构

213
00:07:01,000 --> 00:07:02,033
这个稀疏

214
00:07:02,266 --> 00:07:04,100
稀疏在这里不是稀疏

215
00:07:04,133 --> 00:07:05,033
在的计算上

216
00:07:05,033 --> 00:07:07,700
稀疏的只是激活部分的专家

217
00:07:08,166 --> 00:07:09,866
那还有一个稠密的门控

218
00:07:09,866 --> 00:07:10,866
激活所有的专家

219
00:07:10,866 --> 00:07:12,733
对应的就是稠密的 MoE 架构

220
00:07:12,800 --> 00:07:14,666
另外它可能还会有一些比较神奇

221
00:07:14,666 --> 00:07:16,233
就是软控门

222
00:07:16,233 --> 00:07:18,066
对输入 Token 进行合并

223
00:07:18,066 --> 00:07:19,766
还有专家进行合并

224
00:07:19,800 --> 00:07:20,833
那合并之后

225
00:07:20,833 --> 00:07:22,133
中间的这种合并

226
00:07:22,166 --> 00:07:24,266
相关的方式要做到可微

227
00:07:24,533 --> 00:07:25,866
至少在 ZOMI 的眼中

228
00:07:25,866 --> 00:07:26,633
稀疏门控

229
00:07:26,633 --> 00:07:29,166
可能是现在更多的大模型厂商

230
00:07:29,233 --> 00:07:31,333
或者更多的基础大模型

231
00:07:31,366 --> 00:07:32,766
所采用的方案

232
00:07:33,633 --> 00:07:34,500
那现在来看一下

233
00:07:34,533 --> 00:07:37,566
其实在整个 MoE 架构里面有非常多种

234
00:07:37,566 --> 00:07:40,433
一个 TOP1 的一个门控的方式

235
00:07:40,533 --> 00:07:42,933
那有一些 TOP1 TOP2 的相关的门控

236
00:07:42,933 --> 00:07:43,766
还有一些 base 层

237
00:07:43,766 --> 00:07:45,133
还有一些分组域映射

238
00:07:45,133 --> 00:07:47,133
和随机门的控制组合

239
00:07:47,133 --> 00:07:49,366
不过大家不用去操心太多

240
00:07:49,366 --> 00:07:51,566
到底有哪些不同的种类

241
00:07:51,566 --> 00:07:53,533
将会在后面重点

242
00:07:53,533 --> 00:07:54,466
去解读一下

243
00:07:54,466 --> 00:07:58,133
最新的大模型用的是哪些相关的架构

244
00:07:58,633 --> 00:07:59,133
那现

245
00:07:59,166 --> 00:08:01,800
在还是了解一个基础的概念

246
00:08:01,800 --> 00:08:02,166
首先

247
00:08:02,166 --> 00:08:06,033
的 MoE 的这种类型的大模型

248
00:08:06,033 --> 00:08:06,933
模型的大小

249
00:08:06,966 --> 00:08:09,433
不仅仅只看一个模型的规模

250
00:08:09,433 --> 00:08:11,166
还要看另外一个数据

251
00:08:11,200 --> 00:08:14,033
激活跟总参数量的一个对比

252
00:08:14,433 --> 00:08:16,700
这里面举一个具体的例子

253
00:08:16,733 --> 00:08:18,266
也就是 DeepSeek MoE

254
00:08:18,266 --> 00:08:19,700
最近的 DeepSeek 非常火

255
00:08:19,733 --> 00:08:22,033
除了关心 DeepSeek MoE

256
00:08:22,033 --> 00:08:23,533
就是 DeepSeek 这种大模型

257
00:08:23,566 --> 00:08:26,166
它的整体的模型的规模大小以外

258
00:08:26,166 --> 00:08:27,000
还得关注

259
00:08:27,000 --> 00:08:29,400
它的一个激活的专家的数量

260
00:08:29,400 --> 00:08:31,200
还有总专家的数量

261
00:08:31,400 --> 00:08:32,166
也就两个

262
00:08:32,166 --> 00:08:34,633
合起来才能够代表这个模型的大小

263
00:08:34,633 --> 00:08:38,233
规模不能只看一个就是 600B 多少

264
00:08:38,233 --> 00:08:40,066
就例如 DeepSeek 最新的 V3

265
00:08:40,066 --> 00:08:42,300
671B 你是不合理

266
00:08:42,333 --> 00:08:44,200
它一共有多少个激活的专家

267
00:08:44,200 --> 00:08:46,800
然后整体的每个激活数是多少

268
00:08:46,800 --> 00:08:47,733
这个反而是

269
00:08:47,733 --> 00:08:50,033
可能是在真正的 MoE 大模型里面

270
00:08:50,033 --> 00:08:51,566
关心的参数量

271
00:08:51,966 --> 00:08:53,833
那接着看一下整体

272
00:08:53,833 --> 00:08:55,966
因为这里面的对比非常的多

273
00:08:56,000 --> 00:08:57,466
在这篇 Survey 里面

274
00:08:57,566 --> 00:08:57,966
那这里

275
00:08:57,966 --> 00:08:59,600
还是非常欢迎大家去看一下

276
00:08:59,600 --> 00:09:00,466
这篇文章

277
00:09:00,466 --> 00:09:02,533
那至于这篇文章的中英文对照中

278
00:09:02,566 --> 00:09:03,966
已经分享在夸克网盘

279
00:09:03,966 --> 00:09:05,200
放在视频的最后

280
00:09:05,200 --> 00:09:06,033
链接地址

281
00:09:06,433 --> 00:09:07,666
那回到 PPT 里面

282
00:09:07,666 --> 00:09:08,933
整个 MoE 的参数里面

283
00:09:08,966 --> 00:09:09,533
蛮有意思

284
00:09:09,533 --> 00:09:11,233
就是现在看一个大模型

285
00:09:11,233 --> 00:09:12,366
看到有多少参数量

286
00:09:12,400 --> 00:09:13,933
除了刚才讲到

287
00:09:13,933 --> 00:09:15,733
很关心的一个是 models 的参数量

288
00:09:15,733 --> 00:09:17,166
还有激活的数量以外

289
00:09:17,166 --> 00:09:20,266
后面还有一连串的相关的数字

290
00:09:20,266 --> 00:09:22,466
那这些数字一层一层的来看一看

291
00:09:22,533 --> 00:09:24,366
首先这个 dmodels

292
00:09:24,366 --> 00:09:27,366
就是指的隐藏层的大小

293
00:09:27,366 --> 00:09:30,433
dFFN 就是指 FFN 中间层的大小

294
00:09:30,533 --> 00:09:34,833
DExpert 就是指中间的一个专家层的大小

295
00:09:35,033 --> 00:09:38,266
#l 就是代表有多少层网络模型

296
00:09:38,266 --> 00:09:39,566
多少层 Transformer 层

297
00:09:39,833 --> 00:09:42,433
#h 就是指的 head

298
00:09:42,633 --> 00:09:45,266
注意力头的一个具体的数量

299
00:09:45,366 --> 00:09:46,166
dhead

300
00:09:46,166 --> 00:09:48,533
就是指的一个注意力头

301
00:09:48,533 --> 00:09:50,766
一个维度大小

302
00:09:51,866 --> 00:09:53,533
了解完 survey

303
00:09:53,566 --> 00:09:54,400
现在来看一下

304
00:09:54,400 --> 00:09:56,200
下面的一些相关的工作

305
00:09:56,200 --> 00:09:58,533
就是的历史很核心的文章

306
00:09:58,533 --> 00:10:00,133
跟大家简单的串烧一下

307
00:10:00,133 --> 00:10:01,733
在下一个视频里面

308
00:10:01,733 --> 00:10:03,433
可能针对重点的文章

309
00:10:03,433 --> 00:10:04,633
来进行一个解读

310
00:10:04,933 --> 00:10:06,466
首先看一下 90 年代

311
00:10:06,566 --> 00:10:08,733
初期一个奠基工作

312
00:10:08,733 --> 00:10:10,266
那最核心的 90 年代

313
00:10:10,266 --> 00:10:11,900
Hitten 跟 Jordan

314
00:10:11,933 --> 00:10:13,566
就发布了一篇文章

315
00:10:13,566 --> 00:10:16,800
叫做 attached Mixtral of local Expert

316
00:10:16,800 --> 00:10:18,966
它是最早的一个 MoE 架构

317
00:10:18,966 --> 00:10:19,966
最核心的思想

318
00:10:19,966 --> 00:10:22,766
就通过多个独立的专家去处理

319
00:10:22,766 --> 00:10:24,466
输入不同的数据集

320
00:10:24,466 --> 00:10:26,666
并且由的一个提出了 gating

321
00:10:26,666 --> 00:10:27,966
也就门控的网络

322
00:10:28,033 --> 00:10:29,833
动态的去选择专家

323
00:10:29,833 --> 00:10:31,966
现在的 MoE 架构

324
00:10:32,000 --> 00:10:34,533
最原始的出行就来自于最里面

325
00:10:34,733 --> 00:10:35,400
整体最核心

326
00:10:35,400 --> 00:10:38,766
就是奠基了整个 MoE 的基础

327
00:10:38,766 --> 00:10:39,633
将监督学习

328
00:10:39,633 --> 00:10:41,900
跟分而自治的思想进行一个结合

329
00:10:41,933 --> 00:10:44,000
现在大部分 MoE 的相关的论文

330
00:10:44,000 --> 00:10:45,966
都引用这一篇文章

331
00:10:45,966 --> 00:10:47,266
那么有意思的就是 Hitton

332
00:10:47,266 --> 00:10:50,233
现在已经成为了深度学习巨头

333
00:10:50,366 --> 00:10:53,833
三巨人里面的站在 c 位的一个

334
00:10:54,033 --> 00:10:56,033
那看一下整个文章里面

335
00:10:56,033 --> 00:10:58,433
就是提出了有很多个 Expert

336
00:10:58,466 --> 00:11:00,133
然后通过一个 gating 网络

337
00:11:00,166 --> 00:11:01,000
来去选择

338
00:11:01,000 --> 00:11:04,533
到底怎么去激活哪个专家

339
00:11:04,566 --> 00:11:06,833
这里面就强调了一个模块块的思想

340
00:11:06,866 --> 00:11:08,466
每个专家来处理

341
00:11:08,466 --> 00:11:10,966
输入空间里面的特定领域

342
00:11:12,166 --> 00:11:14,433
说实话 90 年代的 MoE 的加构发展

343
00:11:14,433 --> 00:11:15,833
非常的缓慢

344
00:11:15,833 --> 00:11:17,766
除了 Hitton 提出来之后

345
00:11:17,833 --> 00:11:19,966
经历了一个非常漫长的时间

346
00:11:20,000 --> 00:11:22,366
现在才来到了 RNN 时代

347
00:11:22,566 --> 00:11:25,266
也就是 MoE 架构的雏形形成

348
00:11:25,266 --> 00:11:26,300
那 MoE 架构

349
00:11:26,333 --> 00:11:27,400
刚才讲到

350
00:11:27,400 --> 00:11:29,866
最核心的就是 2017 年的时候

351
00:11:29,866 --> 00:11:31,433
谷歌发表的这篇文章

352
00:11:31,433 --> 00:11:35,100
重新的把 MoE 带入大家的视角里面

353
00:11:35,166 --> 00:11:36,233
那这篇文章

354
00:11:36,233 --> 00:11:38,433
MoE 就结合了 LSTM

355
00:11:38,566 --> 00:11:41,466
训练出当时候最大的一个大模型

356
00:11:41,466 --> 00:11:45,566
137B 那整体的 MoE 的数量或者 Expert 数量

357
00:11:45,666 --> 00:11:46,933
高达 128K

358
00:11:46,966 --> 00:11:48,133
非常的夸张

359
00:11:48,133 --> 00:11:50,000
看一下它都有哪些特点

360
00:11:50,066 --> 00:11:51,633
首先它引入了一个路由

361
00:11:51,633 --> 00:11:52,966
也就是 Routing 机制

362
00:11:53,000 --> 00:11:54,233
现在所谓的路由

363
00:11:54,233 --> 00:11:56,133
跟的门控是一个概念

364
00:11:56,433 --> 00:11:59,166
使得每个输入只激活少量 Expert

365
00:11:59,200 --> 00:12:01,866
也就是引用了稀疏的 MoE

366
00:12:01,866 --> 00:12:04,333
实现整个计算成本和模型规模的分离

367
00:12:04,366 --> 00:12:05,833
这个很重要

368
00:12:05,833 --> 00:12:06,766
之前

369
00:12:06,800 --> 00:12:08,966
或者整个刚提出来的时候

370
00:12:08,966 --> 00:12:10,466
还是一个稠密的 MoE

371
00:12:10,666 --> 00:12:10,900
接着

372
00:12:10,933 --> 00:12:13,233
它还引用了一个 TOPK 的门控的机制

373
00:12:13,266 --> 00:12:15,666
只选择前 k 个 Expert 进行处理

374
00:12:15,666 --> 00:12:18,366
所以就说了我只激活少量的 Expert

375
00:12:18,766 --> 00:12:19,833
就前面 k 个

376
00:12:19,833 --> 00:12:20,466
那进一步

377
00:12:20,466 --> 00:12:22,433
去优化了整体的计算的效率

378
00:12:22,433 --> 00:12:23,033
另外的话

379
00:12:23,033 --> 00:12:24,300
为了训练的时候

380
00:12:24,333 --> 00:12:26,966
有这么夸张 128K 的专家

381
00:12:26,966 --> 00:12:27,266
所以

382
00:12:27,266 --> 00:12:29,566
就引入了一个负载均衡的概念

383
00:12:29,600 --> 00:12:31,400
通过一个辅助损失的 loss

384
00:12:31,400 --> 00:12:32,466
来保证

385
00:12:32,466 --> 00:12:34,833
Expert 就是专家之间的一个均衡

386
00:12:34,833 --> 00:12:36,933
避免了有些专家被过度的激活

387
00:12:36,966 --> 00:12:38,166
过度的计算

388
00:12:38,166 --> 00:12:39,933
其他专家就非常闲

389
00:12:44,666 --> 00:12:46,733
那 2017 年谷歌的这篇文章

390
00:12:46,766 --> 00:12:49,033
就被认为在 MoE LLM 里面

391
00:12:49,033 --> 00:12:52,100
一个非常重要的里程式一个研究

392
00:12:52,133 --> 00:12:54,233
所以现在大部分很多的相关的研究

393
00:12:54,600 --> 00:12:56,566
都会去引用这篇文章

394
00:12:56,600 --> 00:12:57,233
首次

395
00:12:57,233 --> 00:12:59,900
将 MoE 应用于 LLM 里面

396
00:12:59,933 --> 00:13:02,766
不过当时候还是 RNN 时代

397
00:13:04,200 --> 00:13:05,166
现在来看一下

398
00:13:05,166 --> 00:13:06,566
论文里面很重要的一个图

399
00:13:06,566 --> 00:13:08,200
那从这个图里可以看到

400
00:13:08,200 --> 00:13:09,666
RNN 结构

401
00:13:09,666 --> 00:13:11,466
这是一个 RNN 模型

402
00:13:11,666 --> 00:13:13,100
这是一个 RNN 结构

403
00:13:13,133 --> 00:13:14,133
这是一个 RNN 结构

404
00:13:14,133 --> 00:13:14,966
这是 RNN 结构

405
00:13:14,966 --> 00:13:17,266
这里面是双层两层 RNN

406
00:13:17,266 --> 00:13:18,266
不是单层

407
00:13:18,266 --> 00:13:19,866
为什么要做两层 RNN

408
00:13:19,866 --> 00:13:21,866
因为的 MoE 要插进去

409
00:13:21,866 --> 00:13:25,366
所以必须要在建在两层 RNN 中间

410
00:13:25,600 --> 00:13:27,666 
加一个 MoE 的模块

411
00:13:27,666 --> 00:13:28,266
那这里面

412
00:13:28,266 --> 00:13:29,433
刚才讲到了很重要

413
00:13:29,433 --> 00:13:31,500
就选择了 TOPK 的一个

414
00:13:31,533 --> 00:13:34,533
路由 Gating 来去选择不同的 Expert

415
00:13:34,533 --> 00:13:37,866
对可能 Expert 2 跟 Expert n-1 进行激活

416
00:13:37,866 --> 00:13:39,266
其他 Expert 就没有激活

417
00:13:39,266 --> 00:13:42,633
这种就是极度的稀疏的 MoE 的架构

418
00:13:44,266 --> 00:13:46,066
现在来到了第四个内容

419
00:13:46,066 --> 00:13:46,933
看一下

420
00:13:47,066 --> 00:13:50,766
MoE+Transformer 去怎么提升效果

421
00:13:50,800 --> 00:13:52,066
那这里面的 Transformer

422
00:13:52,166 --> 00:13:54,933
更多的是指传统的 Transformer

423
00:13:55,066 --> 00:13:56,933
那所谓的传统的 Transformer

424
00:13:56,966 --> 00:13:59,533
更多就是指有 encorder 跟 decorder

425
00:13:59,533 --> 00:14:01,066
而不是现在的 LLM 

426
00:14:01,066 --> 00:14:03,433
只有一个 decorder 部分

427
00:14:03,733 --> 00:14:05,966
那回到整个内容里面

428
00:14:05,966 --> 00:14:07,733
首先最核心的一篇文章

429
00:14:07,733 --> 00:14:09,266
就是 Gshard

430
00:14:09,266 --> 00:14:11,133
那 2020 年 6 月份的时候

431
00:14:11,166 --> 00:14:11,866
还是谷歌

432
00:14:11,866 --> 00:14:13,033
谷歌在 MoE 这个加构

433
00:14:13,033 --> 00:14:15,066
或者谷歌在 AI 的领域里面

434
00:14:15,066 --> 00:14:17,500
做了很多非常惊人的工作

435
00:14:17,566 --> 00:14:18,866
包括 Transformer 这个架构

436
00:14:18,866 --> 00:14:20,300
也是谷歌提出来

437
00:14:20,333 --> 00:14:22,266
只是被 openAI 发扬光大而已

438
00:14:22,533 --> 00:14:23,933
那么回到正题

439
00:14:24,000 --> 00:14:25,400
2020 年 6 月份的时候

440
00:14:25,400 --> 00:14:27,000
谷歌就提出了这篇文章

441
00:14:27,000 --> 00:14:28,066
把 MoE 架构

442
00:14:28,066 --> 00:14:30,566
首次的应用在整个 Transformer 里面

443
00:14:30,600 --> 00:14:32,800
因为 Transformer 是谷歌自己提出来

444
00:14:32,800 --> 00:14:35,566
然后 MOV 引入 RNN 也是谷歌先开创

445
00:14:35,566 --> 00:14:36,200
那这里面

446
00:14:36,200 --> 00:14:37,366
每两层

447
00:14:37,433 --> 00:14:39,633
将 FFN 替换成 MoE

448
00:14:39,633 --> 00:14:40,566
那参数量

449
00:14:40,600 --> 00:14:41,800
说实话非常的多

450
00:14:41,833 --> 00:14:43,900
从 12.5B 到 600B

451
00:14:43,933 --> 00:14:45,166
它变成了一个系列

452
00:14:45,166 --> 00:14:45,933
Gshard

453
00:14:45,933 --> 00:14:49,133
有非常多的 MoE 的不同的系列的模型

454
00:14:49,166 --> 00:14:51,000
那每层的 Expert 最大

455
00:14:51,000 --> 00:14:52,000
去到了 2048

456
00:14:52,000 --> 00:14:53,033
也就是两 k

457
00:14:53,600 --> 00:14:55,200
那这篇文章的一些特点

458
00:14:55,200 --> 00:14:56,533
来细数一下

459
00:14:56,566 --> 00:14:59,600
首先就提出了一个 MoE+Transformer

460
00:14:59,600 --> 00:15:00,533
这个架构

461
00:15:00,533 --> 00:15:02,533
通过悉数的激活 MoE 层

462
00:15:02,533 --> 00:15:05,033
去替换掉传统的 FFN 层

463
00:15:05,066 --> 00:15:07,500
也就是现在大部分能看到

464
00:15:07,533 --> 00:15:10,600
为什么 MoE 加 Transformer 主要替换 FFN

465
00:15:10,600 --> 00:15:12,233
在这篇文章来提出来

466
00:15:12,533 --> 00:15:12,866
第二个

467
00:15:12,866 --> 00:15:13,700
就提出

468
00:15:13,733 --> 00:15:15,866
或者引入了随机路由

469
00:15:15,866 --> 00:15:18,133
跟 Expert 容量的相关的概念

470
00:15:18,166 --> 00:15:18,800
去优化

471
00:15:18,800 --> 00:15:21,366
可能在大规模分布式并行

472
00:15:21,366 --> 00:15:23,266
或者大规模分布式训练的时候

473
00:15:23,333 --> 00:15:24,866
负载均衡的问题

474
00:15:24,966 --> 00:15:27,033
因为在 Transformer 负载均衡

475
00:15:27,033 --> 00:15:29,300
跟 RNN 负载均衡已经完全不一样

476
00:15:29,366 --> 00:15:31,600
里面的用到的损失函数也不一样

477
00:15:31,600 --> 00:15:32,133
所以说

478
00:15:32,133 --> 00:15:34,366
可能引入了一个新的内容

479
00:15:34,366 --> 00:15:36,833
不是按照刚才那篇文章去讲

480
00:15:36,833 --> 00:15:37,933
解的另外的话

481
00:15:37,966 --> 00:15:40,033
还提出了 Gshard 框架

482
00:15:40,033 --> 00:15:43,066
因为的模型规模非常的大

483
00:15:43,166 --> 00:15:44,566
6,000 亿

484
00:15:44,566 --> 00:15:46,066
非常的夸张

485
00:15:46,133 --> 00:15:48,333
所以需要一个独立的框架或者架构

486
00:15:48,333 --> 00:15:50,833
去支撑大规模的并行

487
00:15:50,833 --> 00:15:52,566
和大规模的一个计算

488
00:15:53,633 --> 00:15:55,266
那整体的影响蛮有意思

489
00:15:55,266 --> 00:15:57,233
就是真正的把 MoE 架构

490
00:15:57,233 --> 00:15:59,500
跟 Transformers 两个去结合起来

491
00:15:59,533 --> 00:16:00,200
而且

492
00:16:00,200 --> 00:16:03,133
去提出了 Gshard 的一个相关的框架架构

493
00:16:03,133 --> 00:16:05,433
然后让整个 MoE+Transformer

494
00:16:05,433 --> 00:16:07,133
能够在一个非常大规模

495
00:16:07,166 --> 00:16:09,333
并行的集区里面去实现

496
00:16:10,733 --> 00:16:11,833
在整篇文章里面

497
00:16:11,833 --> 00:16:13,566
最核心的就是这个图

498
00:16:13,666 --> 00:16:14,566
看到

499
00:16:14,600 --> 00:16:18,066
最左边就是整个 Transformer 的 encorder

500
00:16:18,066 --> 00:16:19,333
那中间的这个

501
00:16:19,366 --> 00:16:22,133
就是 encorder 里面增加了两层

502
00:16:22,133 --> 00:16:23,966
第一层就是 Multi-Head attention

503
00:16:24,233 --> 00:16:26,133
然后就是的自注意力机制

504
00:16:26,166 --> 00:16:28,666
然后有一个 MoE 的专家

505
00:16:28,733 --> 00:16:31,033
接着再加一个 Multi hand attention

506
00:16:31,033 --> 00:16:32,333
再加一个 feed forward

507
00:16:32,366 --> 00:16:33,433
就是 FFN 层

508
00:16:33,766 --> 00:16:35,933
夹在两层的中间

509
00:16:36,000 --> 00:16:38,000
跟谷歌之前 RNN

510
00:16:38,000 --> 00:16:38,933
是不是有点类似

511
00:16:38,933 --> 00:16:40,433
这是一个 RNN 的结构

512
00:16:40,433 --> 00:16:41,933
这是一个 RNN 的结构

513
00:16:41,966 --> 00:16:43,666
然后就这么两层

514
00:16:43,766 --> 00:16:44,633
那整个 transformer

515
00:16:44,633 --> 00:16:45,766
因为有很多层

516
00:16:45,800 --> 00:16:48,466
所以把它最核心的自主意力机制

517
00:16:48,866 --> 00:16:52,233
夹在两个字注意力机制中间

518
00:16:52,733 --> 00:16:56,000
那整个这篇文章为什么叫 Gshard

519
00:16:56,000 --> 00:16:57,733
最重要的就是切片

520
00:16:57,733 --> 00:17:00,266
因为专家数量非常的多

521
00:17:00,266 --> 00:17:01,300
每个专家数量

522
00:17:01,333 --> 00:17:03,366
在整个 Transformer 架构里面

523
00:17:03,366 --> 00:17:05,466
是放不下在一张卡

524
00:17:05,466 --> 00:17:05,900
于是

525
00:17:05,933 --> 00:17:09,200
就分开了很多个切片不同的 Shard

526
00:17:09,433 --> 00:17:12,633
那每个专家放在可能假设是一张卡

527
00:17:12,766 --> 00:17:14,333
那既然放在一张卡

528
00:17:14,333 --> 00:17:14,800
我要路由

529
00:17:14,800 --> 00:17:16,933
游我就需要做一个 All2All 的通讯

530
00:17:16,933 --> 00:17:17,333
就讲

531
00:17:17,333 --> 00:17:20,800
怎么去使用并行的通讯的内容

532
00:17:20,800 --> 00:17:21,666
GShard

533
00:17:21,666 --> 00:17:24,633
不管在 MoE 的一个理论领域

534
00:17:24,633 --> 00:17:25,466
方向的研究

535
00:17:25,466 --> 00:17:27,533
还是在工程方面上的研究

536
00:17:27,566 --> 00:17:30,166
都非常值得大家去回顾一下

537
00:17:31,033 --> 00:17:31,466
那接着

538
00:17:31,466 --> 00:17:33,100
看一下另外一篇文章

539
00:17:33,133 --> 00:17:34,600
叫做 Switch Transformer

540
00:17:34,600 --> 00:17:37,600
在谷歌 2021 年 1 月份的时候去发表

541
00:17:37,600 --> 00:17:39,000
刚才是 20 年

542
00:17:39,033 --> 00:17:41,100
最主要的就是还是基于 Transformer

543
00:17:41,133 --> 00:17:41,566
不过

544
00:17:41,566 --> 00:17:44,566
是基于谷歌发布的 T5 基础之上

545
00:17:44,633 --> 00:17:46,833
优化了整个路由的策略

546
00:17:46,866 --> 00:17:50,366
实现了参数规模更大到了 1.6T

547
00:17:50,733 --> 00:17:53,633
整体的一个超大规模的参数量

548
00:17:53,933 --> 00:17:55,533
这个文章最大的特点

549
00:17:55,533 --> 00:17:57,866
就是提出 switch Transformer 架构

550
00:17:57,966 --> 00:18:00,066
简化了整个路由的机制

551
00:18:00,066 --> 00:18:02,366
所以说刚才路由机制还是蛮复杂

552
00:18:02,633 --> 00:18:05,700
选择单个 Expert 进行一个激活

553
00:18:05,766 --> 00:18:06,400
那并且

554
00:18:06,400 --> 00:18:08,866
还通过一个稀疏的 Gating 的控制

555
00:18:08,866 --> 00:18:10,966
还有 Expert 的容量的控制

556
00:18:11,000 --> 00:18:12,966
去优化计算负载均衡

557
00:18:13,000 --> 00:18:16,566
实现了真正的超大规模的一个参数

558
00:18:16,566 --> 00:18:17,233
那这里面

559
00:18:17,233 --> 00:18:18,666
看到 MoE

560
00:18:18,666 --> 00:18:21,133
是非常利于的大模型

561
00:18:21,200 --> 00:18:23,000
scaling law 的法则

562
00:18:23,200 --> 00:18:23,933
整体的影响

563
00:18:23,933 --> 00:18:25,400
就是展示了 MoE

564
00:18:25,400 --> 00:18:26,533
在 LLM 

565
00:18:26,533 --> 00:18:28,266
大规模模型里面的潜力

566
00:18:28,266 --> 00:18:29,133
对于 Scaling Law

567
00:18:29,166 --> 00:18:30,233
还有相关的蒸馏

568
00:18:30,233 --> 00:18:32,533
最近看到 DeepSeek 也要做蒸馏

569
00:18:32,733 --> 00:18:34,166
最早的 MoE 架构

570
00:18:34,166 --> 00:18:36,533
是在谷歌提出来需要做蒸馏

571
00:18:36,533 --> 00:18:37,800
影响非常的深远

572
00:18:37,800 --> 00:18:41,333
MoE 架构里面最核心的一个工作

573
00:18:41,400 --> 00:18:42,366
这里面的架构

574
00:18:42,366 --> 00:18:44,833
跟刚才的 Gshard 就不一样

575
00:18:44,833 --> 00:18:46,666
这个是现在能看到

576
00:18:46,733 --> 00:18:49,366
大部分 MoE 架构的最核心的出现

577
00:18:49,366 --> 00:18:51,733
也就是一个 self attention

578
00:18:51,733 --> 00:18:53,400
再加一个 Switching FNN 层

579
00:18:53,400 --> 00:18:55,800
也就是 MoE 层

580
00:18:55,800 --> 00:18:56,633
这种架构

581
00:18:56,633 --> 00:18:59,266
就非常符合现代的一个 LLM 

582
00:18:59,266 --> 00:19:01,033
里面的 MoE 架构

583
00:19:02,133 --> 00:19:03,166
还有另外一篇论文

584
00:19:03,166 --> 00:19:04,933
就是叫做 STMoE

585
00:19:04,933 --> 00:19:08,133
2022 年 2 月份的时候谷歌去发布

586
00:19:08,133 --> 00:19:11,400
那基于也是 Transformer 的 encorer decoder 的架构

587
00:19:11,400 --> 00:19:12,666
最大的模型参数

588
00:19:12,666 --> 00:19:15,966
是 269 b 32B 的一个激活参数

589
00:19:16,000 --> 00:19:16,533
蛮有意思

590
00:19:16,533 --> 00:19:18,466
就是直到这篇文章来看

591
00:19:18,466 --> 00:19:19,933
整个 MoE 架构

592
00:19:19,966 --> 00:19:22,733
怎么去算它的一个模型参数量

593
00:19:22,733 --> 00:19:24,633
也是在这篇文章做一个奠基

594
00:19:24,633 --> 00:19:26,066
整体解决了 MoE 架构

595
00:19:26,066 --> 00:19:28,833
在训练和微调不稳定性的问题

596
00:19:28,833 --> 00:19:30,933
而且提升了迁移学习的能力

597
00:19:30,966 --> 00:19:32,966
所以的迁移可能是的蒸馏

598
00:19:32,966 --> 00:19:34,233
这些相关的内容

599
00:19:34,266 --> 00:19:34,966
那整体来说

600
00:19:35,000 --> 00:19:36,733
看一下它有哪些特点

601
00:19:36,966 --> 00:19:38,033
首先这篇文章

602
00:19:38,033 --> 00:19:40,433
就引入了很多相关的技术

603
00:19:40,433 --> 00:19:42,766
去解决训练不稳定问题

604
00:19:42,800 --> 00:19:43,966
例如梯度裁剪

605
00:19:43,966 --> 00:19:44,933
噪声注入

606
00:19:44,933 --> 00:19:47,000
还有路由的限制

607
00:19:47,333 --> 00:19:48,766
刚才讲到了很重要的一点

608
00:19:48,766 --> 00:19:50,666
就是优化了一个微调的策略

609
00:19:50,666 --> 00:19:52,466
使得整个 ST-MoE

610
00:19:52,466 --> 00:19:54,566
提升了他在迁移学习的能力

611
00:19:54,600 --> 00:19:57,366
更好的适配到不同的下游任务

612
00:19:57,866 --> 00:19:59,466
看一下这篇文章的影响

613
00:19:59,466 --> 00:20:00,100
蛮有意思

614
00:20:00,133 --> 00:20:02,366
就是更多的是工程实践上面

615
00:20:02,366 --> 00:20:04,066
一个很重要的一个指导

616
00:20:04,066 --> 00:20:07,133
为后面做大规模的一个拓展

617
00:20:07,166 --> 00:20:08,966
提供了非常好的一个借鉴

618
00:20:10,566 --> 00:20:13,466
来到了最靠近的一个时代

619
00:20:13,466 --> 00:20:15,700
也就是 GPT 的时代

620
00:20:15,733 --> 00:20:16,600
智能的涌现

621
00:20:16,600 --> 00:20:17,933
现在大模型

622
00:20:18,133 --> 00:20:20,733
都去用一个 Decorder 的架构

623
00:20:20,733 --> 00:20:22,366
那这个实际上最开

624
00:20:22,366 --> 00:20:24,066
始引入还是谷歌

625
00:20:29,200 --> 00:20:31,333
叫做 GlaM

626
00:20:31,333 --> 00:20:32,600
为什么会 G 开头

627
00:20:32,600 --> 00:20:34,200
就是谷歌去发表的嘛

628
00:20:34,200 --> 00:20:36,333
那 2021 年的 12 月份的时候

629
00:20:36,333 --> 00:20:38,666
谷歌就采用了一个稀疏的 MoE

630
00:20:39,666 --> 00:20:40,500
模型的参数量

631
00:20:40,533 --> 00:20:41,400
非常大

632
00:20:41,466 --> 00:20:43,433
1.2 万亿的规模

633
00:20:43,433 --> 00:20:45,500
实际激活的参数量是 97B

634
00:20:45,666 --> 00:20:47,166
那最大是 1.2T

635
00:20:47,200 --> 00:20:48,000
其实

636
00:20:48,000 --> 00:20:51,166
它还是一个系列的 decord er only 的架构

637
00:20:51,166 --> 00:20:53,466
马友谊是当时已经采用了 decorder only

638
00:20:53,466 --> 00:20:56,633
那重点来看一下它的有哪些特点

639
00:20:57,166 --> 00:20:57,766
首先第一个

640
00:20:57,766 --> 00:21:00,033
就是稀疏的激活的机制

641
00:21:00,033 --> 00:21:01,733
也就是所谓的每一个输入

642
00:21:01,766 --> 00:21:03,633
都可能通过的门控

643
00:21:03,633 --> 00:21:07,033
Gating 来去动态的选择两个专家

644
00:21:07,033 --> 00:21:07,666
现在

645
00:21:07,666 --> 00:21:10,066
为什么在这么大的规模里面

646
00:21:10,066 --> 00:21:12,066
在看模型大小的时候

647
00:21:12,066 --> 00:21:13,900
还要关心实际激活的参数

648
00:21:14,033 --> 00:21:16,433
是因为会动态的选择部分的 Expert

649
00:21:17,166 --> 00:21:19,233
激活相关的内容

650
00:21:19,366 --> 00:21:21,866
现在来看到大部分的这种 MoE 的加构

651
00:21:21,866 --> 00:21:23,066
稀疏的 MoE

652
00:21:23,200 --> 00:21:26,200
是因为主要是采用动态的激活

653
00:21:26,200 --> 00:21:27,233
而不是全激活

654
00:21:27,233 --> 00:21:28,833
专家那整体来说

655
00:21:28,833 --> 00:21:29,866
通过这种方式

656
00:21:29,866 --> 00:21:32,533
就实现了条件的一个计算

657
00:21:32,566 --> 00:21:33,933
也就是的网络模型

658
00:21:33,933 --> 00:21:36,200
根据输入动态去调整

659
00:21:36,200 --> 00:21:37,400
那至于怎么调整

660
00:21:37,400 --> 00:21:39,400
更多的是的一个损失函数

661
00:21:39,400 --> 00:21:41,366
怎么去设计训练的过程当中

662
00:21:41,366 --> 00:21:43,066
他怎么去选择的问题

663
00:21:43,566 --> 00:21:44,633
每个 MoE 层

664
00:21:44,633 --> 00:21:46,233
就有 64 个 Expert

665
00:21:46,233 --> 00:21:49,133
64 个 Expert 分别在不同的 NPU

666
00:21:49,166 --> 00:21:50,166
上面去实现

667
00:21:50,166 --> 00:21:52,200
就是实现了一个工程上的跨式

668
00:21:52,200 --> 00:21:52,966
备的扩展

669
00:21:52,966 --> 00:21:55,366
也就是 scalling out 这种方式

670
00:21:55,366 --> 00:21:57,833
那看一下整体的影响有哪些

671
00:21:57,866 --> 00:21:58,433
是首先

672
00:21:58,433 --> 00:21:59,833
它展现的整个 MoE 架构

673
00:21:59,833 --> 00:22:00,733
在多任务学习

674
00:22:00,766 --> 00:22:03,000
和多语言处理的相关的优势

675
00:22:03,000 --> 00:22:04,833
叫做 Expert

676
00:22:04,833 --> 00:22:06,666
每个专家处理不同的内容

677
00:22:06,966 --> 00:22:07,833
很多的概念

678
00:22:07,833 --> 00:22:09,566
主要是来源于这篇文章

679
00:22:09,666 --> 00:22:12,233
提升了模型的泛化性的能力

680
00:22:12,266 --> 00:22:14,700
后面很多的大模型厂商

681
00:22:14,733 --> 00:22:15,733
也是很重要

682
00:22:15,733 --> 00:22:18,200
参考了 GlaM 这篇文章

683
00:22:18,200 --> 00:22:19,800
包括之前比较火

684
00:22:19,800 --> 00:22:21,966
去年比较火的 Mistral 8x7B

685
00:22:21,966 --> 00:22:22,833
那现在 Mistral

686
00:22:22,833 --> 00:22:25,266
已经往前引进到 8x22B

687
00:22:25,533 --> 00:22:26,333
整个 GlaM

688
00:22:26,333 --> 00:22:28,400
为后续的 LLM 

689
00:22:28,433 --> 00:22:30,266
提供了很重要的参考

690
00:22:30,600 --> 00:22:31,233
整体来说

691
00:22:31,233 --> 00:22:33,933
这篇文章基本上就长这个样子

692
00:22:34,066 --> 00:22:35,566
最核心在 Attention 之后

693
00:22:35,600 --> 00:22:36,933
对 FFN 层

694
00:22:36,933 --> 00:22:37,866
就扩展

695
00:22:37,866 --> 00:22:39,466
把 FFN 变得很多

696
00:22:39,466 --> 00:22:41,166
每个 FFN 都是一个专家

697
00:22:41,200 --> 00:22:42,666
但 FFN 的大小

698
00:22:42,666 --> 00:22:44,766
就没以前那么的大

699
00:22:47,133 --> 00:22:47,800
那另外的话

700
00:22:47,800 --> 00:22:50,066
看一下最近特别火的一篇文章

701
00:22:50,066 --> 00:22:52,766
就是 DeepSeek MoE 的一个文章

702
00:22:52,800 --> 00:22:54,133
24 年的 1 月份

703
00:22:54,133 --> 00:22:56,533
说实话 DeepSeek 现在已经进化到 V3

704
00:22:56,533 --> 00:22:58,166
但是 V1 的版本

705
00:22:58,166 --> 00:22:59,366
可能会更核心

706
00:22:59,366 --> 00:23:01,666
因为他提出了两个内容

707
00:23:01,833 --> 00:23:02,166
第一个

708
00:23:02,200 --> 00:23:05,233
就是 Expert 专家进行共享

709
00:23:05,233 --> 00:23:06,033
部分的专家

710
00:23:06,033 --> 00:23:08,100
在不同的 Token 过程里面

711
00:23:08,366 --> 00:23:11,533
共享他们之间专家跟专家之间的参数

712
00:23:11,533 --> 00:23:12,800
减少模型的冗余

713
00:23:12,800 --> 00:23:13,600
提升了效果

714
00:23:13,600 --> 00:23:14,000
第二个

715
00:23:14,000 --> 00:23:15,733
就是内存的优化

716
00:23:15,733 --> 00:23:16,766
那这里面蛮有意思

717
00:23:16,766 --> 00:23:19,400
就是提出了一个 MLA 的机制

718
00:23:19,433 --> 00:23:22,233
有多头的潜在注意力机制

719
00:23:22,233 --> 00:23:25,533
Multihead latent attention

720
00:23:25,800 --> 00:23:28,366
来去优化的 KV cache

721
00:23:28,466 --> 00:23:29,866
也就是以存代算

722
00:23:29,866 --> 00:23:30,266
去减少

723
00:23:30,266 --> 00:23:32,533
生产任务当中的一个计算量

724
00:23:32,566 --> 00:23:34,200
提升的一个实验

725
00:23:36,000 --> 00:23:36,966
那整体来说

726
00:23:36,966 --> 00:23:38,000
幻方这

727
00:23:38,000 --> 00:23:39,466
DeepSeek MoE

728
00:23:39,466 --> 00:23:40,633
现在 Qwen

729
00:23:40,633 --> 00:23:43,066
最近也说它已经融入了 DeepSeek

730
00:23:43,066 --> 00:23:45,133
MoE 里面的最核心的相关的特性

731
00:23:45,166 --> 00:23:46,666
包括刚才的 MLA

732
00:23:46,666 --> 00:23:47,833
所以说它整体来说

733
00:23:47,833 --> 00:23:49,100
朱敏认为它最重要

734
00:23:49,133 --> 00:23:51,000
就是对开源和生态建设

735
00:23:51,000 --> 00:23:52,866
一个最重要的推动

736
00:23:54,033 --> 00:23:56,433
那看一下 MoE 这篇文章的一个图

737
00:23:56,433 --> 00:23:58,700
所谓的一个共享专家

738
00:23:58,733 --> 00:24:01,566
其实不是指专家 1

739
00:24:01,666 --> 00:24:04,500
可能另外一个进行一个信息的合并

740
00:24:04,533 --> 00:24:07,633
共享而是指他多了一个共享的专家

741
00:24:07,633 --> 00:24:09,766
真的是多了一个专家出来

742
00:24:09,800 --> 00:24:11,666
所以叫做共享专家

743
00:24:11,666 --> 00:24:14,900
然后参与到其他专家的一个计算

744
00:24:16,733 --> 00:24:18,833
现在来到了

745
00:24:18,833 --> 00:24:20,666
或者这个视频里面的最后一个内容

746
00:24:20,666 --> 00:24:22,433
就是 Mixtral 的一个可视化

747
00:24:22,433 --> 00:24:24,166
以 Mixtral 8x7B

748
00:24:24,200 --> 00:24:26,633
作为一个例子跟大家看一下

749
00:24:26,633 --> 00:24:27,566
每一层 MoE

750
00:24:27,600 --> 00:24:28,633
他都在学什么

751
00:24:28,633 --> 00:24:29,666
都在看什么

752
00:24:29,800 --> 00:24:30,533
那首先

753
00:24:30,533 --> 00:24:31,800
这个图

754
00:24:31,800 --> 00:24:33,266
其实我觉得没什么意义

755
00:24:33,266 --> 00:24:34,566
大家觉得可能路由之后

756
00:24:34,600 --> 00:24:35,666
有很多个专家

757
00:24:35,733 --> 00:24:36,566
实际上

758
00:24:36,566 --> 00:24:37,833
的专家呀

759
00:24:37,833 --> 00:24:41,166
是嵌在的一个 Transformer 架构里面

760
00:24:41,433 --> 00:24:44,333
attention 层之后的一个 FFN

761
00:24:44,366 --> 00:24:45,766
进行一个代替

762
00:24:45,766 --> 00:24:47,666
所以我觉得跟刚才已经讲了很多

763
00:24:47,666 --> 00:24:48,766
大家已经认识

764
00:24:48,800 --> 00:24:49,566
那接下来

765
00:24:49,566 --> 00:24:50,533
这个实验

766
00:24:50,533 --> 00:24:52,866
主要是使用一个 MMLU

767
00:24:53,233 --> 00:24:54,966
所谓的多语言模型的一个基准

768
00:24:55,000 --> 00:24:56,866
来进行一个测评

769
00:24:56,866 --> 00:24:58,533
那 MMLU 说实话里面

770
00:24:58,566 --> 00:25:00,566
有很多个不同的选择题

771
00:25:00,566 --> 00:25:01,666
给的 LLM 

772
00:25:01,666 --> 00:25:04,166
会给的模型进行一个计算

773
00:25:04,566 --> 00:25:06,000
一共有 57 个主题

774
00:25:06,000 --> 00:25:07,633
主题的内容非常的广泛

775
00:25:07,633 --> 00:25:09,333
也就是的测评集

776
00:25:09,400 --> 00:25:11,033
数据集非常广泛包

777
00:25:11,033 --> 00:25:12,166
括有从现在代数

778
00:25:12,200 --> 00:25:12,733
宗教学

779
00:25:12,733 --> 00:25:13,200
解剖学

780
00:25:13,200 --> 00:25:13,966
天文学

781
00:25:13,966 --> 00:25:15,433
相关不同的题目

782
00:25:15,433 --> 00:25:16,433
有很多的选择

783
00:25:16,433 --> 00:25:17,566
让大于模型

784
00:25:17,600 --> 00:25:19,133
看能不能做对

785
00:25:19,266 --> 00:25:20,366
那这里面

786
00:25:20,400 --> 00:25:22,866
就以 Mixtral 8x7P 作为例子

787
00:25:22,866 --> 00:25:23,966
取出了第一层

788
00:25:24,333 --> 00:25:27,233
第 16 层跟最后一层了 32 层

789
00:25:27,433 --> 00:25:28,133
来看一下

790
00:25:28,166 --> 00:25:29,466
整个 Mixtral 里面

791
00:25:29,466 --> 00:25:31,466
因为 Mixtral 为什么 8x7

792
00:25:31,466 --> 00:25:33,433
因为有八个专家

793
00:25:33,600 --> 00:25:36,633
每个专家是 7B 的大小

794
00:25:36,633 --> 00:25:37,433
这种方式

795
00:25:37,433 --> 00:25:37,933
去看一下

796
00:25:37,966 --> 00:25:40,066
每一个 Expert 的一个计划的情况

797
00:25:40,800 --> 00:25:41,233
那接下来

798
00:25:41,233 --> 00:25:43,133
看一下总体来说

799
00:25:43,166 --> 00:25:43,600
第一个

800
00:25:43,600 --> 00:25:46,633
要看一下负载均衡这个内容

801
00:25:46,633 --> 00:25:48,933
因为说到了专家数多

802
00:25:48,966 --> 00:25:50,866
那有可能有些专家算的特别多

803
00:25:50,866 --> 00:25:52,566
有些专家特别算的特别少

804
00:25:52,833 --> 00:25:54,933
所以整体来看一下不同的层

805
00:25:54,966 --> 00:25:55,766
刚才讲到

806
00:25:55,766 --> 00:25:57,166
我可以分开三层来看

807
00:25:57,166 --> 00:25:58,600
第一层第 16 层

808
00:25:58,600 --> 00:25:59,833
最后一层 32 层

809
00:26:00,000 --> 00:26:02,233
可以看到的 TOKEN 输进去

810
00:26:02,233 --> 00:26:03,366
在每一层里面

811
00:26:03,400 --> 00:26:05,266
专家的处理 TOKEN 的数量

812
00:26:05,866 --> 00:26:07,166
整体来说是差不多

813
00:26:07,200 --> 00:26:08,233
也就大差不差

814
00:26:08,266 --> 00:26:10,166
但是最忙碌的专家

815
00:26:10,200 --> 00:26:12,066
跟最闲的专家之间

816
00:26:12,066 --> 00:26:15,533
仍然可能有 40%-60%之间的一个差距

817
00:26:15,566 --> 00:26:18,166
也就是处理的一个比例不同

818
00:26:18,200 --> 00:26:19,800
有些专家还是比较忙

819
00:26:19,866 --> 00:26:21,466
有些专家可能比较闲

820
00:26:21,566 --> 00:26:21,966
但是

821
00:26:21,966 --> 00:26:24,666
通过负债均衡的一个处理之后

822
00:26:24,666 --> 00:26:26,033
相对比较平均

823
00:26:26,033 --> 00:26:27,866
而不是说有些专家非常的高

824
00:26:27,866 --> 00:26:29,900
有些专家直接不干活

825
00:26:30,533 --> 00:26:31,466
那看一下

826
00:26:31,466 --> 00:26:33,433
既然了解完刚才的一个内容之后

827
00:26:33,433 --> 00:26:35,066
看一下每个专家

828
00:26:35,066 --> 00:26:37,766
专家都在干些什么

829
00:26:37,800 --> 00:26:39,466
都在算些什么

830
00:26:40,466 --> 00:26:42,066
这里面针对最后一层

831
00:26:42,066 --> 00:26:43,700
也就是 layer 32

832
00:26:43,733 --> 00:26:45,400
通过不同的数

833
00:26:45,400 --> 00:26:45,766
据

834
00:26:45,766 --> 00:26:47,800
因为的一个测试机的时候

835
00:26:47,800 --> 00:26:48,633
有很多的数据

836
00:26:48,633 --> 00:26:51,100
针对的某些数据来进行处理

837
00:26:51,133 --> 00:26:52,200
那可以看到

838
00:26:52,200 --> 00:26:55,033
呃如果输进去的是抽象的代数

839
00:26:55,033 --> 00:26:56,166
到 32 层里面

840
00:26:56,200 --> 00:26:59,466
专家 3 跟专家 8 是最忙碌

841
00:26:59,466 --> 00:27:00,333
那这个时候

842
00:27:00,366 --> 00:27:02,433
就意味着某些领域

843
00:27:02,433 --> 00:27:05,933
对比起其他领域更能激活部分的 Expert

844
00:27:06,066 --> 00:27:08,566
专家真正就是的 Expert

845
00:27:08,600 --> 00:27:10,766
能够针对具体的领域

846
00:27:10,766 --> 00:27:12,566
进行一个学习的可能

847
00:27:12,566 --> 00:27:14,066
专家 3 跟专家 8

848
00:27:14,066 --> 00:27:16,633
专门的去处理抽象代数的问题

849
00:27:16,633 --> 00:27:17,733
那其他的专家

850
00:27:17,766 --> 00:27:19,600
对抽象代数的处理能力

851
00:27:19,600 --> 00:27:20,400
是不够

852
00:27:20,400 --> 00:27:22,433
所以说真正在做选择的时候

853
00:27:22,433 --> 00:27:23,133
推理的时候

854
00:27:23,166 --> 00:27:25,733
更多的是用专家 3 和专家 8

855
00:27:26,033 --> 00:27:28,300
那从左边的抽象代数

856
00:27:28,333 --> 00:27:29,366
专业的法学

857
00:27:29,366 --> 00:27:32,866
还有的一个语言的领域来看到

858
00:27:32,933 --> 00:27:35,133
每个专家的一个负载分布

859
00:27:35,133 --> 00:27:35,800
是不一样

860
00:27:35,800 --> 00:27:37,366
也就是针对不同的主题

861
00:27:37,366 --> 00:27:38,466
不同的数据集

862
00:27:38,466 --> 00:27:40,066
不同的类型的任务

863
00:27:40,133 --> 00:27:41,933
专家的负载完全不一样

864
00:27:42,400 --> 00:27:43,466
如果所有的样本

865
00:27:43,466 --> 00:27:45,733
都属于某一类型的主题的时候

866
00:27:45,766 --> 00:27:47,666
就可能出现很大概率

867
00:27:47,666 --> 00:27:49,700
分布的不均匀

868
00:27:49,766 --> 00:27:50,533
不过没关系

869
00:27:50,533 --> 00:27:51,433
根据这个实验

870
00:27:51,433 --> 00:27:52,300
可以看到

871
00:27:52,333 --> 00:27:54,433
的专家还是有选择

872
00:27:54,433 --> 00:27:55,966
不是说我什么都学

873
00:27:56,000 --> 00:27:58,866
也不是说我某个专家只学某一个内容

874
00:27:58,866 --> 00:28:00,366
都是在配合

875
00:28:00,400 --> 00:28:00,833
但是

876
00:28:00,833 --> 00:28:02,633
某些专家在某些特定的领域

877
00:28:02,633 --> 00:28:03,900
就特别的牛逼

878
00:28:04,400 --> 00:28:05,466
那么现在来看一下

879
00:28:05,466 --> 00:28:06,733
剩下的最后的实验

880
00:28:06,766 --> 00:28:09,733
就是按 TOKEN 来去划分首选的 Expert

881
00:28:09,766 --> 00:28:10,366
那也是

882
00:28:10,366 --> 00:28:14,200
每个 TOKEN 都是否有自己首选的专家

883
00:28:14,200 --> 00:28:14,733
蛮有意思

884
00:28:14,733 --> 00:28:15,966
也是从第一层

885
00:28:15,966 --> 00:28:17,633
第 16 层和最后一层

886
00:28:17,633 --> 00:28:18,466
来去看一下

887
00:28:18,466 --> 00:28:20,633
如果我输的 TOKEN 是个冒号

888
00:28:20,633 --> 00:28:22,900
那首选的第一层的专家是 1 和 7

889
00:28:22,933 --> 00:28:25,433
貌似经过了多次的一个尝试

890
00:28:25,433 --> 00:28:26,533
和多次的推理

891
00:28:26,866 --> 00:28:27,733
可以看到

892
00:28:27,766 --> 00:28:29,933
所选的砖家完全不一样

893
00:28:29,933 --> 00:28:31,600
但是不是说每一层

894
00:28:31,600 --> 00:28:33,400
都只选择 2 个砖家

895
00:28:33,400 --> 00:28:36,266
而是每一层的砖家数是不一样

896
00:28:36,266 --> 00:28:37,500
所以可以看到

897
00:28:37,600 --> 00:28:40,033
每一层每个 Token 的输入

898
00:28:40,333 --> 00:28:42,833
的专家选择的领域

899
00:28:42,833 --> 00:28:43,900
都是不一样

900
00:28:43,933 --> 00:28:45,733
那针对不同的符号

901
00:28:45,733 --> 00:28:48,133
它采用的领域和选择的专家

902
00:28:48,133 --> 00:28:49,166
也是不一样

903
00:28:49,166 --> 00:28:51,533
刚才的冒号跟现在的句号不一样

904
00:28:51,533 --> 00:28:52,533
也是的不同层

905
00:28:52,533 --> 00:28:55,233
不同的专家是在处理不同的内容

906
00:28:55,233 --> 00:28:56,700
包括 what 这个内容

907
00:28:56,733 --> 00:28:59,666
第一层也是对应的不同的内容

908
00:28:59,666 --> 00:29:01,866
刚才句号是 2 和 8

909
00:29:02,133 --> 00:29:04,166
那现在 what 是 4 和 6

910
00:29:04,166 --> 00:29:04,866
看一下

911
00:29:04,866 --> 00:29:06,166
最后的就是一个 who

912
00:29:06,400 --> 00:29:07,833
它可能的内容是不一样

913
00:29:07,833 --> 00:29:09,633
有可能 who 在整个文章里面

914
00:29:09,633 --> 00:29:12,033
或者在的很多的语料里面

915
00:29:12,533 --> 00:29:13,800
他出现的概率更大

916
00:29:13,800 --> 00:29:16,066
针对在不同的领域都会出现

917
00:29:16,233 --> 00:29:16,833
那这里面

918
00:29:16,833 --> 00:29:19,333
ZOMI 欢迎大家输入一个肚脐

919
00:29:19,533 --> 00:29:20,733
看一下哪个专家

920
00:29:20,733 --> 00:29:22,233
处理这种可能在医疗领域

921
00:29:22,233 --> 00:29:23,066
或者比较神奇

922
00:29:23,066 --> 00:29:25,866
垂直的领域里面的具体的 Token

923
00:29:27,866 --> 00:29:29,366
那今天的内容就差不多

924
00:29:29,400 --> 00:29:32,266
来一个简单的思考和小结

925
00:29:32,366 --> 00:29:34,933
那很重要的就是 MoE 架构核心

926
00:29:34,933 --> 00:29:37,666
最重要的就是通过的稀疏

927
00:29:37,666 --> 00:29:39,533
激活和条件进行计算

928
00:29:39,566 --> 00:29:41,666
也就是的 gating 的路由的方式

929
00:29:41,666 --> 00:29:44,633
去提升大规模的并行能力

930
00:29:44,633 --> 00:29:45,933
第二个就是回顾

931
00:29:45,966 --> 00:29:47,533
从 1991 年开始

932
00:29:47,533 --> 00:29:48,233
首次提出

933
00:29:48,233 --> 00:29:49,266
了 MoE 架构

934
00:29:49,266 --> 00:29:52,166
到了近几年一个万亿 MoE 架构

935
00:29:52,200 --> 00:29:54,400
现在已经成为整个 LLM 

936
00:29:54,533 --> 00:29:56,333
很重要一个演进方向

937
00:29:56,333 --> 00:29:58,033
那 MoE 架构不断的演进

938
00:29:58,033 --> 00:30:00,066
说实话随着的模块化设计

939
00:30:00,066 --> 00:30:01,500
和分布式的发展

940
00:30:01,600 --> 00:30:02,766
未来 MoE 架构

941
00:30:02,766 --> 00:30:05,466
可能会在不管是未来也就是 2025 年

942
00:30:05,466 --> 00:30:07,500
今年了会在更多场景

943
00:30:07,533 --> 00:30:08,566
LLM 场景

944
00:30:08,566 --> 00:30:11,266
发挥更多更核心重要的一个作用

945
00:30:11,266 --> 00:30:12,300
相信

946
00:30:12,333 --> 00:30:13,400
越来越多的大厂

947
00:30:13,400 --> 00:30:15,966
会选择 MoE 这种架构的技术路线

948
00:30:16,466 --> 00:30:17,100
今天的内容

949
00:30:17,133 --> 00:30:18,866
就先到这里为止

950
00:30:18,866 --> 00:30:21,500
PPT 会开源在这条链接

951
00:30:21,533 --> 00:30:25,233
然后整体的相关的 MoE 文章

952
00:30:25,233 --> 00:30:27,333
也会开源在这个夸克链接

953
00:30:27,366 --> 00:30:28,666
欢迎大家去下载

