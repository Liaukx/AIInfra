1
00:00:00,000 --> 00:00:01,900
内容/录制/字幕:Z0MI 酱，视频剪辑:梁嘉铭

2
00:00:01,900 --> 00:00:03,200
hello 大家好

3
00:00:03,200 --> 00:00:04,283
我是 ZOMI

4
00:00:04,283 --> 00:00:04,600
现在

5
00:00:04,600 --> 00:00:07,333
我们还是在整个 MoE 系列视频里面

6
00:00:07,333 --> 00:00:09,133
对所有的论文

7
00:00:09,133 --> 00:00:10,650
进行一个解读的

8
00:00:10,650 --> 00:00:12,933
现在我们来到 MoE 加 Transformer

9
00:00:12,933 --> 00:00:13,800
在上一期视频

10
00:00:13,800 --> 00:00:16,000
我们还是在 RNN 时代

11
00:00:16,000 --> 00:00:17,933
也就是 Moe 加 RNN

12
00:00:18,133 --> 00:00:20,333
现在我们已经来到 transformer 的时代

13
00:00:20,333 --> 00:00:21,733
去看一下 MOE

14
00:00:21,733 --> 00:00:24,966
怎么去提升我们的 transformer 的一个性能

15
00:00:25,000 --> 00:00:25,566
那今天

16
00:00:25,566 --> 00:00:27,050
主要是有两个论文

17
00:00:27,050 --> 00:00:28,483
跟大家一起去分享的

18
00:00:28,733 --> 00:00:29,650
还是谷歌

19
00:00:29,650 --> 00:00:30,966
谷歌在 MOE 架构里面

20
00:00:30,966 --> 00:00:33,000
做非常多铺垫性的工作

21
00:00:34,283 --> 00:00:35,166
但奈何

22
00:00:35,166 --> 00:00:35,766
被换方

23
00:00:35,766 --> 00:00:36,850
抢个热头

24
00:00:39,650 --> 00:00:40,450
我们现在来看一下

25
00:00:40,450 --> 00:00:42,766
2022 年的或者 2020 年的时候

26
00:00:42,766 --> 00:00:45,533
谷歌发表一篇论文叫做 Gshard

27
00:00:45,800 --> 00:00:48,450
所谓的 G 就谷歌 shard 就切片

28
00:00:48,450 --> 00:00:50,366
就是谷歌通过切片的方式

29
00:00:50,366 --> 00:00:53,483
实现整个 MOE 架构的大规模的并行

30
00:00:53,533 --> 00:00:54,566
那我们现在来看一下

31
00:00:54,566 --> 00:00:55,883
整个视频的目录大纲

32
00:00:55,883 --> 00:00:57,333
我们现在来到核心的

33
00:00:57,333 --> 00:00:59,566
相关的论文的工作里面

34
00:00:59,566 --> 00:01:01,966
现在我们在解读朱煞的这一篇文

35
00:01:02,933 --> 00:01:04,400
论文是不是意思马上

36
00:01:04,400 --> 00:01:06,600
进入到 GSAR 的这篇文章

37
00:01:06,600 --> 00:01:08,400
里面的一个具体的解读

38
00:01:08,483 --> 00:01:11,366
跟大家一起去走读一下 GSAR 这篇论文

39
00:01:11,850 --> 00:01:12,083
当然

40
00:01:12,083 --> 00:01:14,683
如果大家比较想获取这个相关的 PPT

41
00:01:14,683 --> 00:01:16,483
或者相关的一个这么一解释过

42
00:01:16,483 --> 00:01:18,850
或者备注过后的一些文章

43
00:01:18,850 --> 00:01:21,366
那大家可以上 PPT 的一个链接

44
00:01:21,366 --> 00:01:24,200
也可以上夸克的这篇链接里面

45
00:01:24,200 --> 00:01:25,850
去获取相对的资源

46
00:01:25,850 --> 00:01:28,650
我们现在马上打开对应的论文

47
00:01:29,450 --> 00:01:31,083
哎呀讲的口干舌燥

48
00:01:31,083 --> 00:01:32,483
ZOMI 一次过录好多的视频

49
00:01:32,483 --> 00:01:33,766
然后慢慢的剪

50
00:01:33,766 --> 00:01:35,483
那我先去喝杯水

51
00:01:39,000 --> 00:01:39,650
ZOMI 回来

52
00:01:39,650 --> 00:01:40,166
我们今天

53
00:01:40,166 --> 00:01:43,850
来解读一下 g shop 在 2020 年的这篇文章

54
00:01:43,850 --> 00:01:44,333
蛮有意思的

55
00:01:44,333 --> 00:01:46,850
我们首先还是来看一下所谓的标题

56
00:01:46,850 --> 00:01:48,166
哦那标题蛮有意思的

57
00:01:48,166 --> 00:01:48,850
就是这里面

58
00:01:48,850 --> 00:01:50,600
就是 scaling giant model

59
00:01:50,600 --> 00:01:51,933
当时还不叫 foundation model

60
00:01:51,933 --> 00:01:53,850
也不叫 LLM 这种

61
00:01:53,850 --> 00:01:55,483
还是叫 giant model

62
00:01:55,733 --> 00:01:58,050
整体 LLM 当时还没成型

63
00:01:58,250 --> 00:01:58,933
那没关系

64
00:01:58,933 --> 00:02:00,850
反正就是模型很大

65
00:02:01,000 --> 00:02:03,083
反正做大模型的 scalling

66
00:02:03,083 --> 00:02:04,000
那里面有两个词

67
00:02:04,000 --> 00:02:04,933
总体觉得蛮有意思的

68
00:02:04,933 --> 00:02:05,483
就一个

69
00:02:05,483 --> 00:02:06,766
就是 condition computer

70
00:02:06,766 --> 00:02:09,250
一选就是可选择性的计算

71
00:02:09,250 --> 00:02:10,800
还有 Automatic shorting

72
00:02:10,800 --> 00:02:12,766
也就是我们的自动的切片

73
00:02:12,766 --> 00:02:13,683
那这里面

74
00:02:13,683 --> 00:02:14,733
condition complication

75
00:02:14,733 --> 00:02:16,000
更多的是指 moe

76
00:02:16,050 --> 00:02:17,683
而 Automatic shorting

77
00:02:17,683 --> 00:02:19,566
更多是指专家并行

78
00:02:19,800 --> 00:02:21,400
或者我们的并行相关的内容

79
00:02:21,400 --> 00:02:22,083
那同样的

80
00:02:22,083 --> 00:02:22,850
我们还是看一下

81
00:02:22,850 --> 00:02:25,200
整体的一个作者的比例

82
00:02:25,200 --> 00:02:26,333
那作者里面蛮有意思的

83
00:02:26,333 --> 00:02:28,600
就是我发现有姓陈的比例蛮高的

84
00:02:28,600 --> 00:02:29,200
然后就没

85
00:02:29,200 --> 00:02:30,650
中国人还占的蛮多的

86
00:02:30,650 --> 00:02:32,166
里面很重要的一个名字

87
00:02:32,166 --> 00:02:34,283
就是我们之前讲到的

88
00:02:34,400 --> 00:02:36,133
在安家 moe 里面

89
00:02:36,133 --> 00:02:37,933
最具有代表性意义的

90
00:02:37,933 --> 00:02:41,650
一个 Excel 大神所发表的文章

91
00:02:42,250 --> 00:02:44,483
那我们现在来看看 f 圈的对应的摘要

92
00:02:44,483 --> 00:02:47,050
梦也是就是现在来看到

93
00:02:47,050 --> 00:02:48,083
整个介绍的

94
00:02:48,083 --> 00:02:51,050
主要是由一组轻量化的一个 API

95
00:02:51,050 --> 00:02:51,966
还有 SLA

96
00:02:51,966 --> 00:02:53,333
编译器来去组成的

97
00:02:53,333 --> 00:02:54,533
或者来去实现的

98
00:02:54,533 --> 00:02:56,933
所以说可以看到介绍的这篇文章

99
00:02:56,933 --> 00:02:58,333
可能工程化的内容

100
00:02:58,333 --> 00:02:59,333
会特别的多

101
00:02:59,333 --> 00:03:00,650
关于算法内容不知道多不多

102
00:03:00,650 --> 00:03:02,133
我们往下继续看一看

103
00:03:02,283 --> 00:03:03,000
这边就说

104
00:03:03,000 --> 00:03:04,483
我用一个 sparset

105
00:03:04,483 --> 00:03:05,483
的 gate 的 Moe

106
00:03:05,483 --> 00:03:08,000
也就是稀疏门控的 Moe

107
00:03:08,000 --> 00:03:11,400
使得我们的网络模型规模增长到 600 币

108
00:03:11,400 --> 00:03:12,683
也就 6,000 亿

109
00:03:12,933 --> 00:03:16,683
现在的一个 Dipstick V3 MOE 架构

110
00:03:16,683 --> 00:03:18,200
才用 671 币哦

111
00:03:18,200 --> 00:03:20,650
所以说当时候已经做的非常的夸张哎

112
00:03:20,650 --> 00:03:24,533
最有意思的就是这个 SPAS 里 gate MOE

113
00:03:24,533 --> 00:03:26,566
用的网络模型规模参数

114
00:03:26,566 --> 00:03:27,733
跟 Dipstick 差不多

115
00:03:27,733 --> 00:03:28,333
同样

116
00:03:28,333 --> 00:03:31,250
也是用一个 204 版搭的一个 GPU

117
00:03:31,250 --> 00:03:33,483
不过这里面用的是 TPU V3

118
00:03:33,483 --> 00:03:34,450
训练 4D

119
00:03:34,450 --> 00:03:36,850
就完成整体的一个预训练

120
00:03:36,850 --> 00:03:38,483
不过当时 gshop 的这个

121
00:03:38,483 --> 00:03:40,766
是全送的 incord Decor 的架构

122
00:03:40,766 --> 00:03:42,366
处理的任务也不一样

123
00:03:42,800 --> 00:03:43,200
所以说

124
00:03:43,200 --> 00:03:45,650
不能完全直接的等比例

125
00:03:45,650 --> 00:03:46,533
不过没关系

126
00:03:46,533 --> 00:03:49,050
这篇文章其实在这里面就说

127
00:03:49,050 --> 00:03:51,283
可能工程化的内容占比比较多

128
00:03:51,400 --> 00:03:53,250
那我们现在就往下看一下

129
00:03:53,250 --> 00:03:54,366
整体的 Internet

130
00:03:54,366 --> 00:03:55,683
Internet 这里面就说到

131
00:03:55,683 --> 00:03:58,166
其实我们做各种各样的 MoE 的训练

132
00:03:58,166 --> 00:04:01,050
现在来说除规模增大以外

133
00:04:01,050 --> 00:04:02,566
chaining the efficiency

134
00:04:02,733 --> 00:04:04,600
也就是我们训练的效率

135
00:04:04,600 --> 00:04:06,283
变得非常的重要

136
00:04:06,283 --> 00:04:08,250
因此为解决训练的效率问题

137
00:04:08,250 --> 00:04:10,650
我们做很多相关的工作

138
00:04:10,733 --> 00:04:12,600
那我们现在来看一下这个图案

139
00:04:12,600 --> 00:04:13,283
随着我们的

140
00:04:13,283 --> 00:04:15,566
一个网络模型的规模的增大

141
00:04:15,566 --> 00:04:19,366
同 37.5B 到 600B 的一个权重

142
00:04:19,366 --> 00:04:22,683
网络模型的一个使用的硬件资源数

143
00:04:22,683 --> 00:04:24,050
也是先性的增长

144
00:04:24,050 --> 00:04:27,683
从 128 个 TPU 到 2048 个 TPU V3

145
00:04:27,683 --> 00:04:28,333
那 TPU

146
00:04:28,333 --> 00:04:30,133
当时候应该是做一个集群的

147
00:04:30,133 --> 00:04:32,483
也就是 TPU port 在 V3 的那一带

148
00:04:32,650 --> 00:04:33,133
那么一次

149
00:04:33,133 --> 00:04:35,883
就是训练的资源不断的在增加

150
00:04:35,883 --> 00:04:36,483
然后我们

151
00:04:36,483 --> 00:04:39,166
的模型的规模上到一定的量

152
00:04:39,166 --> 00:04:41,083
模型的增益就停止

153
00:04:41,083 --> 00:04:42,000
没有那么的夸张

154
00:04:42,000 --> 00:04:43,733
所以我们的网络模型的规模

155
00:04:43,733 --> 00:04:45,166
不是越大越好的

156
00:04:45,800 --> 00:04:48,000
当然这篇文章还是在论证我们

157
00:04:48,000 --> 00:04:49,883
随着我们的网络模型的规模增大

158
00:04:49,883 --> 00:04:50,933
我们的 MoE 结构增大

159
00:04:50,933 --> 00:04:52,050
我们的算力堆多

160
00:04:52,050 --> 00:04:53,933
我们的模型效果会变好

161
00:04:54,133 --> 00:04:54,933
确实

162
00:04:54,933 --> 00:04:56,650
当时候的一个认知也是这样的

163
00:04:56,650 --> 00:04:57,000
但是

164
00:04:57,000 --> 00:04:59,250
随着我们的专家数多到一定的程度

165
00:04:59,250 --> 00:05:00,200
它的效果

166
00:05:00,200 --> 00:05:02,933
就没有那么的明显

167
00:05:03,400 --> 00:05:04,966
那我们继续往下看一下

168
00:05:04,966 --> 00:05:06,483
看一下这篇文章蛮有意思的

169
00:05:06,483 --> 00:05:08,283
就是里面很重要的强调

170
00:05:08,283 --> 00:05:11,733
高领的一个对我们实际的 training

171
00:05:11,850 --> 00:05:13,600
也就是现在来看到

172
00:05:13,600 --> 00:05:14,566
基于全松板架构

173
00:05:14,566 --> 00:05:17,166
已经出现全松板加 MOE

174
00:05:17,333 --> 00:05:18,200
MOE 这个架构

175
00:05:18,200 --> 00:05:20,200
就是为把我们的 FFN 层

176
00:05:20,200 --> 00:05:22,966
替换成很多个 ME 的 FFN

177
00:05:23,283 --> 00:05:23,800
所以说

178
00:05:23,800 --> 00:05:25,933
现在最大的问题就是做 scale 领

179
00:05:25,933 --> 00:05:27,566
于是就提出几个点

180
00:05:27,566 --> 00:05:30,366
第一个就是可能现在的模型并行

181
00:05:30,366 --> 00:05:32,166
很多的 AI 框架是缺失的

182
00:05:32,166 --> 00:05:33,850
不管是探测服还是派套球

183
00:05:33,850 --> 00:05:35,200
在 2020 年的时候

184
00:05:35,200 --> 00:05:37,850
对于分布式并行的框架现在很少

185
00:05:37,850 --> 00:05:39,683
但是现在已经有很多

186
00:05:40,050 --> 00:05:42,600
例如周米之前跟大家去分享的大模型

187
00:05:42,600 --> 00:05:45,450
史囊厂里面的分布式的加速库

188
00:05:45,450 --> 00:05:47,283
或者分布式的并行的框架啦

189
00:05:47,283 --> 00:05:49,800
例如有业界比较著名的 dipstick 啦

190
00:05:49,800 --> 00:05:51,050
英伟的 art Mac trunk 啦

191
00:05:51,083 --> 00:05:53,250
刘洋老师路程科技的 Colossal AI 啦

192
00:05:53,250 --> 00:05:53,966
还有横跟 face

193
00:05:53,966 --> 00:05:54,166
当然

194
00:05:54,166 --> 00:05:58,000
还有华为升腾自己的一个 MID speed 啦

195
00:05:59,450 --> 00:06:01,650
同样的介绍完整个 architecture 一看

196
00:06:01,650 --> 00:06:02,533
model 变形之后

197
00:06:02,533 --> 00:06:04,200
我们现在回到整个

198
00:06:04,200 --> 00:06:06,166
文章的分享那蛮有意思的

199
00:06:06,166 --> 00:06:07,050
就是里面

200
00:06:07,050 --> 00:06:07,683
就说到

201
00:06:07,683 --> 00:06:10,200
可能我们需要去平衡模型的大小

202
00:06:10,200 --> 00:06:12,400
和计算资源的一个问题

203
00:06:12,400 --> 00:06:14,966
所谓的 Supernala 的一个 scaling

204
00:06:14,966 --> 00:06:17,400
可能会引起计算资源的短缺

205
00:06:17,400 --> 00:06:18,566
还有 infrastruction

206
00:06:18,566 --> 00:06:20,133
也就是 AI infer

207
00:06:20,133 --> 00:06:21,250
也就是 ZOMI 现在

208
00:06:21,250 --> 00:06:22,133
在坚持做的

209
00:06:22,133 --> 00:06:24,000
AI infer get up 里面的一个项目

210
00:06:24,000 --> 00:06:26,566
希望给大家讲很多相关因法类的东西

211
00:06:26,566 --> 00:06:28,000
怎么制成好我们的算法

212
00:06:28,000 --> 00:06:29,133
当时候的因法程度

213
00:06:29,133 --> 00:06:30,450
说实话还是比较弱的

214
00:06:30,683 --> 00:06:31,533
年龄伟大的 100 呀

215
00:06:31,533 --> 00:06:32,250
还没有做过

216
00:06:32,250 --> 00:06:35,366
非常大规模的一个集群组网

217
00:06:35,366 --> 00:06:36,200
而华为盛唐

218
00:06:36,200 --> 00:06:37,800
也是在后面有大模型之后

219
00:06:37,800 --> 00:06:39,766
才开始慢慢的把这块做起来的

220
00:06:39,766 --> 00:06:40,850
当时候的鱼

221
00:06:40,850 --> 00:06:41,600
除谷歌

222
00:06:41,600 --> 00:06:43,533
在搞这种超大规模的大模型

223
00:06:43,533 --> 00:06:46,050
大家恨不得把一张显卡分开两半

224
00:06:46,050 --> 00:06:46,800
来去做

225
00:06:46,800 --> 00:06:48,733
所以当时候出现很多 infer drops

226
00:06:48,733 --> 00:06:51,800
成做一个极致性的性能的推理啦

227
00:06:51,800 --> 00:06:53,966
还有一些叛逆切换的工作

228
00:06:54,133 --> 00:06:54,850
那不过

229
00:06:54,850 --> 00:06:57,883
其实我们回到整个大模型领域里面

230
00:06:57,883 --> 00:07:00,450
这篇文章也是做很多铺垫性的工作

231
00:07:00,450 --> 00:07:02,683
特别是在 ten so four 这个框架

232
00:07:02,800 --> 00:07:03,483
那我们看一下

233
00:07:03,483 --> 00:07:05,050
后面还有一些其他的内容

234
00:07:05,483 --> 00:07:07,133
1.21.2 还是蛮有意思的

235
00:07:07,133 --> 00:07:08,200
其实跟刚才差不多

236
00:07:08,200 --> 00:07:09,766
也就是我们有效的训练

237
00:07:09,766 --> 00:07:10,800
在 scaling 的时候

238
00:07:10,800 --> 00:07:12,050
怎么去实现

239
00:07:12,050 --> 00:07:13,966
那论文就说可能有几个点

240
00:07:13,966 --> 00:07:14,333
第一个

241
00:07:14,333 --> 00:07:15,733
就是我们怎么去解决的

242
00:07:15,733 --> 00:07:17,766
那我们提出 subnaler 的 scaling

243
00:07:18,000 --> 00:07:19,450
次线性缩放的方式

244
00:07:19,450 --> 00:07:19,850
还有

245
00:07:19,850 --> 00:07:22,166
对我们的分布式的硬件的进行抽象

246
00:07:22,166 --> 00:07:24,200
也就是 obstruction 相关的能力

247
00:07:24,200 --> 00:07:26,400
还有做一些可扩展的编音器

248
00:07:26,450 --> 00:07:27,566
把 MPMD 相关

249
00:07:27,566 --> 00:07:28,083
的能力

250
00:07:28,083 --> 00:07:29,933
慢慢抽象成为 SPMD 啦

251
00:07:29,933 --> 00:07:32,050
SPMD 就 single program Multi data

252
00:07:32,050 --> 00:07:34,366
注明在我们的一个 AI 编辑器里面

253
00:07:34,366 --> 00:07:35,283
的黄金十年

254
00:07:35,283 --> 00:07:37,200
重点去讲这个内容

255
00:07:37,400 --> 00:07:38,400
不过这篇文章

256
00:07:38,400 --> 00:07:40,200
当时里面其实没有太多的内容

257
00:07:40,200 --> 00:07:42,800
更多是讲一些面临的一些挑战

258
00:07:42,800 --> 00:07:44,483
反正做 Skyline 的过程当中

259
00:07:44,483 --> 00:07:45,800
有很多的挑战

260
00:07:45,800 --> 00:07:47,683
于是这篇文章 gshot

261
00:07:47,683 --> 00:07:50,200
在谷歌的 ten so four 的加持下

262
00:07:50,200 --> 00:07:53,533
就做很多的编译的框架的工作

263
00:07:53,533 --> 00:07:55,000
和编译的优化的工作

264
00:07:55,000 --> 00:07:56,166
分布是并行的

265
00:07:56,200 --> 00:07:57,566
工程化的工作

266
00:07:57,566 --> 00:08:00,166
所以说后面所有的 section 里面的综艺

267
00:08:00,166 --> 00:08:02,850
觉得最核心的就是 section two

268
00:08:03,400 --> 00:08:06,883
描述这个 Transformerarchitecture 为稀疏 gate

269
00:08:06,883 --> 00:08:09,766
稀疏门控的跟 moe 相关的细节

270
00:08:09,933 --> 00:08:10,533
再选 3

271
00:08:10,533 --> 00:08:11,766
就介绍更多的一个

272
00:08:11,766 --> 00:08:13,450
AI infer 相关的内容

273
00:08:13,450 --> 00:08:15,766
不过 AI infer 因为是基于 Tensorfold

274
00:08:15,766 --> 00:08:17,933
XLA 里面的 Halo 来去实现的

275
00:08:17,933 --> 00:08:18,333
所以说

276
00:08:18,333 --> 00:08:21,000
对于现在的一个分布式病情框架

277
00:08:21,050 --> 00:08:23,166
参考的意义并不那么的大

278
00:08:23,166 --> 00:08:24,683
那我们继续往下看一下

279
00:08:24,683 --> 00:08:26,050
所谓的 MPNT

280
00:08:26,050 --> 00:08:28,083
就是 Multi program Multi data

281
00:08:28,083 --> 00:08:31,166
我们针对一个大的矩阵的成

282
00:08:31,650 --> 00:08:34,200
需要把它每一行每一列拿出来

283
00:08:34,200 --> 00:08:36,933
单独的成在每个不同的节点

284
00:08:36,933 --> 00:08:38,533
那这个可能是单独一个节点

285
00:08:38,533 --> 00:08:39,966
中间又是单独一个节点

286
00:08:39,966 --> 00:08:40,450
现在

287
00:08:40,450 --> 00:08:43,650
有 SPMD 的一个编程的方式的抽象

288
00:08:43,650 --> 00:08:45,333
我们就把每一行每一列

289
00:08:45,333 --> 00:08:47,250
自动的成 Dynamic list

290
00:08:47,333 --> 00:08:49,400
Dynamic 自动的进行切分

291
00:08:49,400 --> 00:08:52,283
这种方式也是这篇文章最核心的内容

292
00:08:52,650 --> 00:08:54,450
那我们还是关注这篇文章

293
00:08:54,450 --> 00:08:55,650
我们想关注的点

294
00:08:55,650 --> 00:08:57,083
也就是 MoE 架构的

295
00:08:57,083 --> 00:08:58,283
一个能力

296
00:08:58,283 --> 00:08:59,483
那 MoE 架构里面

297
00:08:59,483 --> 00:09:01,166
就讲到第二个内容

298
00:09:01,166 --> 00:09:03,450
第二大内容就是我们的模型

299
00:09:04,333 --> 00:09:06,050
space scaling of the 诶

300
00:09:06,050 --> 00:09:07,166
Transformer Architecture

301
00:09:07,166 --> 00:09:08,733
不想听我的英文的

302
00:09:08,733 --> 00:09:10,483
没办法你接受吧

303
00:09:10,483 --> 00:09:12,400
我的英文就是这样的

304
00:09:24,050 --> 00:09:25,333
然后哈哈

305
00:09:25,333 --> 00:09:27,533
我们继续来到一个 2.2

306
00:09:27,533 --> 00:09:30,133
其实 2.1 上面的也是一些 integration 的

307
00:09:30,133 --> 00:09:31,250
然后我们来到 2.2

308
00:09:31,250 --> 00:09:33,000
它主要是 position west

309
00:09:33,083 --> 00:09:36,133
那最重要的其实就是下面的这一坨

310
00:09:36,766 --> 00:09:37,283
呃公式

311
00:09:37,283 --> 00:09:39,400
也就是这篇文章的最核心的内容

312
00:09:39,400 --> 00:09:42,133
首先我们对于输入的 sequence x

313
00:09:42,133 --> 00:09:44,000
有一个门控网络 gate 这个

314
00:09:44,050 --> 00:09:44,766
然后

315
00:09:44,766 --> 00:09:47,566
对应的就得到我们的 GSEE

316
00:09:47,566 --> 00:09:49,050
就是我们对应的 s 跑

317
00:09:49,050 --> 00:09:51,133
s 就是对应的 seq 款输入的一个图案

318
00:09:51,133 --> 00:09:51,966
来 say 选也好

319
00:09:51,966 --> 00:09:53,766
大家所谓的 sequence 也好

320
00:09:53,800 --> 00:09:55,450
接着有一个 WI

321
00:09:55,533 --> 00:09:57,133
还有一个 WOWI

322
00:09:57,133 --> 00:09:58,533
w o 就是对应的

323
00:09:58,533 --> 00:10:01,650
input 跟 output 的一个线性变换的矩阵

324
00:10:01,650 --> 00:10:03,283
然后通过一个录的方式

325
00:10:03,283 --> 00:10:05,333
就得到我们的一个 FN 层

326
00:10:05,333 --> 00:10:08,166
那这个就是我们真正的一个 FFN

327
00:10:08,200 --> 00:10:10,400
是我们的每一个专家

328
00:10:10,400 --> 00:10:11,483
因为专家

329
00:10:11,483 --> 00:10:12,133
很多个专家

330
00:10:12,133 --> 00:10:13,600
专家是一个模块

331
00:10:13,650 --> 00:10:14,850
专家的模块具体

332
00:10:14,850 --> 00:10:17,483
是使用 FFN 来去实现的

333
00:10:17,483 --> 00:10:19,166
然后有很多个 FFN

334
00:10:19,166 --> 00:10:22,966
去组成我们的整个网络 YS 的一个输出

335
00:10:23,000 --> 00:10:23,650
那基本上

336
00:10:23,650 --> 00:10:24,600
以这种方式

337
00:10:24,650 --> 00:10:26,200
那我们现在来看一下

338
00:10:26,200 --> 00:10:29,533
整个网络模型的一个结构

339
00:10:29,766 --> 00:10:32,000
因为论文刚好轮到这里面

340
00:10:32,000 --> 00:10:33,133
那整个网络模型

341
00:10:33,133 --> 00:10:33,766
蛮有意思的

342
00:10:33,766 --> 00:10:35,883
就是我们看一下这个 Transformer

343
00:10:35,883 --> 00:10:36,450
的 incode

344
00:10:36,450 --> 00:10:38,133
原来是长这个样子的

345
00:10:38,333 --> 00:10:40,766
然后 MoE 的全收门 encoder 之后

346
00:10:40,766 --> 00:10:43,333
你会发现我们的 MoE 差在哪个地方

347
00:10:43,333 --> 00:10:45,683
差在第一个 attention 层之后

348
00:10:45,683 --> 00:10:47,766
之上的我们加一个 FFN 层的

349
00:10:47,766 --> 00:10:50,650
一个网络模型的一个相关的模块

350
00:10:50,883 --> 00:10:53,050
接着我们在第二个 motile ten 选

351
00:10:53,050 --> 00:10:54,733
多加一个 ten 选的结构

352
00:10:54,733 --> 00:10:56,083
然后输出一个 fifth forward

353
00:10:56,083 --> 00:10:57,400
最后再输出的

354
00:10:57,400 --> 00:10:58,683
因为在整个计算里面

355
00:10:58,683 --> 00:11:00,366
当时用的也是一个 transformer

356
00:11:00,366 --> 00:11:02,000
所以 incorder 跟 decorder

357
00:11:02,000 --> 00:11:03,566
都有对应的能力

358
00:11:03,566 --> 00:11:05,566
那这个是单机版本哦

359
00:11:05,566 --> 00:11:07,800
单机版本里面有很多个 FFN 层

360
00:11:07,800 --> 00:11:08,333
但是

361
00:11:08,333 --> 00:11:10,600
我们的 FFN 层越来越大的时候

362
00:11:10,850 --> 00:11:12,283
必须对每一个专家

363
00:11:12,283 --> 00:11:14,166
因为一个 FM 就是一个专家嘛

364
00:11:14,166 --> 00:11:16,850
我们对每个专家进行个切分

365
00:11:16,850 --> 00:11:19,250
切分每个专家在一个卡上面

366
00:11:19,250 --> 00:11:19,650
那这样

367
00:11:19,650 --> 00:11:20,650
就是我们论文

368
00:11:20,650 --> 00:11:23,483
这篇论文的一个核心叫 SHOT1 到 shot e

369
00:11:23,483 --> 00:11:24,400
有多少个专家

370
00:11:24,400 --> 00:11:25,683
切换到多少张卡

371
00:11:25,683 --> 00:11:28,733
所以这里面就叫做 devisex replacement

372
00:11:28,966 --> 00:11:30,450
那这里面中间的通讯

373
00:11:30,450 --> 00:11:32,533
也就专家跟专家之间的通讯

374
00:11:32,683 --> 00:11:34,283
专家跟路由之间关系

375
00:11:34,283 --> 00:11:36,566
是通过一个 auto 来去实现的

376
00:11:36,566 --> 00:11:39,850
我们看一下这篇文章的一个描述

377
00:11:39,850 --> 00:11:41,683
就最对这张图的描述

378
00:11:42,000 --> 00:11:43,600
m a 相关的层

379
00:11:43,600 --> 00:11:47,166
是通过夸机器进行一个交互的

380
00:11:47,166 --> 00:11:48,683
也是通过上面这个 auto

381
00:11:48,683 --> 00:11:50,366
AUD despair 来去实现的

382
00:11:50,400 --> 00:11:51,650
但是其他层

383
00:11:51,650 --> 00:11:53,166
类似于那个 martyr 探选

384
00:11:53,166 --> 00:11:54,483
QKV 的相层

385
00:11:54,483 --> 00:11:55,933
那我们来筛选 inbanding

386
00:11:55,933 --> 00:11:56,733
还有 ad

387
00:11:56,733 --> 00:11:58,000
相关的计算

388
00:11:58,000 --> 00:11:58,800
所有的层

389
00:11:58,800 --> 00:12:00,450
都是一个副本

390
00:12:00,600 --> 00:12:01,766
所以我们讲到 m

391
00:12:01,766 --> 00:12:03,800
a 层在 10 倍之间进行页片

392
00:12:03,883 --> 00:12:07,000
其他成就被作为一个复制的副本

393
00:12:07,166 --> 00:12:09,000
我们继续往下看一看

394
00:12:09,000 --> 00:12:10,000
往下看的内容

395
00:12:10,000 --> 00:12:11,533
就越来越有意思

396
00:12:11,533 --> 00:12:13,766
首先这边有很多相关的定义

397
00:12:13,766 --> 00:12:14,366
这些定义

398
00:12:14,366 --> 00:12:15,050
我们会将

399
00:12:15,050 --> 00:12:17,766
在后面的这个算法结构里面

400
00:12:17,766 --> 00:12:19,283
去跟大家也讲一讲的

401
00:12:19,283 --> 00:12:21,166
那接着我们还是看回去

402
00:12:21,166 --> 00:12:24,166
在整个 MoE 架构里面讲哪些东西

403
00:12:24,166 --> 00:12:24,933
那首先

404
00:12:24,933 --> 00:12:26,400
这里面的这篇文章介绍的

405
00:12:26,400 --> 00:12:28,933
就是说我只取用两个专家

406
00:12:28,933 --> 00:12:30,883
最多两个专家来去实现

407
00:12:31,133 --> 00:12:31,883
另外的话

408
00:12:31,883 --> 00:12:33,766
gate 就是我们的门控函数

409
00:12:33,766 --> 00:12:35,450
说实话对 MO1 层来说

410
00:12:35,450 --> 00:12:37,400
是非常非常的重要的

411
00:12:37,400 --> 00:12:37,966
那一开始

412
00:12:37,966 --> 00:12:40,133
主要是由 Softmax 跟这个计划函数

413
00:12:40,133 --> 00:12:41,166
来去建模的

414
00:12:41,166 --> 00:12:42,600
来使得每个专家

415
00:12:42,600 --> 00:12:44,400
在处理输入 TOKEN

416
00:12:44,450 --> 00:12:46,333
或者输入粉丝的时候的

417
00:12:46,333 --> 00:12:47,850
一个权重的输出

418
00:12:47,850 --> 00:12:48,766
那换句话说

419
00:12:48,766 --> 00:12:50,166
就是这里面很重要的

420
00:12:50,166 --> 00:12:52,650
就是表明专家处理传入的脱壳的能力

421
00:12:52,650 --> 00:12:54,533
如何也是我们的门控网络的

422
00:12:54,533 --> 00:12:56,200
一个重要的能力

423
00:12:56,200 --> 00:12:57,250
为使这个能力

424
00:12:57,250 --> 00:12:58,800
其实有两个点

425
00:12:58,800 --> 00:12:59,683
或者有两个目标

426
00:12:59,683 --> 00:13:02,400
第一个就是做我们的 balance load

427
00:13:02,483 --> 00:13:04,400
也就是我们的均衡负载

428
00:13:04,933 --> 00:13:06,133
最简单的方法

429
00:13:06,133 --> 00:13:07,366
就是通过 softmare

430
00:13:07,366 --> 00:13:11,850
来去选择 TOPK 的一个前 k 个专家

431
00:13:11,850 --> 00:13:13,283
但是这种方式不太好

432
00:13:13,283 --> 00:13:15,250
所以后面就为解决这个方式

433
00:13:15,250 --> 00:13:16,166
就选择 TOPK

434
00:13:16,166 --> 00:13:19,283
就等于 2 只选择前面两个专家

435
00:13:19,450 --> 00:13:20,283
那另外的话

436
00:13:20,283 --> 00:13:21,200
第二个目标

437
00:13:21,200 --> 00:13:24,250
就是大规模的扩展 efficients s gilling

438
00:13:24,450 --> 00:13:25,933
对于给定的一个专家呀

439
00:13:25,933 --> 00:13:27,050
输入的 batch 当中

440
00:13:27,050 --> 00:13:28,683
所有的 n 个 TOKEN

441
00:13:28,800 --> 00:13:29,733
所以说整体来说

442
00:13:29,733 --> 00:13:31,050
门控的计算成本

443
00:13:31,050 --> 00:13:32,883
至少为 o 的 n 乘以一

444
00:13:33,450 --> 00:13:35,283
但是在作者的研究过程当中

445
00:13:35,283 --> 00:13:36,850
and 的数量是百万级的

446
00:13:36,850 --> 00:13:39,883
而专家的数量是至少是数千的

447
00:13:39,883 --> 00:13:40,850
所以整个 get

448
00:13:40,850 --> 00:13:42,650
使得大份的自算资源

449
00:13:42,650 --> 00:13:44,650
其实大部分时间是空闲的

450
00:13:44,650 --> 00:13:45,933
为解决这个问题

451
00:13:46,050 --> 00:13:47,450
我们需要一个高效的

452
00:13:47,450 --> 00:13:49,166
而且利用并行的能力

453
00:13:49,166 --> 00:13:51,600
去使能我们大规模的一个集群

454
00:13:51,600 --> 00:13:52,650
所以扩展的时候

455
00:13:52,650 --> 00:13:54,283
我们这个门控网络的设计

456
00:13:54,283 --> 00:13:55,400
就变得很重要

457
00:13:55,400 --> 00:13:57,683
在后面的一些可能 experience 里面

458
00:13:57,683 --> 00:13:59,483
就讲到门控的过程当中

459
00:13:59,483 --> 00:14:00,850
做分布式并行的时候

460
00:14:00,850 --> 00:14:03,366
通讯其实占很高的成本

461
00:14:03,766 --> 00:14:04,883
为解决这个问题

462
00:14:04,883 --> 00:14:06,450
我们现在就这篇文章

463
00:14:06,450 --> 00:14:09,050
就提出几个重要的关键点

464
00:14:09,050 --> 00:14:11,250
也就是它对应的一个算法

465
00:14:11,250 --> 00:14:14,683
低调使都 illustrate 在算法一里面

466
00:14:14,850 --> 00:14:15,650
那我们现在

467
00:14:15,650 --> 00:14:18,600
简单的看一下他做的几个重要的事情

468
00:14:19,566 --> 00:14:21,933
现在哈我们才正式的来到这篇文章

469
00:14:21,933 --> 00:14:23,366
最核心的内容

470
00:14:23,366 --> 00:14:26,250
也就是对应的一些算法的第一条

471
00:14:26,333 --> 00:14:28,250
首先第一个就是专家容量

472
00:14:28,250 --> 00:14:31,000
叫做 exper capacity

473
00:14:31,200 --> 00:14:32,933
那在整个专家容量里面

474
00:14:32,933 --> 00:14:34,883
为确保整体的负载均衡的

475
00:14:34,883 --> 00:14:36,166
一个 balances

476
00:14:36,600 --> 00:14:36,850
所以

477
00:14:36,850 --> 00:14:39,483
这里面就不希望有少量的专家

478
00:14:39,483 --> 00:14:41,733
ASP 需要处理很多的 TOKEN

479
00:14:41,733 --> 00:14:43,450
因此强制性的去规定

480
00:14:43,450 --> 00:14:45,800
每一个 ASP 所负责 TOKEN 的

481
00:14:45,800 --> 00:14:47,200
一个最大的数量

482
00:14:47,200 --> 00:14:50,966
那这个最大的数量叫做专家的容量

483
00:14:51,083 --> 00:14:52,883
在 gshart 这篇文章里面

484
00:14:52,883 --> 00:14:54,566
最多只允许两个专家

485
00:14:54,566 --> 00:14:55,366
所以这里面

486
00:14:55,400 --> 00:14:56,650
最大的专家容量

487
00:14:56,650 --> 00:14:58,650
就设置为 2N 除以一

488
00:14:58,650 --> 00:14:59,933
相相当于平均分

489
00:14:59,933 --> 00:15:01,200
配对两个专家

490
00:15:01,200 --> 00:15:02,166
那证明蛮有意思

491
00:15:02,166 --> 00:15:05,050
就是整个专家的 capacity 里面

492
00:15:05,333 --> 00:15:08,283
通过门控的网络给每一个专家

493
00:15:08,283 --> 00:15:12,050
维护一个计数器 CE 来进行监控的

494
00:15:12,050 --> 00:15:13,000
如果一个 touch

495
00:15:13,000 --> 00:15:15,166
所选择的两个专家呀

496
00:15:15,166 --> 00:15:17,533
都已经超过设定专家的容量

497
00:15:17,533 --> 00:15:18,366
那么这个 touch

498
00:15:18,366 --> 00:15:21,733
就不再被之前的任何层的 s Pro

499
00:15:21,733 --> 00:15:22,966
或者专家进行处理

500
00:15:22,966 --> 00:15:25,533
而是直接通过残差的一个方式

501
00:15:25,533 --> 00:15:27,050
透传到下一层

502
00:15:27,050 --> 00:15:29,800
也就我设定一个最大的专家容量

503
00:15:29,800 --> 00:15:32,166
反正你超过两个专家容量

504
00:15:32,166 --> 00:15:33,850
或者超过我设定的专家容量

505
00:15:33,850 --> 00:15:35,533
你就不能往下执行

506
00:15:35,533 --> 00:15:37,083
你最多传到下层

507
00:15:37,366 --> 00:15:38,083
那第二个

508
00:15:38,083 --> 00:15:39,766
就是分组分配

509
00:15:39,766 --> 00:15:42,883
我们叫做 LOCO group dispatching

510
00:15:42,966 --> 00:15:43,883
那分组分配

511
00:15:43,883 --> 00:15:45,450
主要是指所有的 TOKEN

512
00:15:45,450 --> 00:15:46,800
都分为居组

513
00:15:46,883 --> 00:15:48,766
不同的组并行处理

514
00:15:48,766 --> 00:15:49,450
每个组

515
00:15:49,450 --> 00:15:52,883
相应的把组类的专家容量变成 2N

516
00:15:52,883 --> 00:15:54,133
除以一乘以居

517
00:15:54,133 --> 00:15:55,333
那这样做的好处

518
00:15:55,333 --> 00:15:57,933
相当于在向前的前向的执行的时候

519
00:15:57,933 --> 00:15:59,133
也就是在推理的时候

520
00:15:59,250 --> 00:16:01,883
把大的 Betch size 拆成小的 Betch size

521
00:16:01,933 --> 00:16:05,000
每个小的 Betch size 就是对应一个 group

522
00:16:05,050 --> 00:16:06,600
对应一个大机

523
00:16:07,000 --> 00:16:08,333
那这种方式的好处

524
00:16:08,333 --> 00:16:09,450
就在通讯的时候

525
00:16:09,450 --> 00:16:11,766
特别是在 moe 专家通讯

526
00:16:11,766 --> 00:16:13,083
auto 通讯的时候

527
00:16:13,083 --> 00:16:15,200
只要在每个 group 内就可以

528
00:16:15,200 --> 00:16:17,000
减少我们的通讯量

529
00:16:17,166 --> 00:16:19,283
进而在反向的时候

530
00:16:19,283 --> 00:16:21,083
这些 group 都可以合并起来

531
00:16:21,083 --> 00:16:23,683
相当于进行一次梯度的累积

532
00:16:23,683 --> 00:16:25,883
我们叫做 gradient accumulation

533
00:16:26,566 --> 00:16:28,600
第三个就是辅助的损失

534
00:16:28,600 --> 00:16:31,166
我们叫做 accelery last

535
00:16:32,000 --> 00:16:32,800
光去试机

536
00:16:32,800 --> 00:16:34,483
我们的专家的容量

537
00:16:34,483 --> 00:16:36,566
结实把我们的专家限定成两个

538
00:16:36,566 --> 00:16:38,333
不能够使得整个 getting

539
00:16:38,333 --> 00:16:39,600
进行个负载均衡

540
00:16:39,600 --> 00:16:41,850
而且有可能导致大量的溢出

541
00:16:41,850 --> 00:16:44,166
因为就两个专家能激活

542
00:16:44,366 --> 00:16:46,133
其他的专家学不东西

543
00:16:46,133 --> 00:16:46,533
所以

544
00:16:46,533 --> 00:16:49,333
之前就参考 STM MOE 的相关的工作

545
00:16:49,333 --> 00:16:51,050
所以 14S 也在里面

546
00:16:51,050 --> 00:16:51,533
这里面

547
00:16:51,533 --> 00:16:54,533
同样的定义一个辅助损失函数

548
00:16:54,533 --> 00:16:57,333
来帮助进行一个负载均衡的

549
00:16:57,566 --> 00:16:59,683
同样的有一个 c 1 除以 s

550
00:16:59,683 --> 00:17:00,850
代表我们输入的

551
00:17:00,850 --> 00:17:02,600
路由道的每一个专家

552
00:17:02,600 --> 00:17:05,166
然后我们想最小化这个平均值数

553
00:17:05,166 --> 00:17:06,683
我们要除以 s

554
00:17:07,166 --> 00:17:08,366
s 是 TOKEN 数

555
00:17:08,366 --> 00:17:09,400
一是专家数

556
00:17:09,400 --> 00:17:12,050
c 就是分配给第一个专家

557
00:17:12,050 --> 00:17:13,733
哦这里面有 CE

558
00:17:13,766 --> 00:17:16,250
分配给第一个专家的 TOKEN 数

559
00:17:16,250 --> 00:17:17,283
那 m e

560
00:17:17,283 --> 00:17:18,733
对应的我要找找

561
00:17:18,733 --> 00:17:21,250
m e mind print Esper

562
00:17:21,283 --> 00:17:21,966
那 m e

563
00:17:21,966 --> 00:17:22,566
主要是指

564
00:17:22,566 --> 00:17:23,450
第一个专家

565
00:17:23,450 --> 00:17:27,000
在第 s 个 TOKEN 所获得的平均的权重

566
00:17:27,483 --> 00:17:28,650
整体的示诺就是

567
00:17:28,650 --> 00:17:31,333
本来我需要算 c 除以 s 的平方的

568
00:17:31,333 --> 00:17:31,766
但是

569
00:17:31,766 --> 00:17:34,283
因为这样做是离散值不可以倒

570
00:17:34,283 --> 00:17:36,483
所以把平方当中的 c 除以 s

571
00:17:36,483 --> 00:17:37,683
就换成 m e

572
00:17:37,683 --> 00:17:39,533
算成一个专家的平均值

573
00:17:39,566 --> 00:17:40,600
平均权重

574
00:17:41,050 --> 00:17:42,733
m e 是第一个 s per

575
00:17:42,733 --> 00:17:44,933
在 s 个 Tokan 获得的平均权重

576
00:17:44,933 --> 00:17:47,166
在平均的分配的情况下

577
00:17:47,250 --> 00:17:49,533
使得这个 loss 达到最小的值

578
00:17:49,533 --> 00:17:53,083
跟前面的一个 RNN 的 MOE 相比

579
00:17:53,083 --> 00:17:55,366
负载均衡的损失这个事迹

580
00:17:55,366 --> 00:17:57,533
就变得简单很多

581
00:17:57,533 --> 00:17:59,533
因为之前的 RN 是 SARS 里面

582
00:17:59,533 --> 00:18:01,933
就有一个 l important 跟 outload

583
00:18:01,933 --> 00:18:03,800
那这个就确实简单很多

584
00:18:03,883 --> 00:18:04,600
那最后

585
00:18:04,600 --> 00:18:06,366
就是一个随机的路由

586
00:18:06,366 --> 00:18:07,050
我们叫做

587
00:18:07,050 --> 00:18:09,483
微灯 voting 其实前面也提到过

588
00:18:09,483 --> 00:18:11,766
每一场会选择最多两个专家

589
00:18:11,766 --> 00:18:12,766
来进行激活

590
00:18:12,766 --> 00:18:15,366
因此就有随机路由的这个机制

591
00:18:15,366 --> 00:18:16,200
简单的来说

592
00:18:16,200 --> 00:18:16,600
就是

593
00:18:16,600 --> 00:18:19,600
如果我们 TOPK 的专家的权重很高

594
00:18:19,883 --> 00:18:21,200
第二个砖家的权重

595
00:18:21,200 --> 00:18:21,883
如果很小

596
00:18:21,883 --> 00:18:24,683
有可能只用到第一个砖家就够

597
00:18:24,766 --> 00:18:25,883
随机陆游的机制

598
00:18:25,883 --> 00:18:28,450
就保证我们第一个砖家套问的砖家

599
00:18:28,450 --> 00:18:30,200
永远都会被激活的

600
00:18:30,650 --> 00:18:32,883
如果第二个专家的权重很小

601
00:18:32,883 --> 00:18:34,483
那我们就可以忽略它

602
00:18:34,733 --> 00:18:35,883
所以真正陆游的时候

603
00:18:35,883 --> 00:18:38,966
会第二个专家的权重 G2 的比例

604
00:18:39,166 --> 00:18:41,200
来去激活第二个专家

605
00:18:41,200 --> 00:18:43,600
所以这里面说到 wait G2

606
00:18:43,650 --> 00:18:45,166
就相关的内容就我

607
00:18:45,166 --> 00:18:46,966
不一定是完全的激活两个

608
00:18:46,966 --> 00:18:49,766
有可能让算法自动的去选择

609
00:18:49,766 --> 00:18:50,366
训练的时候

610
00:18:50,366 --> 00:18:54,000
选择讲完相关的内容之后

611
00:18:54,000 --> 00:18:56,800
其实这个相关的内容它翻的比较散

612
00:18:56,800 --> 00:18:58,650
但是它有一个算法哦

613
00:18:58,650 --> 00:19:00,533
就是 Gshart 里面的算法

614
00:19:00,533 --> 00:19:01,683
里面就描述

615
00:19:01,683 --> 00:19:05,800
我的一个 group level 的 top two getting with

616
00:19:06,166 --> 00:19:07,200
辅助函数

617
00:19:07,200 --> 00:19:09,683
那把刚才上面的几个重点

618
00:19:09,683 --> 00:19:11,133
都囊括进来

619
00:19:11,766 --> 00:19:12,400
第一个重点

620
00:19:12,400 --> 00:19:14,166
就是专家容量没有在这里面

621
00:19:14,166 --> 00:19:15,566
但是分组分配

622
00:19:15,566 --> 00:19:17,400
good level 已经提出来

623
00:19:17,450 --> 00:19:19,566
辅助损失函数也提出来

624
00:19:19,566 --> 00:19:21,283
然后到 top two

625
00:19:21,283 --> 00:19:23,083
就是我们的随机路由

626
00:19:23,133 --> 00:19:25,733
两个 getting 之间也提出来

627
00:19:25,733 --> 00:19:28,166
那么现在看一下它具体的一个输入

628
00:19:28,600 --> 00:19:29,650
那输入的 SS

629
00:19:29,650 --> 00:19:31,883
就是一个 group TOKEN 的一个 size

630
00:19:31,883 --> 00:19:35,200
x 那 c 就是我们的专家容量

631
00:19:35,250 --> 00:19:36,166
那 GSE

632
00:19:36,166 --> 00:19:38,766
就是我们的 group combine 的一个 weight 权重

633
00:19:39,200 --> 00:19:40,366
LLUS 就是我

634
00:19:40,366 --> 00:19:42,450
们的一个 group 的一个 xlrobe 的 locks

635
00:19:42,450 --> 00:19:43,050
也有辅助

636
00:19:43,050 --> 00:19:44,766
省事那你可以看到这里面

637
00:19:44,766 --> 00:19:46,533
大部分都是以 group 为主的

638
00:19:46,533 --> 00:19:49,333
也是我们进行分组进行处理

639
00:19:49,566 --> 00:19:50,566
为什么要分组

640
00:19:50,566 --> 00:19:52,133
方便我们做并形嘛

641
00:19:52,200 --> 00:19:53,250
那么现在来看一下

642
00:19:53,250 --> 00:19:54,166
一开始的时候

643
00:19:54,166 --> 00:19:56,800
我们把 C1 设置一个具体的值

644
00:19:56,966 --> 00:19:58,766
每个专家的门控容量

645
00:19:59,133 --> 00:20:00,733
通过 softmas 的计算之后

646
00:20:00,733 --> 00:20:01,800
我们就得到

647
00:20:01,800 --> 00:20:04,733
每个专家的一个所处理的拓砍数

648
00:20:04,733 --> 00:20:07,250
那 WG 就是我们可以训练的权重

649
00:20:07,250 --> 00:20:09,450
那接下来我们要求 MOE

650
00:20:09,450 --> 00:20:11,250
那 MOE 就是我们刚才讲到的

651
00:20:11,650 --> 00:20:12,800
第一个专家

652
00:20:12,800 --> 00:20:15,800
在 s 个 TOKEN 里面所获得的平均权重

653
00:20:15,800 --> 00:20:19,200
设下面是除以 s 的所有的 TOKEN size

654
00:20:19,200 --> 00:20:19,683
那接着

655
00:20:19,683 --> 00:20:21,366
我们现在对于所有的 TOKEN size

656
00:20:21,366 --> 00:20:24,000
从第 0 个到第 s 减一个

657
00:20:24,000 --> 00:20:26,250
也就是第一个到第 s 个进行计算

658
00:20:26,283 --> 00:20:26,683
首先

659
00:20:26,683 --> 00:20:29,683
我们需要通过 TOPK 的或者 top two 的

660
00:20:29,683 --> 00:20:32,133
一个 getting 的一个路由的方式

661
00:20:32,133 --> 00:20:33,200
选出两个

662
00:20:33,200 --> 00:20:34,600
第一个是 G1

663
00:20:34,650 --> 00:20:35,933
第一二个是 G2

664
00:20:35,933 --> 00:20:38,083
还有对应的 11 和一二

665
00:20:38,566 --> 00:20:40,600
接着我们对 G1 求会一化

666
00:20:40,600 --> 00:20:42,450
然后去检测我们的 c

667
00:20:42,566 --> 00:20:45,250
就是否满足我们的一个容量大小

668
00:20:45,683 --> 00:20:46,800
接着我们看一下

669
00:20:46,800 --> 00:20:49,450
分配给第一个专家的一个容量数

670
00:20:49,450 --> 00:20:51,083
是否满足我们的上限

671
00:20:51,133 --> 00:20:52,366
如果没有满足上限

672
00:20:52,366 --> 00:20:55,166
那我们就更新我们的 GS 一一

673
00:20:55,683 --> 00:20:57,083
否则我们就更新

674
00:20:57,083 --> 00:20:59,966
分配给第一挨个专家的一个托肯数

675
00:20:59,966 --> 00:21:01,083
那就结束

676
00:21:01,083 --> 00:21:03,400
然后我们下面来去看一下

677
00:21:03,400 --> 00:21:04,283
下一个计算

678
00:21:04,283 --> 00:21:06,933
就是我们的一个辅助损失

679
00:21:07,050 --> 00:21:09,050
辅助损失也是同样的上面

680
00:21:09,050 --> 00:21:09,566
的参数

681
00:21:09,566 --> 00:21:11,483
其实是给下面也会用到的

682
00:21:11,483 --> 00:21:13,733
同样的我们去算一个损失

683
00:21:13,733 --> 00:21:15,333
有一个损失函数的定义

684
00:21:15,333 --> 00:21:17,766
接着我们的变例 s 从 1 到大 s

685
00:21:18,000 --> 00:21:21,250
同样地取出 TOPK 的两个 gate

686
00:21:21,250 --> 00:21:24,050
跟期望的一个缩影 index

687
00:21:24,200 --> 00:21:24,600
然后

688
00:21:24,600 --> 00:21:27,733
我们对一个 g o 进行一个求解

689
00:21:27,933 --> 00:21:29,766
也就是归化的过程当中

690
00:21:30,366 --> 00:21:32,133
通过引入一个 uniform 嘛

691
00:21:32,133 --> 00:21:34,683
也就是我们的标准的高斯分布

692
00:21:34,683 --> 00:21:35,883
获得一个随机值

693
00:21:35,883 --> 00:21:37,333
然后这个随机值

694
00:21:37,333 --> 00:21:39,966
去派给我们的一个第二个专家

695
00:21:39,966 --> 00:21:41,333
也是自由的专家

696
00:21:41,333 --> 00:21:43,050
然后同样的方式去判断

697
00:21:43,050 --> 00:21:44,800
我们的一个自由的专家

698
00:21:44,850 --> 00:21:47,333
能不能满足我们的一个容量的上限

699
00:21:47,366 --> 00:21:49,050
如果没有满足容量的上限

700
00:21:49,050 --> 00:21:51,400
那就更新我们的 GS12

701
00:21:51,600 --> 00:21:51,966
否则的话

702
00:21:51,966 --> 00:21:53,050
那我们也是同样

703
00:21:53,050 --> 00:21:55,400
更新我们的一个容量的上限

704
00:21:55,483 --> 00:21:57,966
那可以看到这里面有一个 capacity

705
00:21:57,966 --> 00:21:59,366
也就是我们的专家容量

706
00:21:59,366 --> 00:22:01,400
就在这个方式来去体现

707
00:22:01,566 --> 00:22:04,333
然后我们的一个 s 辅助损失函数

708
00:22:04,333 --> 00:22:05,933
通过这种方式来去实现

709
00:22:05,933 --> 00:22:06,450
而 good

710
00:22:06,450 --> 00:22:08,450
就把我们的数据进行一个分布

711
00:22:08,450 --> 00:22:10,333
把我们的 moe 进行分布

712
00:22:10,333 --> 00:22:12,250
最终选出两个专家

713
00:22:12,250 --> 00:22:14,533
两个专家之间进行一个更新

714
00:22:14,966 --> 00:22:16,400
m e 就是第一个专家

715
00:22:16,400 --> 00:22:18,883
在 Asktok 里面获得的平均权重

716
00:22:18,883 --> 00:22:21,400
通过平均权重进行一个规划

717
00:22:21,400 --> 00:22:23,200
之后再进行处理的

718
00:22:23,200 --> 00:22:25,600
那整个算法基本上就这样

719
00:22:25,600 --> 00:22:26,566
那有兴趣的小伙伴

720
00:22:26,566 --> 00:22:28,250
也可以看一下对应的代码

721
00:22:28,250 --> 00:22:30,366
不过这个我觉得最核心的

722
00:22:30,366 --> 00:22:32,483
还不是说他的算法有多牛逼

723
00:22:32,483 --> 00:22:34,083
而是最核心的就是

724
00:22:34,366 --> 00:22:36,050
辅助的损失怎么去定义

725
00:22:36,050 --> 00:22:37,966
更简单的更方便的去实

726
00:22:37,966 --> 00:22:39,483
现我们的辅助的损失

727
00:22:39,483 --> 00:22:40,650
也就是我们的 getting 的

728
00:22:40,650 --> 00:22:42,000
一个实现的方案

729
00:22:42,050 --> 00:22:43,733
我们将会在后面的内容

730
00:22:43,883 --> 00:22:46,333
跟大家一起去手把手的撸一个的

731
00:22:46,333 --> 00:22:48,683
那接下来内容我们简单的翻一翻

732
00:22:48,683 --> 00:22:50,333
蛮有意思的就是第三点

733
00:22:50,366 --> 00:22:53,083
大部分都是 AI infer 相关的内容

734
00:22:53,083 --> 00:22:55,650
通过 SLA 来去实现 SPMD

735
00:22:55,650 --> 00:22:57,366
那之前是 MPMD

736
00:22:57,366 --> 00:22:59,200
这种方式去太耗人力

737
00:22:59,200 --> 00:23:01,966
所以中间要搞一个 SLA 的编译器

738
00:23:03,450 --> 00:23:04,883
那 SLA 的编辑里面

739
00:23:04,883 --> 00:23:06,166
实现最多的一个通讯

740
00:23:06,166 --> 00:23:07,733
就是集合的 Premiu 通讯

741
00:23:07,733 --> 00:23:10,566
还有 OKDARoffice 跟 auto or 的通讯

742
00:23:10,566 --> 00:23:10,883
当然

743
00:23:10,883 --> 00:23:13,533
这里面还说到用很多相关的算法

744
00:23:13,533 --> 00:23:15,683
反正就是 AI 因法相关内容

745
00:23:15,683 --> 00:23:17,533
因为不在我们整个 MOE 的架构

746
00:23:17,533 --> 00:23:19,883
所以这里面就不详细的展开

747
00:23:20,450 --> 00:23:21,283
那在最后

748
00:23:21,283 --> 00:23:22,683
我们还是要看一下

749
00:23:22,683 --> 00:23:25,166
所谓的一些相关的实验

750
00:23:25,166 --> 00:23:27,766
也是对应的第五部分 experiences

751
00:23:27,766 --> 00:23:29,733
那从这个图我们可以看到

752
00:23:29,733 --> 00:23:31,733
随着我们的网络模型的

753
00:23:31,733 --> 00:23:33,366
一个规模的增大

754
00:23:33,600 --> 00:23:37,533
从我们的 128 个专家到 20 百多个专家

755
00:23:37,533 --> 00:23:40,333
然后 36 层层数也是越来越多

756
00:23:40,333 --> 00:23:41,000
可以看到

757
00:23:41,000 --> 00:23:44,733
网络模型的效果是越来越好的

758
00:23:44,733 --> 00:23:46,133
那左跟右

759
00:23:46,133 --> 00:23:47,566
就是我们现在所使用到的

760
00:23:47,566 --> 00:23:49,533
一个数据的参数量

761
00:23:49,533 --> 00:23:52,200
到我们的一个语料的数据越来越多

762
00:23:52,200 --> 00:23:55,250
网络模型的一个效果是越来越好

763
00:23:55,250 --> 00:23:56,800
但不是多到一定程度

764
00:23:56,800 --> 00:23:58,766
它就绝对性的好的

765
00:23:58,766 --> 00:24:00,166
是有一个比值的

766
00:24:00,166 --> 00:24:01,650
所以这里面就很讲究

767
00:24:01,650 --> 00:24:04,050
我们的模型的规模应该怎么去增长

768
00:24:04,050 --> 00:24:06,200
不过没有在这篇论文里面

769
00:24:06,283 --> 00:24:08,533
详细的去进行讨论

770
00:24:08,766 --> 00:24:10,450
那我们现在中米觉得还蛮

771
00:24:10,450 --> 00:24:11,283
有意思的一个点

772
00:24:11,283 --> 00:24:12,483
就往下看一看

773
00:24:12,650 --> 00:24:14,600
整个 MOE 的加工里面

774
00:24:14,600 --> 00:24:17,400
随着我们的 MOE 的一个集群的规模增大

775
00:24:17,400 --> 00:24:20,083
也就是 ASP 增大和网络模型成增大

776
00:24:20,333 --> 00:24:21,966
真正增大还不是激活

777
00:24:21,966 --> 00:24:23,166
而是我们的权重

778
00:24:23,166 --> 00:24:25,400
所使用到的权重是越来越多的

779
00:24:25,400 --> 00:24:26,566
那另外一个报表

780
00:24:26,566 --> 00:24:27,766
就是里面的一个实验

781
00:24:27,766 --> 00:24:28,600
可以看到

782
00:24:28,883 --> 00:24:30,200
左边的就是我们的专家

783
00:24:30,200 --> 00:24:33,000
比较小右边的就是专家越来越多

784
00:24:33,083 --> 00:24:35,333
陈述网络模型程度相同的

785
00:24:35,333 --> 00:24:37,533
那这里面最耗时的

786
00:24:37,533 --> 00:24:39,566
或者我们的耗时增长的是什么

787
00:24:39,566 --> 00:24:41,400
耗时增长的是中间的这个

788
00:24:41,400 --> 00:24:43,766
ME dispatch and combine

789
00:24:44,050 --> 00:24:46,200
那 MOE distribution combined

790
00:24:46,200 --> 00:24:49,083
就意味着我中间的一个跨集群的

791
00:24:49,083 --> 00:24:51,200
通讯的成本在增加

792
00:24:51,250 --> 00:24:53,933
所以可以看到中间蓝色的这块

793
00:24:53,933 --> 00:24:56,250
随着我们的一个 MOE 的数量多

794
00:24:56,250 --> 00:24:58,733
我们的集群规模也要做分布式并行

795
00:24:59,000 --> 00:25:01,600
然后我们的通讯的成本也在增加

796
00:25:01,600 --> 00:25:03,133
而下面的这些

797
00:25:03,133 --> 00:25:04,650
其实是随着我们的矩阵成后

798
00:25:04,650 --> 00:25:06,683
我们矩阵的计算量的增加

799
00:25:06,850 --> 00:25:08,450
而逐步的增加

800
00:25:08,450 --> 00:25:10,133
而不是等比例的增加的

801
00:25:10,133 --> 00:25:11,533
这也是我们为什么说

802
00:25:11,533 --> 00:25:13,366
整个其他的 Transformer 的结构

803
00:25:13,966 --> 00:25:15,800
会做一个副本并行计算的原因

804
00:25:15,800 --> 00:25:18,200
说他增长的不是非常的夸张

805
00:25:18,200 --> 00:25:20,133
而是随着我们的网络模型层

806
00:25:20,200 --> 00:25:21,566
我们的一个 MOE

807
00:25:21,566 --> 00:25:22,483
你可以看到

808
00:25:22,533 --> 00:25:24,283
整体的 fighting layer 的计算量

809
00:25:24,283 --> 00:25:25,333
其实是差不多的

810
00:25:25,333 --> 00:25:28,083
更多是我们的通讯成本在增加

811
00:25:28,133 --> 00:25:29,333
每一个 s 跑过来

812
00:25:29,333 --> 00:25:30,050
每一个专家

813
00:25:30,050 --> 00:25:31,800
都在不同的卡上面去跑

814
00:25:31,800 --> 00:25:33,600
所以整体的 MOE 的计算量

815
00:25:33,600 --> 00:25:35,366
不会说急剧的膨胀

816
00:25:35,850 --> 00:25:38,200
那整篇论文的内容就到这里为止

817
00:25:38,200 --> 00:25:39,333
我们今天简单

818
00:25:39,333 --> 00:25:40,283
的分析一下

819
00:25:40,283 --> 00:25:42,166
巨鲨对然后这么已经非常的累

820
00:25:42,166 --> 00:25:43,050
PP 的客人在这

821
00:25:43,050 --> 00:25:44,333
夸克链接客人在这

822
00:25:44,650 --> 00:25:46,050
有兴趣的自己去看吧

823
00:25:46,050 --> 00:25:46,850
拜拜

824
00:25:47,333 --> 00:25:47,966
卷的不行

825
00:25:47,966 --> 00:25:48,933
卷的不行

826
00:25:48,933 --> 00:25:50,600
AI 系统的全套知识都在这里

827
00:25:50,600 --> 00:25:52,250
欢迎跟着目录进行学习

828
00:25:52,650 --> 00:25:53,566
给我一键三连

829
00:25:53,566 --> 00:25:54,766
给我一键三连

830
00:25:55,250 --> 00:25:55,933
谢谢各位

831
00:25:55,933 --> 00:25:56,966
拜个拜

