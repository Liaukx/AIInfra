1
00:00:00,000 --> 00:00:01,616
内容/录制:Z0MI 酱，视频剪辑/字幕:梁嘉铭

2
00:00:01,966 --> 00:00:02,733
hello 大家好

3
00:00:02,733 --> 00:00:03,733
我是 ZOMI

4
00:00:03,733 --> 00:00:06,400
今天会了后面的这一系列的视频

5
00:00:06,400 --> 00:00:08,600
ZOMI 希望跟大家一起去解读一下

6
00:00:08,600 --> 00:00:09,733
或者揭秘一下

7
00:00:09,766 --> 00:00:12,000
为什么 dipsic 会用 Moe

8
00:00:12,000 --> 00:00:13,966
这个混合专家的架构

9
00:00:16,733 --> 00:00:18,166
在这个视频开头之前

10
00:00:18,166 --> 00:00:19,766
我们首先要看几个问题

11
00:00:19,766 --> 00:00:21,733
为什么钟米会做这一系列

12
00:00:21,733 --> 00:00:23,166
或者这一期的视频

13
00:00:23,250 --> 00:00:25,533
就是因为有很多人去问钟米哎

14
00:00:25,533 --> 00:00:28,850
为什么换方的 Dipstick V3 和 I1 的模型

15
00:00:28,850 --> 00:00:30,850
能够把价格打得这么的低

16
00:00:30,883 --> 00:00:31,933
那第二个问题

17
00:00:31,933 --> 00:00:34,483
就是 dipstick 的一个 MOE 的架构

18
00:00:34,483 --> 00:00:37,133
它的主要的特性到底是哪些

19
00:00:37,250 --> 00:00:39,850
能够使得的算力利用率来打上去

20
00:00:39,850 --> 00:00:41,650
也就用了 2,000 的集群

21
00:00:41,683 --> 00:00:45,733
能够吊打 Meta 的 1.6 k 的拉马的这个集群

22
00:00:45,733 --> 00:00:48,800
那第三个就是 deep six MV 的架构

23
00:00:48,800 --> 00:00:49,650
会不会降低

24
00:00:49,650 --> 00:00:51,933
对训练和推理的算力的需求

25
00:00:51,933 --> 00:00:53,050
来带着这个疑问

26
00:00:53,050 --> 00:00:55,600
我们往下去看一下这一期视频

27
00:00:55,600 --> 00:00:56,733
或者这一系列的视频

28
00:00:56,733 --> 00:00:57,800
我们有哪些内容

29
00:00:58,133 --> 00:01:00,450
首先我们在这一期的视频里面

30
00:01:00,450 --> 00:01:02,083
重点去跟大家聊聊

31
00:01:02,450 --> 00:01:04,850
什么是 moe 这个架构

32
00:01:04,850 --> 00:01:06,133
什么是混合专家

33
00:01:06,600 --> 00:01:07,800
了解完凯文专家之后

34
00:01:07,800 --> 00:01:10,483
我们看一下整个混合专家的一个简史

35
00:01:10,483 --> 00:01:11,883
就他发展的历史

36
00:01:11,883 --> 00:01:15,400
从很久 1990 年伊藤的那篇文章到现在

37
00:01:15,400 --> 00:01:16,650
他是怎么发展的

38
00:01:16,850 --> 00:01:18,483
接着我们看一下第三个内容

39
00:01:18,483 --> 00:01:21,933
看一下混合专家对训练的影响有哪些

40
00:01:21,933 --> 00:01:24,733
我们到底要配多少专家才合理

41
00:01:24,733 --> 00:01:27,600
我们的训练到底应该怎么训才合适

42
00:01:27,650 --> 00:01:29,400
那有了相关的内容之后

43
00:01:29,400 --> 00:01:31,800
我们看一下对 MOE 这个架构

44
00:01:31,800 --> 00:01:33,683
让他训练跟推理起飞

45
00:01:33,683 --> 00:01:35,450
让他价格打下来的原

46
00:01:35,450 --> 00:01:37,733
因和相关的内容和特性

47
00:01:38,200 --> 00:01:38,650
那最后

48
00:01:38,650 --> 00:01:40,600
我们对整个产业进行思考

49
00:01:40,683 --> 00:01:44,450
为什么 dipstick 需要去用 MOE 这个架构

50
00:01:44,450 --> 00:01:47,450
为什么我们刚才的提的一系列的问题

51
00:01:47,450 --> 00:01:48,533
那带着这些疑问

52
00:01:48,533 --> 00:01:51,250
我们马上开始今天的内容

53
00:01:51,966 --> 00:01:53,966
那真正在开始今天的内容之前

54
00:01:53,966 --> 00:01:55,566
其实这一期

55
00:01:55,566 --> 00:01:57,683
钟敏说他是一系列的视频

56
00:01:57,800 --> 00:02:00,200
是因为我们现在是在第一个内容

57
00:02:00,200 --> 00:02:02,166
就是 moe 的基础介绍

58
00:02:02,200 --> 00:02:04,283
去了解我们相关的 moe 的内容

59
00:02:04,283 --> 00:02:06,683
我们将会在后面的一系列视频里面

60
00:02:06,683 --> 00:02:08,200
重点去挖坑的积蓄

61
00:02:08,450 --> 00:02:10,400
看一下 Moe 的前世今生

62
00:02:10,400 --> 00:02:11,850
很多相关的工作

63
00:02:11,850 --> 00:02:14,533
接着我们会打开 Moe 的核心的工作

64
00:02:14,533 --> 00:02:16,400
相关的论文进行解读

65
00:02:16,400 --> 00:02:17,050
解读完之后

66
00:02:17,050 --> 00:02:19,400
我们看一下 Moe 的架构的原理

67
00:02:19,400 --> 00:02:22,200
真正的把它里面最核心的模块

68
00:02:22,200 --> 00:02:23,800
里面的计算打开

69
00:02:23,883 --> 00:02:25,250
那有了这些基础之后

70
00:02:25,250 --> 00:02:27,083
我们就看一下最近特别火的 DPC

71
00:02:27,083 --> 00:02:30,000
还有 Mr 各种各样的模型都用了 Moe

72
00:02:30,000 --> 00:02:31,200
到底是怎么用的

73
00:02:31,200 --> 00:02:32,850
它们里面有哪些区别

74
00:02:32,850 --> 00:02:35,133
到底是小参数量多专家

75
00:02:35,133 --> 00:02:37,400
还是大参数量少专家的模式

76
00:02:37,400 --> 00:02:38,933
它们之间又有什么区别

77
00:02:39,283 --> 00:02:40,883
那了解完一系列的原理之后

78
00:02:40,883 --> 00:02:42,050
我们现在要上代码了

79
00:02:42,050 --> 00:02:42,883
上代码了

80
00:02:42,933 --> 00:02:45,650
真正的去手撕 moe 的代码

81
00:02:45,650 --> 00:02:47,650
去打开里面的 moe 的结构

82
00:02:47,683 --> 00:02:48,800
自己去撸一遍

83
00:02:48,800 --> 00:02:50,766
钟敏带着大家一起去撸一遍

84
00:02:50,766 --> 00:02:53,533
来了解 Moe 到底是怎么实现的

85
00:02:53,533 --> 00:02:55,333
所以我们看一下 moe 的未来

86
00:02:55,333 --> 00:02:56,850
看一下 vision moe

87
00:02:56,850 --> 00:03:00,366
还有多模态的 Moe 到底有哪些不一样

88
00:03:02,000 --> 00:03:03,366
马上来到了第一个内容

89
00:03:03,366 --> 00:03:05,333
去了解一下什么是 Moe

90
00:03:05,400 --> 00:03:08,766
Mr off Asper 混合专家模型

91
00:03:08,766 --> 00:03:11,450
而不是 Multi off asper 就不是多专家

92
00:03:11,450 --> 00:03:12,733
是混合专家

93
00:03:13,000 --> 00:03:13,450
那首先

94
00:03:13,450 --> 00:03:14,400
我们来看一下

95
00:03:14,400 --> 00:03:16,250
moe 的主要的组成部分

96
00:03:16,250 --> 00:03:20,083
有两个第一个就是稀疏的 Moe 层

97
00:03:20,250 --> 00:03:21,400
moe 层它是个稀疏的

98
00:03:21,400 --> 00:03:23,083
所以用了 moe 的模型

99
00:03:23,083 --> 00:03:24,250
或者用了 moe 层

100
00:03:24,250 --> 00:03:26,133
我们都叫做稀疏的模型

101
00:03:26,166 --> 00:03:28,200
那最重要的就是 moe 层

102
00:03:28,200 --> 00:03:29,333
就直接代替了

103
00:03:29,400 --> 00:03:32,200
传统全塑网架构里面的 FFN 层

104
00:03:32,400 --> 00:03:35,966
那 Moe 层就包含很多个专家

105
00:03:35,966 --> 00:03:37,133
每个专家自己

106
00:03:37,133 --> 00:03:38,933
本身是一个独立的模型

107
00:03:38,933 --> 00:03:41,333
我们看一下左边的这个模块

108
00:03:41,333 --> 00:03:42,400
左边这一坨

109
00:03:42,400 --> 00:03:45,933
其实就是可以当成一个全缩码的 decoder

110
00:03:45,933 --> 00:03:47,333
我们有 self attention

111
00:03:47,333 --> 00:03:48,000
有罗马莱西选

112
00:03:48,000 --> 00:03:49,333
然后有 FFN 层

113
00:03:49,400 --> 00:03:50,250
有罗马莱 c 选

114
00:03:50,250 --> 00:03:53,083
成这是一个完整的全缩码的架构

115
00:03:53,083 --> 00:03:53,933
那这里面

116
00:03:53,933 --> 00:03:55,566
就把 FFN 层

117
00:03:55,966 --> 00:03:58,600
这里面代替成为我们的专家

118
00:03:58,600 --> 00:04:00,966
这里面一个 FFN 就是一个专家

119
00:04:00,966 --> 00:04:03,333
一个 FN 又是另外一个专家

120
00:04:03,333 --> 00:04:06,166
所以我们把多专家 Moe 的架构

121
00:04:06,166 --> 00:04:10,050
代替了传统全梭门里面的 FFN 层

122
00:04:10,133 --> 00:04:12,333
所以这里面是第一个关键内容

123
00:04:12,800 --> 00:04:13,933
第二个关键内容

124
00:04:13,933 --> 00:04:16,400
就是门控的网络或者路由

125
00:04:16,400 --> 00:04:18,133
那我们现在都叫做 working

126
00:04:18,133 --> 00:04:19,166
都叫做路由了

127
00:04:19,166 --> 00:04:22,600
门控网络是应该 17 年之前的一个叫法

128
00:04:22,600 --> 00:04:24,483
那随着我们的学术的引进

129
00:04:24,483 --> 00:04:26,850
现在名字基本上都叫路由了

130
00:04:26,850 --> 00:04:28,966
那这个门控网络或者路由

131
00:04:28,966 --> 00:04:29,733
最重要的作用

132
00:04:29,733 --> 00:04:30,000
就是

133
00:04:30,000 --> 00:04:34,600
决定哪些 TOKEN 发送到哪个专家去用的

134
00:04:35,333 --> 00:04:39,133
例如我们现在要处理 Moe 这么一个 TOKEN

135
00:04:39,133 --> 00:04:39,566
那当然了

136
00:04:39,566 --> 00:04:41,450
它真正的丢到模型里面

137
00:04:41,450 --> 00:04:42,850
它变成一个项链了

138
00:04:42,850 --> 00:04:44,050
那 Ver f 我们现在

139
00:04:44,050 --> 00:04:45,366
把它当成一个 TOKEN

140
00:04:45,366 --> 00:04:46,933
我处理这个 MO 的时候

141
00:04:46,933 --> 00:04:48,000
有可能哦

142
00:04:48,000 --> 00:04:48,933
下面全说嘛

143
00:04:48,933 --> 00:04:49,883
基本的内容

144
00:04:49,883 --> 00:04:50,800
基本的模块的

145
00:04:50,800 --> 00:04:51,883
探选的模块

146
00:04:51,883 --> 00:04:52,850
都是一样的

147
00:04:52,850 --> 00:04:55,083
到了那个路由的模块之后

148
00:04:55,083 --> 00:04:57,933
有可能 MO 这一个 TOKEN 或它的 Vector

149
00:04:57,933 --> 00:05:01,366
就路由到 F F N 二里面进行处理

150
00:05:01,366 --> 00:05:03,766
当然有可能我们的 parmet

151
00:05:03,800 --> 00:05:04,766
另外一个 TOKEN

152
00:05:04,766 --> 00:05:06,400
另外一个参数

153
00:05:06,400 --> 00:05:07,850
或我们另外一个项量了

154
00:05:07,850 --> 00:05:10,650
就路由到 F F N 1 进行处理

155
00:05:10,650 --> 00:05:13,766
当然我们可以看一下这里面的 F F N 1 2

156
00:05:13,766 --> 00:05:15,450
这里面也是 F312

157
00:05:15,450 --> 00:05:16,933
我们的每一个参数

158
00:05:16,933 --> 00:05:18,966
都有可能陆游到不同的专家

159
00:05:18,966 --> 00:05:19,483
这里面的

160
00:05:19,483 --> 00:05:21,766
陆游的作用也就门控网络的作用

161
00:05:21,850 --> 00:05:24,050
就是决定我们那些托肯

162
00:05:24,050 --> 00:05:26,333
发送到哪个专家进行处理的

163
00:05:26,333 --> 00:05:28,000
那有了这两个模块之后

164
00:05:28,000 --> 00:05:29,883
我们就认为这个网络模型

165
00:05:29,883 --> 00:05:32,333
就是一个稀疏的 MOE 的架构

166
00:05:32,566 --> 00:05:33,766
不过这个 MV 的加固

167
00:05:33,766 --> 00:05:36,200
其实也有蛮多的一些挑战的

168
00:05:36,200 --> 00:05:37,566
那在训练的过程当中

169
00:05:37,566 --> 00:05:38,650
我们可以看到

170
00:05:38,733 --> 00:05:39,766
M1 说实话

171
00:05:39,766 --> 00:05:42,800
它能够实现更高效的训练的效率

172
00:05:42,966 --> 00:05:44,566
但是在微调阶段

173
00:05:44,566 --> 00:05:47,050
经常会面临我们的泛化能力不足

174
00:05:47,050 --> 00:05:49,166
很容易引起过敏和现象

175
00:05:49,166 --> 00:05:50,766
然后预训练的过程中

176
00:05:50,766 --> 00:05:51,733
比较难收敛

177
00:05:51,733 --> 00:05:54,400
也就是为什么 Moe 的架构

178
00:05:54,400 --> 00:05:56,333
有了大模型这么久以后

179
00:05:56,333 --> 00:05:58,366
慢慢的才出现了一些稀疏

180
00:05:58,366 --> 00:05:59,800
以前我们都是稠密的

181
00:05:59,800 --> 00:06:01,683
为什么 DPC 出现之后

182
00:06:01,683 --> 00:06:02,483
就发现哎呦

183
00:06:02,483 --> 00:06:03,966
Moe 的架构真牛逼

184
00:06:04,250 --> 00:06:06,933
是因为整体的训练的过程当中

185
00:06:06,933 --> 00:06:08,166
嗯 deepsick

186
00:06:08,166 --> 00:06:09,766
处理了很多的问题

187
00:06:09,766 --> 00:06:12,250
也解决了整个 MOE 架构训练

188
00:06:12,250 --> 00:06:13,733
难以收敛的问题

189
00:06:13,733 --> 00:06:15,283
接着我们看一下第二点

190
00:06:15,366 --> 00:06:16,966
推理的挑战

191
00:06:17,083 --> 00:06:17,366
虽然

192
00:06:17,366 --> 00:06:20,366
我们 MOE 有很大量的一些参数了

193
00:06:20,366 --> 00:06:22,650
那 deep sick 的一个 MOE

194
00:06:22,766 --> 00:06:26,650
第三有 671 币这么大的一个参数量

195
00:06:26,650 --> 00:06:28,566
但是因为在推理的过程当中

196
00:06:28,566 --> 00:06:30,366
只激活了其中一部分参数

197
00:06:30,366 --> 00:06:31,966
网上说好像 38 币

198
00:06:31,966 --> 00:06:35,050
看它的参数量只有 32 还是 38 币

199
00:06:35,050 --> 00:06:36,000
那这样的话

200
00:06:36,050 --> 00:06:37,650
使得整个推理的速度

201
00:06:37,650 --> 00:06:39,083
就变得非常的快了

202
00:06:39,083 --> 00:06:39,800
对比起

203
00:06:39,800 --> 00:06:42,483
具有相同参数量的一个同音模型

204
00:06:42,966 --> 00:06:44,650
不过 Moe

205
00:06:44,650 --> 00:06:45,966
他需要把所有的

206
00:06:45,966 --> 00:06:49,883
也就是 670 亿币这么大的一个参数量

207
00:06:49,883 --> 00:06:51,800
都塞到我们的内存里面

208
00:06:51,800 --> 00:06:53,050
也就是 HBM 里面

209
00:06:53,050 --> 00:06:56,050
因此对整个 HBM 的需求量很高

210
00:06:56,133 --> 00:06:57,883
所以为什么现在换方

211
00:06:57,883 --> 00:07:00,650
他现在公布的一个技术文章里面

212
00:07:00,650 --> 00:07:03,050
就说了我在推理的时候

213
00:07:03,050 --> 00:07:05,000
分为 prefume 跟 decode 阶段

214
00:07:05,000 --> 00:07:06,450
也就是 PD 分离

215
00:07:06,450 --> 00:07:07,933
但是低的过程当中

216
00:07:07,933 --> 00:07:10,133
用了 40 台 H800 呀

217
00:07:10,133 --> 00:07:12,366
整体它使用的硬件资源

218
00:07:12,366 --> 00:07:13,683
是非常的夸张的

219
00:07:13,683 --> 00:07:16,850
因为它需要大量的显存去存下来

220
00:07:17,333 --> 00:07:18,566
看一个具体例子

221
00:07:18,566 --> 00:07:22,200
就是隐秘传 8 乘以 7B 这个 MV 模型架构

222
00:07:22,200 --> 00:07:24,883
说实话 8 乘以 7B 这个 m 模型架构

223
00:07:24,933 --> 00:07:25,650
基本上

224
00:07:25,650 --> 00:07:29,883
是要使用两台或者两张卡来去跑的

225
00:07:29,883 --> 00:07:30,650
那为什么

226
00:07:30,650 --> 00:07:31,450
是因为呃

227
00:07:31,450 --> 00:07:33,566
你觉得 7B 的模型不是很小吗

228
00:07:33,566 --> 00:07:35,800
但是我们在悉数的架构里面

229
00:07:35,800 --> 00:07:37,000
需要有足够的内存

230
00:07:37,000 --> 00:07:39,800
来容纳下 78B 的一个参数量

231
00:07:39,800 --> 00:07:41,283
那 78B 的参数量

232
00:07:41,283 --> 00:07:41,933
实际上

233
00:07:41,933 --> 00:07:43,650
对应我们的硬盘内存

234
00:07:43,650 --> 00:07:45,250
大概是 30G 左右

235
00:07:45,250 --> 00:07:45,933
那这里面

236
00:07:45,933 --> 00:07:47,050
有人就会问了

237
00:07:47,083 --> 00:07:48,250
苏明老师你好

238
00:07:48,333 --> 00:07:49,933
为什么是 47B

239
00:07:50,083 --> 00:07:52,333
8 乘以 7 不是 56 吗

240
00:07:52,600 --> 00:07:54,533
哎 47 币是怎么来的

241
00:07:54,533 --> 00:07:54,850
实际上

242
00:07:54,850 --> 00:07:58,450
我们看一下刚才的一个 MOE 的架构

243
00:07:58,450 --> 00:08:00,000
那这里面我们可以看到

244
00:08:00,000 --> 00:08:04,166
self attention 是所有的专家都共享的参数

245
00:08:04,200 --> 00:08:04,966
i 跟罗姆莱

246
00:08:04,966 --> 00:08:06,933
c 群是所有专家共享的参数

247
00:08:06,933 --> 00:08:10,166
我们字里面这个 FFN 二这个专家

248
00:08:10,166 --> 00:08:12,650
有可能他是 7 b f f n e

249
00:08:12,650 --> 00:08:14,283
这个专家是 7B

250
00:08:14,283 --> 00:08:16,283
那我们有 8 个专家

251
00:08:16,283 --> 00:08:17,333
所以 7 乘以 8

252
00:08:17,333 --> 00:08:19,850
但是我们有很多的参数量

253
00:08:19,850 --> 00:08:21,050
是共享的

254
00:08:21,800 --> 00:08:23,766
所以真正需要的参数量

255
00:08:23,766 --> 00:08:26,250
就不是 8 乘以 7 等于 56

256
00:08:26,250 --> 00:08:28,766
而是我们节省了一些共享的参数

257
00:08:28,766 --> 00:08:31,050
因此只有 47 币

258
00:08:31,133 --> 00:08:32,283
朱明老师你好

259
00:08:32,283 --> 00:08:33,366
你说共享

260
00:08:33,366 --> 00:08:35,683
那到底是怎么去共享

261
00:08:36,283 --> 00:08:37,766
哎这个问题问的挺好的

262
00:08:37,766 --> 00:08:38,566
小新同学

263
00:08:38,566 --> 00:08:39,483
那我们现在

264
00:08:39,483 --> 00:08:40,683
假设每个托卡

265
00:08:40,683 --> 00:08:42,566
都只使用了两个专家

266
00:08:42,566 --> 00:08:43,850
那推理的速度时候

267
00:08:43,850 --> 00:08:46,200
其实类似于使用 12B 的模型

268
00:08:46,400 --> 00:08:47,800
这里面不是 14B

269
00:08:47,850 --> 00:08:50,600
因为我们有一些相关的参数共享了

270
00:08:50,650 --> 00:08:51,133
虽然

271
00:08:51,133 --> 00:08:54,083
我们进行了 2 乘以 7B 的矩阵的乘法运算

272
00:08:54,166 --> 00:08:55,850
但是 MO1 长了

273
00:08:56,000 --> 00:08:58,283
通过通讯来实现参数共享

274
00:08:58,283 --> 00:09:00,200
而并非重复的计算

275
00:09:00,250 --> 00:09:02,483
那我们还是回到这个图里面

276
00:09:02,483 --> 00:09:05,533
因为我们的 self attention 是只算一次

277
00:09:05,533 --> 00:09:06,933
那真正如果不够算快

278
00:09:06,933 --> 00:09:08,050
一张卡放不下

279
00:09:08,250 --> 00:09:09,883
我们会对这个 attention 层

280
00:09:09,883 --> 00:09:11,850
进行一个张量并行

281
00:09:11,933 --> 00:09:13,533
也就是探测并行了

282
00:09:13,533 --> 00:09:14,850
或者叫做模型并行

283
00:09:14,850 --> 00:09:16,600
把它切成两张卡

284
00:09:16,600 --> 00:09:18,133
那切完两张卡之后

285
00:09:18,166 --> 00:09:20,966
NPU1 就只算左边的这部分

286
00:09:21,050 --> 00:09:23,000
NPU2 只算右边这部分

287
00:09:23,000 --> 00:09:23,683
算完之后

288
00:09:23,683 --> 00:09:25,450
把 N1 跟 N2 的结果

289
00:09:25,450 --> 00:09:27,533
进行一个通讯就得到了

290
00:09:27,533 --> 00:09:29,166
我们不需要重复的计算

291
00:09:29,200 --> 00:09:31,450
但是对于 FFN2

292
00:09:31,450 --> 00:09:32,200
这个专家

293
00:09:32,200 --> 00:09:34,483
我们是要单独计算它的内容的

294
00:09:34,483 --> 00:09:35,483
对于 FFN1

295
00:09:35,483 --> 00:09:37,533
我们也需要单独计算的

296
00:09:39,283 --> 00:09:39,766
因此

297
00:09:39,766 --> 00:09:42,333
通过这种方式进行一个共享

298
00:09:42,333 --> 00:09:44,650
通过我们的网络通讯进行共享

299
00:09:44,650 --> 00:09:46,483
而并非重复的计算哦

300
00:09:47,050 --> 00:09:50,650
哎呦了解完什么是 MOE 架构

301
00:09:50,650 --> 00:09:52,650
就是 MOE 这个架构到底代表是什么

302
00:09:52,650 --> 00:09:54,850
它的一个简单的原理之后

303
00:09:54,850 --> 00:09:56,483
我们现在来到了第二个内容

304
00:09:56,483 --> 00:09:59,600
看一下 MOE 的混合专家的简史

305
00:09:59,600 --> 00:10:02,000
说实话去看一下 MOE 架构的发展历史

306
00:10:02,400 --> 00:10:03,250
那最重要

307
00:10:03,250 --> 00:10:05,283
为什么说现在的伊藤的影响力

308
00:10:05,283 --> 00:10:07,483
会比杨乐川还有其他几个要高

309
00:10:07,650 --> 00:10:08,133
是因为

310
00:10:08,133 --> 00:10:09,450
在 82 年的时候

311
00:10:09,450 --> 00:10:11,683
伊藤发明了反向传播之后

312
00:10:11,733 --> 00:10:15,400
后面提出 MV 架构的也是 Hitter

313
00:10:15,400 --> 00:10:17,200
所以非常的牛逼

314
00:10:17,200 --> 00:10:18,733
那这个 MV 的架构

315
00:10:18,733 --> 00:10:20,766
在 1991 年的时候

316
00:10:20,800 --> 00:10:24,083
由 Hitter 跟桌等两位大神

317
00:10:24,083 --> 00:10:26,050
联合来去发布的

318
00:10:26,050 --> 00:10:28,483
那在整个 MV 的架构里面

319
00:10:28,483 --> 00:10:29,566
就提出了一个

320
00:10:29,566 --> 00:10:31,650
跟集成学习的方法比较相似

321
00:10:31,800 --> 00:10:34,450
为很多个单独的一个网络模型

322
00:10:34,450 --> 00:10:36,883
组成一个整体的监控的机制

323
00:10:36,883 --> 00:10:38,000
也就是监控的系统

324
00:10:38,000 --> 00:10:42,050
反正把很多个 ASP 专家集合起来

325
00:10:42,250 --> 00:10:44,083
然后网络模型架构当中

326
00:10:44,083 --> 00:10:45,250
为每一个专家

327
00:10:45,250 --> 00:10:46,600
这里面有三个专家

328
00:10:46,650 --> 00:10:49,166
去处理不同的训练的数据集

329
00:10:49,166 --> 00:10:50,366
或者样本的数据集

330
00:10:50,450 --> 00:10:53,000
专门去专注于特定的空间

331
00:10:53,000 --> 00:10:54,400
或特定的计算

332
00:10:54,683 --> 00:10:56,483
那到底我们对于输入的数据

333
00:10:56,483 --> 00:10:58,250
到底选哪个专家

334
00:10:58,250 --> 00:11:01,566
那这里面右边就提出了一个门控网络

335
00:11:01,600 --> 00:11:02,366
那现在都

336
00:11:02,366 --> 00:11:03,366
叫做路由了

337
00:11:03,366 --> 00:11:04,766
那通过这个门控网络

338
00:11:04,766 --> 00:11:07,533
来决定到底分配给哪个专家

339
00:11:07,566 --> 00:11:09,133
进行一个计算的

340
00:11:09,200 --> 00:11:11,683
在网络模型训练的过程当中

341
00:11:11,683 --> 00:11:13,566
我们的门控网络

342
00:11:13,566 --> 00:11:16,766
跟我们的专家一起进行学习的

343
00:11:16,766 --> 00:11:19,083
来优化整体的模型的性能

344
00:11:19,083 --> 00:11:20,366
和决策的能力

345
00:11:20,366 --> 00:11:20,850
那这个

346
00:11:20,850 --> 00:11:24,000
就是最早期的这篇论文所讲的内容

347
00:11:24,050 --> 00:11:24,966
那么现在

348
00:11:25,400 --> 00:11:26,850
来回顾一下历史上

349
00:11:26,850 --> 00:11:30,533
对 MV 架构产生重大影响的一些文献

350
00:11:30,533 --> 00:11:32,083
那刚才那篇文章

351
00:11:32,083 --> 00:11:35,166
是 91 年的时候由伊藤提出来的

352
00:11:35,166 --> 00:11:38,000
为整个 MOE 架构奠定了整体的基础

353
00:11:38,000 --> 00:11:39,600
我们现在来看一下第二个

354
00:11:39,600 --> 00:11:42,083
就是 94 年的这篇文章

355
00:11:42,083 --> 00:11:44,850
通过 EM 期望最大化的算法

356
00:11:44,850 --> 00:11:46,566
来进行一个训练

357
00:11:46,600 --> 00:11:49,400
使得整个 hittern 这个提出的架构

358
00:11:49,400 --> 00:11:51,883
能够训练出来一个比较合理的

359
00:11:51,883 --> 00:11:53,450
比较好的效果

360
00:11:53,566 --> 00:11:57,000
但是中间因为在 94 年到 17 年之间

361
00:11:57,000 --> 00:11:58,133
你会发现哎

362
00:11:58,133 --> 00:11:59,200
断层了蛮久的

363
00:11:59,200 --> 00:12:00,800
这段时间大家都干嘛了

364
00:12:00,850 --> 00:12:02,933
都在研究互联网的配曲令

365
00:12:02,933 --> 00:12:05,400
等机器学习的算法

366
00:12:05,400 --> 00:12:07,083
直到 17 年的时候

367
00:12:07,083 --> 00:12:08,966
深度学习或者一二年之后

368
00:12:08,966 --> 00:12:10,450
深度学习又回来了

369
00:12:10,733 --> 00:12:12,450
于是到 17 年的时候

370
00:12:12,450 --> 00:12:13,600
就有专家呀

371
00:12:13,650 --> 00:12:14,683
重新提出了

372
00:12:14,683 --> 00:12:17,883
这个稀疏 gate 的一个 moe 的架构

373
00:12:17,966 --> 00:12:19,000
通过新的方式

374
00:12:19,000 --> 00:12:22,200
来实现大规模的模型的推理

375
00:12:22,200 --> 00:12:24,483
那到了 2020 年的时候

376
00:12:24,483 --> 00:12:27,600
蛮有意思的就是谷歌提出了 g shot

377
00:12:27,850 --> 00:12:29,533
首次将 MA 的架构

378
00:12:29,533 --> 00:12:32,200
引入到他的一个全松板架构里面

379
00:12:32,200 --> 00:12:34,566
提升了分布式计算的能力

380
00:12:34,566 --> 00:12:36,533
到了 21 年的时候

381
00:12:36,533 --> 00:12:39,966
又有了另外一篇门将叫做 GLAM

382
00:12:39,966 --> 00:12:41,733
为什么叫 g 开头

383
00:12:41,966 --> 00:12:43,850
因为是谷歌去发明的嘛

384
00:12:43,850 --> 00:12:44,800
谷歌搞的

385
00:12:44,800 --> 00:12:45,600
所以谷歌都

386
00:12:45,600 --> 00:12:46,366
加了一个居

387
00:12:46,366 --> 00:12:47,683
上面的这篇文章

388
00:12:47,683 --> 00:12:50,200
最重要的就是讲利用了 MV 的架构

389
00:12:50,200 --> 00:12:52,933
在 NLP 自然原领域里面

390
00:12:52,933 --> 00:12:53,483
所以这里面

391
00:12:53,483 --> 00:12:54,766
叫做 language model

392
00:12:54,800 --> 00:12:58,083
去实现了一个真正的非常好的效果

393
00:12:58,333 --> 00:12:59,650
那到了 21 年的时候

394
00:12:59,650 --> 00:13:00,766
朱敏觉得这篇文章

395
00:13:00,766 --> 00:13:02,250
特别的重要

396
00:13:02,250 --> 00:13:03,800
就是 speech 抢说嘛

397
00:13:03,933 --> 00:13:07,850
首次提出了一个超大规模的 M1 的架构

398
00:13:07,850 --> 00:13:10,333
来去提升网络模型的效果

399
00:13:10,333 --> 00:13:12,250
比 Glam 更好

400
00:13:12,400 --> 00:13:12,966
那接着

401
00:13:12,966 --> 00:13:14,450
到 22 年 到 23 年

402
00:13:14,450 --> 00:13:16,966
就不断的有新的网络模型

403
00:13:16,966 --> 00:13:18,400
和新的内容出现了

404
00:13:18,400 --> 00:13:19,933
包括拉玛的 MOA

405
00:13:19,966 --> 00:13:23,333
直到我们现在最轰动的也就 2024 年

406
00:13:23,366 --> 00:13:26,133
deep sick 用了 MOA 的架构

407
00:13:26,166 --> 00:13:28,683
引入了细腻度的垂直的专家

408
00:13:28,683 --> 00:13:30,683
也就是参数小

409
00:13:30,683 --> 00:13:33,533
多专家的方式和共享专家的概念

410
00:13:33,533 --> 00:13:36,933
提升了整个专家在大圆模型的能力

411
00:13:36,933 --> 00:13:37,850
那因此

412
00:13:37,850 --> 00:13:39,566
整个 MOE 的架构

413
00:13:39,566 --> 00:13:41,400
才被现在很多人所关注

414
00:13:41,400 --> 00:13:42,000
也是为什么

415
00:13:42,000 --> 00:13:44,883
钟敏做这一期视频的一个原因

416
00:13:46,450 --> 00:13:46,933
那接着

417
00:13:46,933 --> 00:13:49,083
我们刚才回顾了一下历史

418
00:13:49,083 --> 00:13:51,850
当然前面的这一些文章

419
00:13:51,850 --> 00:13:53,650
或者最核心的这几篇文章

420
00:13:53,650 --> 00:13:55,850
钟敏会在后面的内容里面

421
00:13:55,966 --> 00:13:59,483
深度的跟大家一起组读对应的 paper

422
00:13:59,483 --> 00:14:02,050
也希望大家多给钟敏一点意见哦

423
00:14:02,250 --> 00:14:04,650
因为作品是纯粹为爱发电的

424
00:14:04,650 --> 00:14:07,366
所以说还是希望做一些知识的共享

425
00:14:07,366 --> 00:14:09,483
希望大家多给点支持

426
00:14:09,566 --> 00:14:10,400
那接着

427
00:14:10,400 --> 00:14:11,200
我们看一下

428
00:14:11,200 --> 00:14:14,366
近期发布的一个 moe 的大模型哦

429
00:14:14,366 --> 00:14:15,966
也是从 23 年开始

430
00:14:15,966 --> 00:14:17,800
你会发现从 GBT4

431
00:14:17,966 --> 00:14:20,333
当然 GP4 到底是不是用 moe

432
00:14:20,333 --> 00:14:22,000
其实大家没有一个定论的

433
00:14:22,000 --> 00:14:23,600
只是不断的有人爆料

434
00:14:23,600 --> 00:14:24,933
不过怎么样

435
00:14:24,933 --> 00:14:27,000
miss 闯 8 乘以 7B 这个模型

436
00:14:27,000 --> 00:14:29,566
用了 MO 的架构非常的火

437
00:14:29,566 --> 00:14:30,766
而且还开源

438
00:14:30,766 --> 00:14:33,083
后来拉玛有自己的 Moe

439
00:14:33,083 --> 00:14:34,000
然后换方了

440
00:14:34,000 --> 00:14:36,650
在 24 年的 1 月份出现了自己的 moe

441
00:14:36,650 --> 00:14:38,483
从此一发不可收拾哦

442
00:14:38,733 --> 00:14:41,650
不管是 Google 千问还有 data bricks

443
00:14:41,650 --> 00:14:42,600
还有 mission 自己的

444
00:14:42,600 --> 00:14:45,650
还有微软的还有 Snowflake 的

445
00:14:45,650 --> 00:14:47,966
到特斯拉的 XAI

446
00:14:48,283 --> 00:14:49,966
直到 Dipstick V3

447
00:14:50,050 --> 00:14:53,050
这个模型的效果真正的凸显出来了

448
00:14:53,050 --> 00:14:53,566
因此

449
00:14:53,566 --> 00:14:56,283
Dipstick V3 被现在所有人都所熟悉

450
00:14:56,400 --> 00:14:57,483
那另外的话

451
00:14:57,483 --> 00:14:58,533
国内的六小虎

452
00:14:58,533 --> 00:15:01,683
Minimax 还有千万也用了 MV 的架构

453
00:15:01,683 --> 00:15:03,650
看来 Moe 的架构

454
00:15:03,650 --> 00:15:05,083
是 24 年年底

455
00:15:05,400 --> 00:15:08,533
或者 25 年的一个很重要的趋势

456
00:15:08,566 --> 00:15:10,000
未来我们的多模态

457
00:15:10,000 --> 00:15:11,800
会不会用上 Moe 的架构

458
00:15:11,800 --> 00:15:13,733
让我们拭目以待哦

459
00:15:13,733 --> 00:15:15,650
不过多模态用 Moe

460
00:15:15,650 --> 00:15:18,766
我们将会在后面详细的展开

461
00:15:18,800 --> 00:15:22,883
什么叫做 Vmoe vision Moe 的新的架构体系

462
00:15:23,566 --> 00:15:24,966
哎呀歇一会歇一会

463
00:15:24,966 --> 00:15:26,283
录课是很难的

464
00:15:26,283 --> 00:15:27,083
要看论文

465
00:15:27,083 --> 00:15:28,133
要梳理 PPT

466
00:15:28,133 --> 00:15:29,400
还得去讲

467
00:15:29,966 --> 00:15:32,133
终于这两天已经录得有点恶心了

468
00:15:38,400 --> 00:15:41,333
我们现在还是要坚持的完成这个视频

469
00:15:41,333 --> 00:15:43,883
我们现在来到了第三个内容

470
00:15:43,883 --> 00:15:45,450
看一下 M1 混合专家

471
00:15:45,450 --> 00:15:47,850
对训练相关的一些影响

472
00:15:48,400 --> 00:15:50,600
或者训练对 MO 的影响

473
00:15:50,600 --> 00:15:52,683
首先我们了解一下什么是稀疏性哦

474
00:15:52,966 --> 00:15:54,083
稠密的大模型

475
00:15:54,083 --> 00:15:55,683
那意味着我们的网络模型里面

476
00:15:55,683 --> 00:15:57,133
所有的参数

477
00:15:57,133 --> 00:15:59,250
都会对我们的输入的数据 x

478
00:15:59,250 --> 00:15:59,933
进行处理

479
00:15:59,933 --> 00:16:02,000
也就是 y 等于 a

480
00:16:02,050 --> 00:16:03,200
x 加 b

481
00:16:03,483 --> 00:16:06,400
或者 a w 加 b 这种方式来去计算的

482
00:16:06,400 --> 00:16:07,200
那稀疏

483
00:16:07,200 --> 00:16:10,133
就是允许我们模型当中的某些部分

484
00:16:10,133 --> 00:16:12,250
也是所谓的专家执行计算

485
00:16:12,250 --> 00:16:13,600
有些专家可以执行计算

486
00:16:13,600 --> 00:16:15,800
有些专家可以不执行计算

487
00:16:15,800 --> 00:16:17,283
因此我们叫做稀疏

488
00:16:17,366 --> 00:16:20,000
跟传统的稀疏是不一致一个概念的

489
00:16:20,000 --> 00:16:22,166
就我们网络模型的参数量部分

490
00:16:22,166 --> 00:16:24,083
有稀疏不是一个意思哦

491
00:16:24,400 --> 00:16:25,733
在 ME 加工里面

492
00:16:25,733 --> 00:16:27,083
不是所有的参数

493
00:16:27,083 --> 00:16:30,450
都会参与我们的输入 x 进行计算的

494
00:16:30,450 --> 00:16:31,800
而是根据陆游

495
00:16:31,800 --> 00:16:34,050
去选择什么时候被激活什么

496
00:16:34,050 --> 00:16:35,566
去使用我们的专家

497
00:16:35,566 --> 00:16:36,166
那这里面

498
00:16:36,166 --> 00:16:36,966
更重要的是

499
00:16:36,966 --> 00:16:38,400
根据我们的数的特征

500
00:16:38,450 --> 00:16:40,566
去选择我们的专家

501
00:16:40,566 --> 00:16:42,250
这也是陆游的作用

502
00:16:42,250 --> 00:16:43,250
那实际上

503
00:16:43,450 --> 00:16:46,283
我们看一下这里面有两个问题

504
00:16:46,283 --> 00:16:47,000
第一个问题

505
00:16:47,000 --> 00:16:49,966
我们的 batch 就是我们的输入的数据

506
00:16:50,050 --> 00:16:51,600
如果大小不均衡

507
00:16:51,600 --> 00:16:52,366
有可能

508
00:16:52,366 --> 00:16:54,133
会导致我们专家

509
00:16:54,133 --> 00:16:56,283
或者真正在陆游的过程当中

510
00:16:56,366 --> 00:16:58,600
资源利用率并不高

511
00:16:59,766 --> 00:17:02,333
因此为了解决这些问题

512
00:17:02,450 --> 00:17:04,333
于是业界就发明了

513
00:17:04,333 --> 00:17:06,683
或者去重点解决两个问题

514
00:17:06,683 --> 00:17:09,483
第一个就是可学习的门控网络

515
00:17:09,483 --> 00:17:11,966
可学习的 voting 路由器

516
00:17:12,000 --> 00:17:13,733
第二个就是专家之间

517
00:17:13,733 --> 00:17:15,133
要进行一个负载

518
00:17:15,133 --> 00:17:17,483
去解决我们训练不公平

519
00:17:17,483 --> 00:17:19,650
或者我们的专家之间负载不公平

520
00:17:19,650 --> 00:17:22,283
我们的算力不均匀的问题

521
00:17:23,250 --> 00:17:25,533
为了解决这两个问题

522
00:17:25,533 --> 00:17:28,083
谷歌的书写就对 Moe

523
00:17:28,083 --> 00:17:30,250
其实在 NLP 的翻译过程当中

524
00:17:30,250 --> 00:17:32,283
就引入了条件计算

525
00:17:32,533 --> 00:17:33,650
那所谓的条件计算

526
00:17:33,650 --> 00:17:35,800
就是我们把 gating 的网络

527
00:17:35,800 --> 00:17:36,733
变成一个路由

528
00:17:36,733 --> 00:17:38,683
在每个样本的基础上面

529
00:17:38,683 --> 00:17:41,566
去激活我们的一个具体的 Esper

530
00:17:41,733 --> 00:17:43,483
那不增加额外的情况下

531
00:17:43,483 --> 00:17:45,483
去扩展整个 MA 的架构

532
00:17:45,533 --> 00:17:48,566
使得整个 MOE 能够实现更多的专家

533
00:17:48,933 --> 00:17:50,333
专家的利用率当

534
00:17:50,333 --> 00:17:51,883
时这篇文章

535
00:17:52,000 --> 00:17:54,133
上面的是一个 RNN

536
00:17:54,133 --> 00:17:55,766
下面也是一个 RNN

537
00:17:55,766 --> 00:17:56,883
哦不是全松模

538
00:17:56,966 --> 00:17:57,683
蛮有意思的

539
00:17:57,683 --> 00:17:58,400
非常创新

540
00:17:58,400 --> 00:17:59,883
在 17 年的一个文章

541
00:18:00,283 --> 00:18:01,533
那解决第二个问题

542
00:18:01,533 --> 00:18:03,800
就是使用 TOKEN 的负载了

543
00:18:03,800 --> 00:18:05,966
说实话我们的一些 getting

544
00:18:05,966 --> 00:18:07,083
呃所谓的路由

545
00:18:07,083 --> 00:18:09,366
其实更倾向于激活的

546
00:18:09,366 --> 00:18:11,283
就主要使用那几个专家

547
00:18:11,283 --> 00:18:12,766
反正专家越受欢迎

548
00:18:12,766 --> 00:18:14,000
他训练的越坏

549
00:18:14,000 --> 00:18:15,133
越容易被选择

550
00:18:15,133 --> 00:18:16,533
这是正常的方式

551
00:18:16,533 --> 00:18:16,850
于是

552
00:18:16,850 --> 00:18:19,733
我们需要引入一个辅助的损失函数

553
00:18:19,733 --> 00:18:22,200
去鼓励所有专家的重要性

554
00:18:22,200 --> 00:18:22,933
都很重要

555
00:18:22,933 --> 00:18:24,650
去平衡我们整体的计算量

556
00:18:24,650 --> 00:18:26,366
那这个损失

557
00:18:26,366 --> 00:18:27,883
或者我们的辅助损失

558
00:18:27,883 --> 00:18:30,083
最重要的就是确保所有的专家

559
00:18:30,333 --> 00:18:33,333
接受大致相同的训练的数据样本

560
00:18:33,333 --> 00:18:35,766
从而平衡专家之间的选择

561
00:18:35,766 --> 00:18:37,250
所以说我们这里面的 getting

562
00:18:37,250 --> 00:18:38,800
我们的路由的算法

563
00:18:38,800 --> 00:18:40,850
或者我们均衡负载的算法

564
00:18:40,850 --> 00:18:43,200
就变得非常的核心了

565
00:18:45,200 --> 00:18:46,683
但是我们反观回来

566
00:18:46,683 --> 00:18:49,283
看一下专家之间是怎么去学习的

567
00:18:49,283 --> 00:18:50,650
因此我们在后面

568
00:18:50,650 --> 00:18:53,766
会重点去解读 STME 这篇文章

569
00:18:53,766 --> 00:18:54,450
那这篇文章

570
00:18:54,450 --> 00:18:55,933
我们重点看看右边

571
00:18:55,933 --> 00:18:56,766
他就说了

572
00:18:57,000 --> 00:18:58,766
不是每一个专家

573
00:18:58,766 --> 00:19:00,933
都会学习深层次的事情的

574
00:19:00,933 --> 00:19:03,133
但是也不是每个专家都是独立的

575
00:19:03,166 --> 00:19:05,166
专家跟专家之间有交互

576
00:19:05,166 --> 00:19:06,600
但是有部分专家

577
00:19:06,600 --> 00:19:08,650
也会学习一些浅层的知识

578
00:19:08,650 --> 00:19:10,283
或者特定领域的东西

579
00:19:10,283 --> 00:19:10,800
例如

580
00:19:10,800 --> 00:19:14,333
在 LIA6 跟 LIA2 里面的某一个专家

581
00:19:14,333 --> 00:19:15,250
就专门学习

582
00:19:15,250 --> 00:19:18,250
或者专门去处理标点符号的内容

583
00:19:18,250 --> 00:19:19,283
那有些专家

584
00:19:19,283 --> 00:19:21,600
就专门学习我们的标志符

585
00:19:21,733 --> 00:19:23,800
或者我们的某些特殊的 ID

586
00:19:23,883 --> 00:19:25,283
当然了某些专家

587
00:19:25,283 --> 00:19:27,450
可能这里面有一个叫做

588
00:19:27,766 --> 00:19:30,283
window discretion 就是专门的视觉的内容

589
00:19:30,283 --> 00:19:30,800
可以看到

590
00:19:30,800 --> 00:19:34,450
他专门去处理一些颜色的相关的内容

591
00:19:34,450 --> 00:19:36,766
所以说我们的每个专家

592
00:19:36,766 --> 00:19:38,600
都处理不一样的东西

593
00:19:38,600 --> 00:19:40,000
但是你不要觉得

594
00:19:40,000 --> 00:19:41,566
可能真正的每个专家

595
00:19:41,566 --> 00:19:43,000
去处理不同的东西哦

596
00:19:43,283 --> 00:19:45,250
真正的在我们训练过当中

597
00:19:45,250 --> 00:19:47,800
因为我们引入了陆游结束 voting

598
00:19:47,800 --> 00:19:49,450
还有我们的负载均衡

599
00:19:49,450 --> 00:19:50,200
所以这里面

600
00:19:50,200 --> 00:19:53,133
真正的我们没有任何专家被指定运用

601
00:19:53,133 --> 00:19:56,200
或者指定去处理任何特殊的内容

602
00:19:56,200 --> 00:19:58,000
这个也是所谓的呃

603
00:19:58,000 --> 00:19:59,333
mystery of expert

604
00:19:59,333 --> 00:20:02,450
而不是 multiply expert 的一个原因

605
00:20:02,450 --> 00:20:03,733
虽然都叫 m e

606
00:20:04,166 --> 00:20:04,650
那另外的话

607
00:20:04,650 --> 00:20:07,250
我们看一下专家的数量对于预训练

608
00:20:07,250 --> 00:20:08,200
有哪些影响

609
00:20:08,200 --> 00:20:09,933
说实话我们增加专家

610
00:20:09,933 --> 00:20:11,800
可以提升处理样本的效率

611
00:20:11,800 --> 00:20:14,650
反正每包数据都分到不同的专家

612
00:20:14,650 --> 00:20:17,566
整体加速我们模型的运算的速率

613
00:20:17,850 --> 00:20:20,283
但是随着砖家数量的增加呀

614
00:20:20,283 --> 00:20:23,766
特别是砖家数量达到 256512 之后

615
00:20:23,766 --> 00:20:26,800
他的模型的效果是在递减的

616
00:20:26,850 --> 00:20:27,566
另外的话

617
00:20:27,566 --> 00:20:29,333
增加砖家的过程当中

618
00:20:29,333 --> 00:20:32,766
我们的显存的占用量也会更大

619
00:20:32,966 --> 00:20:33,566
所以因此

620
00:20:33,566 --> 00:20:35,166
我们的专家的数量

621
00:20:35,166 --> 00:20:36,766
也会对我们的模型效果

622
00:20:36,883 --> 00:20:38,200
有一个明显的影响

623
00:20:38,200 --> 00:20:40,333
到底多专家还是小专家

624
00:20:40,333 --> 00:20:43,650
到底像 Mr 的大模型大参数量少

625
00:20:43,650 --> 00:20:44,566
小的专家

626
00:20:44,566 --> 00:20:47,133
还是像换方的多专家

627
00:20:47,133 --> 00:20:48,766
小参数量比较好

628
00:20:48,766 --> 00:20:50,133
大家好我是渣渣飞

629
00:20:50,283 --> 00:20:52,333
哈哈是兄弟就来砍

630
00:20:52,333 --> 00:20:54,400
我普通话学的不太好

631
00:20:54,400 --> 00:20:55,850
大家看一下字幕吧

632
00:20:55,883 --> 00:20:57,766
中你尽可能把字幕搞对

633
00:20:57,800 --> 00:20:58,933
那我们现在来看一下

634
00:20:58,933 --> 00:21:00,566
稠密跟稀疏怎么选

635
00:21:00,566 --> 00:21:02,450
其实这个事情蛮有意思的

636
00:21:02,450 --> 00:21:03,850
有些就觉得在

637
00:21:03,850 --> 00:21:05,400
固定的预训练的计算资源下

638
00:21:05,400 --> 00:21:07,533
就是我假设有 1 万张卡

639
00:21:07,933 --> 00:21:08,683
在稀疏模型

640
00:21:08,683 --> 00:21:10,450
能够实现更优的效果

641
00:21:10,450 --> 00:21:12,600
我是不是在万卡里面使用稀疏

642
00:21:12,600 --> 00:21:13,883
如果只有 2 千卡

643
00:21:13,883 --> 00:21:17,050
我就在 2 千卡里面实现一个重密

644
00:21:17,283 --> 00:21:17,650
那另外

645
00:21:17,650 --> 00:21:18,650
还会有一些观点

646
00:21:18,650 --> 00:21:19,083
就是

647
00:21:19,083 --> 00:21:22,333
在我们的显存 HBM 较小的情况下

648
00:21:22,333 --> 00:21:23,533
吞吐要求不高

649
00:21:23,533 --> 00:21:26,283
那稠密的可能是更好的一个选择

650
00:21:26,333 --> 00:21:27,683
但是实际上

651
00:21:27,683 --> 00:21:28,366
中美觉得

652
00:21:28,366 --> 00:21:30,400
我们直接的去对比

653
00:21:30,400 --> 00:21:32,566
稀疏模型跟稠密模型的参数量

654
00:21:32,566 --> 00:21:33,533
其实不对的

655
00:21:33,533 --> 00:21:35,000
这两类模型的概念

656
00:21:35,000 --> 00:21:36,683
和参数量的计算和方法

657
00:21:36,933 --> 00:21:37,733
完全不对

658
00:21:37,733 --> 00:21:38,566
所以现在

659
00:21:38,566 --> 00:21:40,450
没有说一定要选从命令

660
00:21:40,450 --> 00:21:41,683
一定要选稀疏

661
00:21:41,683 --> 00:21:43,283
反而看我们的模型的效果

662
00:21:43,283 --> 00:21:44,733
我们的训练的策略

663
00:21:44,733 --> 00:21:47,650
能不能让专家更均衡的负载

664
00:21:47,766 --> 00:21:49,650
来实现更好的效果

665
00:21:49,766 --> 00:21:51,450
这个可能是研究的重点

666
00:21:52,133 --> 00:21:53,566
现在才来到了第四个内容

667
00:21:53,566 --> 00:21:56,050
让 moe 的训练和推理起飞

668
00:21:56,083 --> 00:21:57,333
怎么起飞

669
00:21:57,333 --> 00:21:58,533
说实话在早期

670
00:21:58,533 --> 00:22:01,166
也就是在 2017 年那篇文章之前

671
00:22:01,200 --> 00:22:02,133
整个 moe

672
00:22:02,133 --> 00:22:03,733
主要是采用分支结构的

673
00:22:03,733 --> 00:22:05,566
所以它的计算效率非常的低

674
00:22:05,566 --> 00:22:07,766
没有太多人的去用 moe

675
00:22:07,766 --> 00:22:08,450
这个问题

676
00:22:08,450 --> 00:22:09,850
就是 GPU

677
00:22:09,850 --> 00:22:11,166
devices 跟 device 之间

678
00:22:11,166 --> 00:22:12,733
也就是 GPU 跟 GPU 之间

679
00:22:12,733 --> 00:22:14,966
需要传递数据的网络

680
00:22:14,966 --> 00:22:15,850
模型的代化

681
00:22:15,850 --> 00:22:18,050
就成为整体的性能的瓶颈

682
00:22:18,050 --> 00:22:20,250
17 年的时候还没有迎来大模型

683
00:22:20,283 --> 00:22:22,283
大家希望把 m a 堆上去

684
00:22:22,283 --> 00:22:23,166
但是堆上去

685
00:22:23,166 --> 00:22:25,566
就要证明我们需要用分布式并行了

686
00:22:25,566 --> 00:22:27,200
当时候还是 tensile four

687
00:22:27,483 --> 00:22:28,650
这个 AI 框架流行的时候

688
00:22:28,650 --> 00:22:30,883
我们做分布式并行还是用 Hoffer

689
00:22:30,883 --> 00:22:32,966
而不是用现在的 DISC 或者

690
00:22:32,966 --> 00:22:33,850
那个 Megatron

691
00:22:34,133 --> 00:22:35,000
那我们看一下

692
00:22:35,000 --> 00:22:36,600
真正的让 m o e 起飞的

693
00:22:36,600 --> 00:22:37,850
有哪些重要的手段

694
00:22:37,850 --> 00:22:39,133
第一个就是英法层面的

695
00:22:39,133 --> 00:22:40,766
我们要做专家的并行

696
00:22:40,766 --> 00:22:42,333
我们要做通讯的优化了

697
00:22:42,333 --> 00:22:43,883
我们还可以做一些增流

698
00:22:43,883 --> 00:22:45,083
让 m o e

699
00:22:45,366 --> 00:22:47,366
在预训练的阶段和推理的阶段

700
00:22:47,366 --> 00:22:48,933
更加高效和实用

701
00:22:48,933 --> 00:22:50,083
那接下来我们看一下

702
00:22:50,083 --> 00:22:52,166
怎么样 moe 去起飞

703
00:22:52,333 --> 00:22:52,850
首先

704
00:22:52,850 --> 00:22:55,766
我们要考虑一下变形计算的问题

705
00:22:56,000 --> 00:22:57,083
因为 M1 多了

706
00:22:57,083 --> 00:22:57,683
专家多了

707
00:22:57,683 --> 00:22:59,050
模型差数量大了

708
00:22:59,050 --> 00:23:01,850
所以我们需要对专家进行并行

709
00:23:01,850 --> 00:23:04,483
每个专家放在不同的 MPU 卡上面

710
00:23:04,483 --> 00:23:06,366
跟数据并行结合起来

711
00:23:06,366 --> 00:23:07,733
因为我们每个专家

712
00:23:07,733 --> 00:23:10,766
是吞吐到不同的一个数据里面的

713
00:23:10,933 --> 00:23:11,450
因此

714
00:23:11,450 --> 00:23:14,200
数据在所有 MPU 之间进行一个分割

715
00:23:14,250 --> 00:23:15,083
当然了我们后面

716
00:23:15,083 --> 00:23:16,683
会重点讲 MOV 并行

717
00:23:16,683 --> 00:23:19,883
或者专家并行等相关的一个内容的

718
00:23:21,200 --> 00:23:22,533
那这里面的专家并行

719
00:23:22,533 --> 00:23:23,883
综面解释的比较苍白

720
00:23:23,883 --> 00:23:25,850
我们将会在后面的内容里面

721
00:23:26,050 --> 00:23:29,600
详细的深入的去跟大家看一下

722
00:23:29,600 --> 00:23:31,733
专家并行的相关的代码

723
00:23:31,883 --> 00:23:34,250
第六个就是容量因子跟显存带宽了

724
00:23:34,250 --> 00:23:36,800
那容量因子其实里面的一个超餐了

725
00:23:36,800 --> 00:23:38,533
那提高里面的容量因子

726
00:23:38,533 --> 00:23:40,133
可以增加 Moe 的性能

727
00:23:40,133 --> 00:23:42,933
但是会带来更高的一个通讯成本

728
00:23:42,933 --> 00:23:44,600
和对我们的保存的激活词

729
00:23:44,600 --> 00:23:47,333
就是 HBN 我们的显存的要求更高

730
00:23:47,450 --> 00:23:49,050
在我们的设备的通讯带宽

731
00:23:49,050 --> 00:23:50,366
有限的情况下

732
00:23:50,366 --> 00:23:52,333
我们可以选择更加小的

733
00:23:52,333 --> 00:23:54,366
一个超餐的容量因子

734
00:23:54,533 --> 00:23:55,766
在评估性能的时候

735
00:23:55,766 --> 00:23:57,933
可能也需要找到一个平衡点

736
00:23:57,933 --> 00:24:00,533
也就在通讯的成本跟计算之间

737
00:24:00,533 --> 00:24:02,200
找到一个平衡点

738
00:24:03,050 --> 00:24:05,083
说到很多我们会发现哎

739
00:24:05,200 --> 00:24:09,050
为什么整个 dipstick R1 哦会做那么多蒸馏

740
00:24:09,050 --> 00:24:10,133
蒸馏到千万里面

741
00:24:10,133 --> 00:24:11,366
蒸馏到辣妈里面

742
00:24:11,450 --> 00:24:14,050
是因为在整个 me 的架构里面

743
00:24:14,050 --> 00:24:15,450
蒸馏还蛮有意思的

744
00:24:15,600 --> 00:24:17,566
我们希望通过 me 的这个模型

745
00:24:17,566 --> 00:24:19,933
增馏回去稠密的模型里面

746
00:24:19,933 --> 00:24:22,200
假设我们采用了吸收的架构之后

747
00:24:22,200 --> 00:24:24,933
提升了 30%-40%的一个性能

748
00:24:25,200 --> 00:24:27,133
我们把这些性能的争议

749
00:24:27,133 --> 00:24:28,533
通过争流的方式

750
00:24:28,533 --> 00:24:31,000
推到我们的稠密的小模型里面

751
00:24:31,000 --> 00:24:33,966
这是一个研究 Moe 架构人里面

752
00:24:33,966 --> 00:24:35,483
或者算法人员里面的

753
00:24:35,483 --> 00:24:37,166
最通用的一个认识

754
00:24:37,200 --> 00:24:38,733
反正 MOE 架构

755
00:24:38,733 --> 00:24:40,000
因为很稀疏

756
00:24:40,000 --> 00:24:42,650
所以他希望能够变成稠密的去计算

757
00:24:42,733 --> 00:24:44,800
减少我们的一个 HBM

758
00:24:44,800 --> 00:24:46,366
又减少我们的险存的浪费

759
00:24:46,600 --> 00:24:46,933
第二个

760
00:24:46,933 --> 00:24:49,800
我们可以尝试一些任务级别的路由

761
00:24:49,883 --> 00:24:50,733
所谓的任务级别

762
00:24:50,733 --> 00:24:53,166
我们刚才解释的就是某一个 TOKEN

763
00:24:53,166 --> 00:24:56,166
我们单独的给到某一个专家

764
00:24:56,250 --> 00:24:58,966
那我可不可以一句话或者一个 bitch

765
00:24:59,083 --> 00:24:59,850
一个任务

766
00:24:59,850 --> 00:25:02,400
直接路由到一个确定的专家里面

767
00:25:02,400 --> 00:25:02,766
于是

768
00:25:02,766 --> 00:25:05,450
业界又提出了一些任务级别的路由

769
00:25:05,450 --> 00:25:07,650
有助于简化我们的模型的结构

770
00:25:07,650 --> 00:25:08,850
把多个专家

771
00:25:08,850 --> 00:25:12,200
合并成一个专家这种方式进行处理

772
00:25:12,933 --> 00:25:13,566
那另外的话

773
00:25:13,566 --> 00:25:15,883
刚才讲到了还有一些专家的聚合

774
00:25:15,883 --> 00:25:17,283
我们叫做 SP 聚合

775
00:25:17,283 --> 00:25:19,933
就允许我们很多个专家的权重了

776
00:25:19,933 --> 00:25:22,450
推理的时候删掉某些专家

777
00:25:22,450 --> 00:25:23,533
或者某几个专家

778
00:25:23,533 --> 00:25:24,733
像刚才那样合并

779
00:25:24,733 --> 00:25:25,800
所以说这个时候

780
00:25:25,800 --> 00:25:27,283
可以减少我们推理时候

781
00:25:27,683 --> 00:25:29,133
所需要的参数量

782
00:25:29,133 --> 00:25:30,483
和显存的量了

783
00:25:30,483 --> 00:25:32,683
这也是业界在重点的探索

784
00:25:32,683 --> 00:25:33,850
那证明这个

785
00:25:33,850 --> 00:25:35,083
探索的比较成功

786
00:25:35,083 --> 00:25:36,050
特别是幻方

787
00:25:36,050 --> 00:25:39,000
deepsick R1 这个网络模型

788
00:25:39,283 --> 00:25:41,800
最后我们来一个思考和小结

789
00:25:41,800 --> 00:25:43,000
每一期都有

790
00:25:43,000 --> 00:25:44,566
作品精灵已经非常疲惫了

791
00:25:44,566 --> 00:25:46,283
录的都快快吐了

792
00:25:50,050 --> 00:25:51,683
那我们其实在开片的时候

793
00:25:51,683 --> 00:25:54,000
已经讲到了换方

794
00:25:54,000 --> 00:25:56,450
为什么采用 Deepsick moe 这种架构

795
00:25:56,450 --> 00:25:58,533
为什么能把价格打折这么低

796
00:25:58,533 --> 00:26:00,483
说实话我们刚才已经介绍了很多了

797
00:26:00,483 --> 00:26:02,883
是因为 moe 架构的一个天然的优势

798
00:26:03,000 --> 00:26:03,600
推理的时候

799
00:26:03,600 --> 00:26:05,683
我们只行部分的参数量

800
00:26:05,683 --> 00:26:07,000
但是我们的模型

801
00:26:07,000 --> 00:26:07,883
我们的 moe

802
00:26:07,883 --> 00:26:08,850
我们的专家多了

803
00:26:08,850 --> 00:26:10,083
我们的参数量大了

804
00:26:10,166 --> 00:26:11,766
根据我们的 scaling no

805
00:26:11,933 --> 00:26:12,966
参数量越大

806
00:26:12,966 --> 00:26:13,883
模型越大

807
00:26:13,883 --> 00:26:15,083
我们的数据量越多

808
00:26:15,083 --> 00:26:16,283
用的算力越大

809
00:26:16,650 --> 00:26:18,533
它整体的模型效果就越好

810
00:26:18,533 --> 00:26:20,800
所以这是 MV 架构的一个天然的优势

811
00:26:20,800 --> 00:26:21,800
而且推理的时候

812
00:26:21,800 --> 00:26:23,083
执行部分的参数

813
00:26:23,083 --> 00:26:25,283
那它不就把成本做下来了吗

814
00:26:25,283 --> 00:26:27,283
那我们做一个简单的总结

815
00:26:27,400 --> 00:26:29,083
首先整个 MV 的优势

816
00:26:29,083 --> 00:26:30,933
就我们看模型的架构

817
00:26:30,933 --> 00:26:32,766
第一个就是稀疏性带来的

818
00:26:32,766 --> 00:26:33,533
那这里面

819
00:26:33,533 --> 00:26:35,400
因为部分的专家被激活

820
00:26:35,400 --> 00:26:37,050
不是所有专家都参与计算

821
00:26:37,333 --> 00:26:39,650
所以就可以实现一个高效的推理

822
00:26:39,650 --> 00:26:41,166
但是高效推理的过程中

823
00:26:41,166 --> 00:26:43,683
我们可以扩大我们的网络模型的规模

824
00:26:43,733 --> 00:26:46,083
实现模型的表彰能力加强

825
00:26:46,450 --> 00:26:48,733
6 个就是专家模块化

826
00:26:48,733 --> 00:26:49,683
所以专家模块

827
00:26:49,683 --> 00:26:52,133
就不同的专家学习不同的特征

828
00:26:52,133 --> 00:26:53,400
学习不同的模式

829
00:26:53,400 --> 00:26:54,800
学习不同的数据

830
00:26:54,800 --> 00:26:56,683
因此可以处理更多的

831
00:26:56,683 --> 00:26:58,283
或者更大规模的数据集

832
00:26:58,283 --> 00:27:00,000
我们的 bitch size 可以变大

833
00:27:00,000 --> 00:27:01,450
我们吞吐可以变大

834
00:27:01,450 --> 00:27:04,250
不同的专家吃掉一部分数据

835
00:27:04,250 --> 00:27:06,166
这种方式来做计算的

836
00:27:06,166 --> 00:27:07,333
这个是 MV 架构

837
00:27:07,333 --> 00:27:08,133
的一个优势

838
00:27:08,333 --> 00:27:09,850
那在训练的过程当中

839
00:27:09,850 --> 00:27:10,483
我们怎么

840
00:27:10,483 --> 00:27:12,366
或者在训练和推理过程当中

841
00:27:12,366 --> 00:27:13,366
怎么提升效率

842
00:27:13,366 --> 00:27:13,733
第一个

843
00:27:13,733 --> 00:27:14,800
就是训练过程当中

844
00:27:14,800 --> 00:27:17,333
我们有专家变心使用 auto 的通讯

845
00:27:17,533 --> 00:27:18,200
奥托的通讯

846
00:27:18,200 --> 00:27:20,366
对比起可能模型并行

847
00:27:20,366 --> 00:27:22,083
张量并行的这种 overdused

848
00:27:22,083 --> 00:27:24,050
通讯的通讯量更少

849
00:27:24,166 --> 00:27:25,800
通讯的频次更多

850
00:27:25,850 --> 00:27:26,200
所以说

851
00:27:26,200 --> 00:27:28,400
我们可以利用更少的一个通讯贷款

852
00:27:28,400 --> 00:27:30,800
实现更多的内容或者更好的效果

853
00:27:30,800 --> 00:27:31,733
那每一个专家

854
00:27:31,733 --> 00:27:33,083
只处理一部分的 bitch

855
00:27:33,083 --> 00:27:35,483
同时也增加我们的数据的吞吐

856
00:27:35,766 --> 00:27:36,333
推理的环境

857
00:27:36,333 --> 00:27:37,333
比较明确了

858
00:27:37,333 --> 00:27:38,650
因为我们推理的时候

859
00:27:38,650 --> 00:27:40,566
只激活部分的专家

860
00:27:40,566 --> 00:27:42,333
所以 MV 的推理的实验

861
00:27:42,333 --> 00:27:44,683
低于同等规模的模型哦

862
00:27:44,850 --> 00:27:46,333
增加 MV 的数量

863
00:27:46,333 --> 00:27:47,166
训练的时候

864
00:27:47,166 --> 00:27:48,083
只会增加

865
00:27:48,083 --> 00:27:49,650
或者只会提升我们训练

866
00:27:49,650 --> 00:27:50,933
或者模型的效果

867
00:27:50,933 --> 00:27:52,733
不会增加推理的成本

868
00:27:52,733 --> 00:27:53,566
所以蛮有意思的

869
00:27:53,566 --> 00:27:55,366
我们的模型规模看上去很大

870
00:27:55,366 --> 00:27:56,650
但是他推理成本

871
00:27:56,650 --> 00:27:57,766
并不会很高

872
00:27:58,083 --> 00:28:00,283
当然了你要实现这些内容

873
00:28:00,283 --> 00:28:01,600
你要让 MV 很牛逼

874
00:28:01,600 --> 00:28:03,566
你要让 DISC 很牛逼

875
00:28:03,733 --> 00:28:05,533
最重要的是我们要解决一些问题

876
00:28:05,533 --> 00:28:06,766
就是专家越多

877
00:28:06,850 --> 00:28:08,200
经常不负载的问题

878
00:28:08,200 --> 00:28:09,800
专家越多学习越难

879
00:28:09,800 --> 00:28:11,200
训练越难的问题

880
00:28:11,250 --> 00:28:12,400
那今天的内容

881
00:28:12,400 --> 00:28:13,166
就到这里为止

882
00:28:13,166 --> 00:28:15,766
我们已经简单的回答了这些问题

883
00:28:15,766 --> 00:28:17,533
我们希望在下面的内容

884
00:28:17,533 --> 00:28:19,200
或者后续的内容里面

885
00:28:19,533 --> 00:28:22,883
更加深入的跟各位探讨一下

886
00:28:22,883 --> 00:28:24,450
这一些内容

887
00:28:24,483 --> 00:28:27,283
钟敏希望大家多给点赞和关注

888
00:28:27,283 --> 00:28:28,683
今天就先到这里为止了

889
00:28:28,683 --> 00:28:29,200
谢谢各位

890
00:28:29,200 --> 00:28:30,000
拜了个拜

