1
00:00:00,000 --> 00:00:01,750
内容/录制/字幕:Z0MI 酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,933 --> 00:00:03,883
hello 大家好

3
00:00:03,883 --> 00:00:04,883
我是宗米

4
00:00:04,883 --> 00:00:06,533
现在我们在 m e 环套

5
00:00:06,533 --> 00:00:08,800
专家路线的一个算法解读里面

6
00:00:08,800 --> 00:00:10,800
来到了一个第三节了

7
00:00:10,800 --> 00:00:11,283
第三节

8
00:00:11,283 --> 00:00:13,200
我们主要是跟大家一起去走读一下

9
00:00:13,200 --> 00:00:14,650
经典的论文

10
00:00:14,966 --> 00:00:16,200
那在这个视频里面

11
00:00:16,200 --> 00:00:18,450
我们重点主要分开四个内容了

12
00:00:18,450 --> 00:00:21,766
或者五六篇论文跟大家一起去解读的

13
00:00:21,766 --> 00:00:22,483
首先第一篇

14
00:00:22,483 --> 00:00:23,166
我们会重点

15
00:00:23,166 --> 00:00:25,000
去看一下电机工作 90 年代

16
00:00:25,000 --> 00:00:29,600
1991 年 hint 发表的 MOE 的初创的内容

17
00:00:29,933 --> 00:00:30,333
接着

18
00:00:30,333 --> 00:00:33,083
我们会在每一个 LP 的不同的时代

19
00:00:33,083 --> 00:00:34,733
里面去选读一两篇论文

20
00:00:34,733 --> 00:00:36,400
去跟大家一起解读的

21
00:00:36,966 --> 00:00:38,200
在整一个系列里面

22
00:00:38,200 --> 00:00:40,533
我们现在来到了 M1 的核心的工作

23
00:00:40,533 --> 00:00:43,400
看看相关的论文到底有哪些区别

24
00:00:43,400 --> 00:00:45,766
去解读一下具体的论文的细节

25
00:00:46,450 --> 00:00:48,566
我们现在马上来到了第一个内容

26
00:00:48,566 --> 00:00:50,800
Adaptive mixture of local Esper

27
00:00:50,800 --> 00:00:53,450
那当时这篇文章还不叫 M1 的架构

28
00:00:53,450 --> 00:00:56,333
而是说我混合多个本地的专家

29
00:00:56,333 --> 00:00:57,766
去做一个适配的

30
00:00:58,533 --> 00:01:00,733
呀我们现在来打开了这一篇论文

31
00:01:00,733 --> 00:01:02,483
说实话这篇论文已经非常的旧了

32
00:01:02,483 --> 00:01:04,366
大家可以看到里面的字体

33
00:01:04,366 --> 00:01:05,800
都不是印刷字体了

34
00:01:05,800 --> 00:01:07,366
或者不是那个电子版的字体

35
00:01:07,366 --> 00:01:09,333
而是扫描版的字体了

36
00:01:09,333 --> 00:01:12,050
不过不影响我们去学习和了解

37
00:01:12,083 --> 00:01:13,800
首先这篇文章的标题

38
00:01:13,800 --> 00:01:16,200
叫做 Adaptive mixture of local Esper

39
00:01:16,600 --> 00:01:18,533
主要是由佐敦跟 Hitton 来去

40
00:01:18,533 --> 00:01:20,533
在 1991 年的时候发表了

41
00:01:20,533 --> 00:01:21,800
非常的早了

42
00:01:21,800 --> 00:01:22,850
那我们现在可以看到

43
00:01:22,850 --> 00:01:25,000
大部分 Moe 的创新的论文

44
00:01:25,000 --> 00:01:27,883
都会引用最早的这篇的论文

45
00:01:27,883 --> 00:01:30,483
那这篇的论文的思路比较明确

46
00:01:30,483 --> 00:01:33,683
可以看一下整体的第一段话就是 FSHRE

47
00:01:33,683 --> 00:01:35,366
了那它的思路就是

48
00:01:35,600 --> 00:01:37,883
对于一些非常复杂的问题

49
00:01:37,883 --> 00:01:39,200
completive learning

50
00:01:39,200 --> 00:01:41,483
我们可以拆成多个子的任务

51
00:01:41,533 --> 00:01:42,933
那这个多个子的任务

52
00:01:42,933 --> 00:01:45,566
我们可以分开很多个网络来去实现

53
00:01:45,566 --> 00:01:47,050
在训练的过程当中

54
00:01:47,050 --> 00:01:49,566
分别学习和处理我们训练数据集

55
00:01:49,566 --> 00:01:50,600
其中的一个子集

56
00:01:50,600 --> 00:01:51,883
也就是其中一个任务

57
00:01:52,083 --> 00:01:52,850
那这个思路

58
00:01:52,850 --> 00:01:55,883
就是现代 Moe 的一个具体的思路了

59
00:01:55,883 --> 00:01:57,650
或者现代 Moe 具体的架构

60
00:01:57,683 --> 00:02:01,166
那文中最重要的就是讲它在当年

61
00:02:01,166 --> 00:02:02,250
1990 年的时候

62
00:02:02,250 --> 00:02:03,683
处理 NLP 的任务

63
00:02:03,733 --> 00:02:05,083
主要是处理 wow

64
00:02:05,083 --> 00:02:06,566
discrimination text

65
00:02:06,683 --> 00:02:10,133
也就是元音的辨别识别的任务

66
00:02:10,133 --> 00:02:11,600
那元音辨别识别任务

67
00:02:11,600 --> 00:02:13,483
就是指我们在语音学当中

68
00:02:13,483 --> 00:02:15,766
去区分不同的元音的能力

69
00:02:15,766 --> 00:02:17,050
在元音学当中

70
00:02:17,050 --> 00:02:20,400
模型需要去学习和辨别不同的元音

71
00:02:20,400 --> 00:02:21,133
的因素

72
00:02:21,133 --> 00:02:23,966
来准确的去识别和理解语音的输入

73
00:02:23,966 --> 00:02:25,850
所以 MV 的架构的出现

74
00:02:26,050 --> 00:02:28,933
原来就是为了去解决语言的问题

75
00:02:28,933 --> 00:02:32,050
让多个子模型分别学习不同的元音

76
00:02:32,050 --> 00:02:34,333
ROEUE 等子样物

77
00:02:34,333 --> 00:02:36,933
最终去提升模型的效果

78
00:02:37,133 --> 00:02:39,566
那我们接下来去看一下里面

79
00:02:39,600 --> 00:02:42,050
这个文章最核心的一个图

80
00:02:42,166 --> 00:02:43,683
那这个图蛮有意思的

81
00:02:43,683 --> 00:02:46,366
就是整个 me 的架构的思路了

82
00:02:46,683 --> 00:02:48,766
这里面有四个正方框

83
00:02:48,766 --> 00:02:49,533
一个是 ASP1

84
00:02:49,533 --> 00:02:50,683
一个是 ASP2

85
00:02:50,733 --> 00:02:53,166
还有 ASP3 和 getting lambot

86
00:02:53,200 --> 00:02:54,533
那这三个

87
00:02:54,533 --> 00:02:56,000
就是我们的专家网络

88
00:02:56,000 --> 00:02:57,450
还有一个路由网络

89
00:02:57,450 --> 00:02:59,133
或者我们叫做路由器也好

90
00:02:59,133 --> 00:03:00,800
还是门控网络也好

91
00:03:00,800 --> 00:03:03,283
我们接受同样的输入

92
00:03:03,366 --> 00:03:05,200
也就是我们的 input 的特征

93
00:03:05,200 --> 00:03:07,283
都会输给四个方框

94
00:03:07,333 --> 00:03:08,083
每个 exper

95
00:03:08,083 --> 00:03:10,566
就给出了各自的一个处理的结果

96
00:03:10,566 --> 00:03:11,483
而 getting

97
00:03:11,483 --> 00:03:14,166
就是我们的路由网络就输出 exper

98
00:03:14,166 --> 00:03:15,683
每个 exper 的权重

99
00:03:15,933 --> 00:03:17,883
这个 get 就像一个开关一样

100
00:03:17,883 --> 00:03:21,283
控制每个 ASP 对当前输入的打开程度

101
00:03:21,283 --> 00:03:23,333
那这个开关不是离散的

102
00:03:23,333 --> 00:03:25,283
而是 sophisticate 的

103
00:03:25,283 --> 00:03:26,850
给出的不是 too 和 force

104
00:03:26,850 --> 00:03:28,600
而是每个 ASP 的权重

105
00:03:28,600 --> 00:03:29,683
我到底有多少

106
00:03:29,683 --> 00:03:31,966
是真正输出到最终的结果

107
00:03:32,483 --> 00:03:33,083
那接下来

108
00:03:33,083 --> 00:03:35,933
我们正式的主读一下这一篇文章

109
00:03:36,000 --> 00:03:36,966
首先蛮有意思的

110
00:03:36,966 --> 00:03:39,133
就是可以看一下第一个标题

111
00:03:39,250 --> 00:03:41,883
making associated learning competitive

112
00:03:42,333 --> 00:03:43,683
这个标题很有意思

113
00:03:43,683 --> 00:03:44,800
就你觉得翻译

114
00:03:44,800 --> 00:03:45,883
就有点 low 了

115
00:03:45,883 --> 00:03:46,650
那实际上

116
00:03:46,650 --> 00:03:47,250
这里面

117
00:03:47,250 --> 00:03:48,200
一开始就说了

118
00:03:48,200 --> 00:03:49,850
其实要呃 MV

119
00:03:49,850 --> 00:03:50,533
这个 idea

120
00:03:50,533 --> 00:03:51,733
在这篇文章之前

121
00:03:51,733 --> 00:03:52,733
已经有了

122
00:03:52,733 --> 00:03:54,250
例如在 1988 年的时候

123
00:03:54,250 --> 00:03:57,366
还是 1989 年和 1990 年的时候

124
00:03:57,366 --> 00:03:58,650
就已经讨论过了

125
00:03:58,800 --> 00:04:01,083
不过之前的工作主要是在 lost 的

126
00:04:01,083 --> 00:04:01,650
实际上

127
00:04:01,650 --> 00:04:03,050
跟 example 比较接近

128
00:04:03,050 --> 00:04:06,966
那多个 ASPO 之间更倾向于 associative

129
00:04:06,966 --> 00:04:08,766
也就是更倾向于合作

130
00:04:08,800 --> 00:04:09,600
每个 ASP

131
00:04:09,600 --> 00:04:13,000
就会学习其他 ASP 的一个残差的部分

132
00:04:13,333 --> 00:04:13,966
具体来说

133
00:04:13,966 --> 00:04:15,250
对于每一个 case

134
00:04:15,250 --> 00:04:15,883
这些 case

135
00:04:15,883 --> 00:04:17,483
我们用 c 来去表达

136
00:04:17,483 --> 00:04:18,533
也就是 e c

137
00:04:18,600 --> 00:04:19,683
对于每一个 case

138
00:04:19,683 --> 00:04:21,133
假设现在 d c

139
00:04:21,133 --> 00:04:21,883
也就是 d c

140
00:04:21,883 --> 00:04:23,333
对应的是一个光 truff

141
00:04:23,366 --> 00:04:24,366
第 i 个 ASP

142
00:04:24,366 --> 00:04:25,933
我们的下边是 i

143
00:04:26,050 --> 00:04:29,133
第 i 个 ASP 的输出是 o i c

144
00:04:29,400 --> 00:04:30,200
而 PIC

145
00:04:30,200 --> 00:04:31,200
是 getting network

146
00:04:31,200 --> 00:04:32,650
也就是我们的门控网络

147
00:04:32,650 --> 00:04:35,133
给出第 igsport 的分配的权重

148
00:04:35,166 --> 00:04:36,566
那么之前的工作

149
00:04:36,566 --> 00:04:38,333
所有的损失函数

150
00:04:38,333 --> 00:04:39,533
我们叫做 EC

151
00:04:39,800 --> 00:04:41,733
一就是期望 expectation

152
00:04:41,766 --> 00:04:44,133
那我们现在已经深度学习统一的范式

153
00:04:44,133 --> 00:04:46,200
所以现在都叫损失函数了

154
00:04:46,200 --> 00:04:48,533
那这样的损失的计算的方式

155
00:04:48,533 --> 00:04:49,933
是把期望的输出

156
00:04:49,933 --> 00:04:52,850
跟所有的 Esper 的输出混合的结果

157
00:04:52,850 --> 00:04:54,083
进行一个计算的

158
00:04:54,283 --> 00:04:55,050
这样做的结果

159
00:04:55,050 --> 00:04:56,483
就是在训练的过程当中

160
00:04:56,533 --> 00:04:58,000
每个 exper 学习到的

161
00:04:58,000 --> 00:05:01,283
其实是其他 exper 组合所剩下的残差

162
00:05:01,283 --> 00:05:02,966
我们叫做 residues

163
00:05:02,966 --> 00:05:05,083
所以我们叫做 LOCO exper

164
00:05:05,450 --> 00:05:06,000
这样做

165
00:05:06,000 --> 00:05:06,933
学习的目的

166
00:05:06,933 --> 00:05:07,966
或者学习的方式

167
00:05:07,966 --> 00:05:08,733
不是很好的

168
00:05:08,733 --> 00:05:11,166
使得每一个专家都能够独立的

169
00:05:11,166 --> 00:05:12,166
输出好的结果

170
00:05:12,166 --> 00:05:13,600
也就是每个专家都很强

171
00:05:13,683 --> 00:05:16,133
因此不能得到稀疏的模型

172
00:05:16,450 --> 00:05:18,166
那从另外一个角度来看

173
00:05:18,166 --> 00:05:20,483
这样的损失的计算也就是我们这一条

174
00:05:20,483 --> 00:05:22,533
把所有的专家都偶合在一起的

175
00:05:22,533 --> 00:05:23,200
大家可以看到

176
00:05:23,200 --> 00:05:26,000
对所有的专家的输出的结果进行 getting

177
00:05:26,000 --> 00:05:28,933
然后进行一个求和这种方式

178
00:05:29,250 --> 00:05:30,966
因此这种学习的方式

179
00:05:30,966 --> 00:05:33,200
各个专家之间更倾向于合作

180
00:05:33,200 --> 00:05:34,450
也就是 cooperation

181
00:05:34,450 --> 00:05:36,683
而不是相互竞争 competitive

182
00:05:36,683 --> 00:05:39,533
所以这篇文章的标题蛮有意思的

183
00:05:39,533 --> 00:05:41,366
就是 making a social learning

184
00:05:41,366 --> 00:05:43,566
competitive 的一个原因了

185
00:05:43,566 --> 00:05:45,566
我们希望各个专家之间

186
00:05:45,566 --> 00:05:47,683
更好地去做一个竞争

187
00:05:47,683 --> 00:05:50,083
更好地去学习到自己独立的知识

188
00:05:50,083 --> 00:05:51,966
而不是 cooperative 合作

189
00:05:52,400 --> 00:05:52,733
虽然

190
00:05:52,733 --> 00:05:54,966
这里面其实还有另外一篇文章

191
00:05:54,966 --> 00:05:55,683
Jack bones

192
00:05:55,683 --> 00:05:57,683
在 1990 年的时候就说

193
00:05:57,683 --> 00:05:59,600
我其实可以增加辅助的

194
00:05:59,600 --> 00:06:00,650
损失函数的做法

195
00:06:00,650 --> 00:06:01,683
使得我们的模型

196
00:06:01,683 --> 00:06:03,483
给出激活激素的结果

197
00:06:03,683 --> 00:06:04,450
但是这样的话

198
00:06:04,450 --> 00:06:07,133
也就相当于加了很多其他鲜艳的知识

199
00:06:07,133 --> 00:06:09,650
或者鲜艳的一个情况 case 去引导了

200
00:06:09,650 --> 00:06:10,366
不是非常

201
00:06:10,366 --> 00:06:11,650
的纯粹于是

202
00:06:11,650 --> 00:06:13,650
hitten 跟 Jordan 在这个工作里面

203
00:06:13,650 --> 00:06:15,966
就提出了更简单的方法

204
00:06:15,966 --> 00:06:18,683
就是对我们的 loss 进行一个修改

205
00:06:18,850 --> 00:06:20,366
使得各个专家之间的

206
00:06:20,366 --> 00:06:21,766
各个 ASP 之间的关系

207
00:06:21,966 --> 00:06:23,850
从合作变成竞争

208
00:06:23,850 --> 00:06:26,200
从 cooperative 变成 competitive

209
00:06:26,200 --> 00:06:26,883
那最重要的

210
00:06:26,883 --> 00:06:30,000
就体现在这一条公式里面了

211
00:06:30,000 --> 00:06:30,566
这里面

212
00:06:30,566 --> 00:06:31,850
就假设 getting 到我了

213
00:06:31,850 --> 00:06:34,566
每次随机去选择一个 ASP

214
00:06:34,600 --> 00:06:36,333
那下面的这条 e c

215
00:06:36,333 --> 00:06:38,133
同样是损失的函数

216
00:06:38,133 --> 00:06:39,933
不过在这个损失函数

217
00:06:39,933 --> 00:06:41,533
每个 ASP 的输出的结果

218
00:06:41,533 --> 00:06:45,283
就会单独和期望结果进行一个对比

219
00:06:45,283 --> 00:06:46,050
所以可以看到

220
00:06:46,050 --> 00:06:46,766
光触符

221
00:06:46,766 --> 00:06:48,733
d c 是挪进去跟 o c

222
00:06:48,733 --> 00:06:50,850
就是我们的 ASP 输出的结果

223
00:06:50,850 --> 00:06:52,400
然后再去给我们的 getting

224
00:06:52,400 --> 00:06:53,483
给我们的路由网络

225
00:06:53,483 --> 00:06:56,250
给我们的门控进行一个计算的

226
00:06:56,366 --> 00:06:57,450
这样的输出的好处

227
00:06:57,450 --> 00:06:58,966
就是要求每个 ASP

228
00:06:58,966 --> 00:07:00,450
单独给出完整的结果

229
00:07:00,450 --> 00:07:02,966
而不是进行学习其他 ASP 的一个长差

230
00:07:03,283 --> 00:07:05,450
这样的 loss 比较具有 local 性

231
00:07:05,450 --> 00:07:06,883
就是 localization 的特性

232
00:07:06,883 --> 00:07:08,533
如果一个训练 case 错了

233
00:07:08,533 --> 00:07:09,800
那整体的修改

234
00:07:09,800 --> 00:07:12,166
不是说整一个网络模型

235
00:07:12,166 --> 00:07:14,483
而是这个网络模型 getting 所对应的

236
00:07:14,483 --> 00:07:15,400
自己选错的

237
00:07:15,400 --> 00:07:17,966
或者性能不太好的一个专家 asper

238
00:07:18,333 --> 00:07:19,650
而不是像上面那样

239
00:07:19,650 --> 00:07:22,766
影响到所有的整个网络模型

240
00:07:22,800 --> 00:07:23,450
这样一来

241
00:07:23,450 --> 00:07:24,450
所有的 sport 之间

242
00:07:24,450 --> 00:07:26,800
就不会有相互性的一个影响

243
00:07:26,800 --> 00:07:28,933
但是虽然又会有一些间接性的影响

244
00:07:28,933 --> 00:07:30,766
例如某个专家的输出变了

245
00:07:30,766 --> 00:07:33,683
get 可能会重新的分配权重

246
00:07:33,683 --> 00:07:34,250
但至少

247
00:07:34,250 --> 00:07:36,533
不会影响其他 sport 的一个大的

248
00:07:36,533 --> 00:07:37,333
变化

249
00:07:38,333 --> 00:07:39,650
那最终实现的结果

250
00:07:39,650 --> 00:07:40,883
就给定输出

251
00:07:40,883 --> 00:07:41,533
这样的系统

252
00:07:41,533 --> 00:07:44,933
就会倾向于选择高权重的分配 exper

253
00:07:44,933 --> 00:07:46,133
来预测结果

254
00:07:46,933 --> 00:07:49,600
这个就是一条损失改变的

255
00:07:49,600 --> 00:07:52,166
一个具体的原因或者问题了

256
00:07:52,166 --> 00:07:53,566
那我们现在来看看

257
00:07:53,566 --> 00:07:54,966
具体的一些细节

258
00:07:54,966 --> 00:07:56,933
也就是我们的 loss 怎么去做变化

259
00:07:56,933 --> 00:07:57,966
怎么去求导的

260
00:07:57,966 --> 00:07:58,850
对我们的理论

261
00:07:58,850 --> 00:08:01,766
实际的时候的实践有哪些不同的影响

262
00:08:02,083 --> 00:08:03,966
上面 hitter 新的这个 loss

263
00:08:03,966 --> 00:08:05,000
理论上没什么问题

264
00:08:05,000 --> 00:08:06,850
实际上也能够做一个训练的

265
00:08:06,850 --> 00:08:08,766
但是为了得到更好的结果

266
00:08:08,766 --> 00:08:09,733
所以原则者

267
00:08:09,733 --> 00:08:12,683
就把 loss 做了一个简单的变化

268
00:08:12,933 --> 00:08:14,283
先进行指数化

269
00:08:14,283 --> 00:08:15,933
然后再进行对数求和

270
00:08:15,933 --> 00:08:18,733
最终得到我们优化后的 loss ET

271
00:08:19,050 --> 00:08:20,050
所以说真正的 los

272
00:08:20,050 --> 00:08:21,566
是经过一个修改的

273
00:08:21,683 --> 00:08:22,933
这样做有什么好处

274
00:08:22,933 --> 00:08:25,366
那就需要看一下下面的这两个导数了

275
00:08:25,366 --> 00:08:26,533
那原来的导数

276
00:08:26,533 --> 00:08:27,333
是这一条

277
00:08:27,333 --> 00:08:28,400
优化后的导数

278
00:08:28,400 --> 00:08:30,000
是下面的这一条

279
00:08:30,083 --> 00:08:32,250
相比起原来的 los 导数

280
00:08:32,250 --> 00:08:34,366
优化后的一个函数的导数

281
00:08:34,366 --> 00:08:38,083
把当前第 i 个 x per 的一个表现

282
00:08:38,083 --> 00:08:40,133
跟其他 DJ 一个 s 表的表现

283
00:08:40,133 --> 00:08:41,933
就其他所有 s 表的表现

284
00:08:41,933 --> 00:08:43,450
进行联系起来了

285
00:08:43,483 --> 00:08:46,250
能够更好的整晚的去衡量 s per i 呀

286
00:08:46,250 --> 00:08:47,166
d i 个 s per

287
00:08:47,200 --> 00:08:48,450
对当前 case 的一个

288
00:08:48,450 --> 00:08:51,083
或者当前我们的元音的一个好坏

289
00:08:51,283 --> 00:08:52,733
特别是在训练的初期

290
00:08:52,733 --> 00:08:53,850
getting 网络的权重

291
00:08:53,850 --> 00:08:55,766
其实是平均的分配的

292
00:08:55,766 --> 00:08:58,083
那使用原来的 lost 的一个方式

293
00:08:58,083 --> 00:08:59,283
去进行计算

294
00:08:59,283 --> 00:09:01,250
对当前 k 效果比较好的 ASP

295
00:09:01,250 --> 00:09:02,850
学习速度是比较慢的

296
00:09:03,050 --> 00:09:05,933
而使用了优化后的一个 lost

297
00:09:06,050 --> 00:09:08,133
也就对应取了对数

298
00:09:08,133 --> 00:09:09,400
优化后的一个 lost

299
00:09:09,400 --> 00:09:11,200
就可以让当前最好的 Esport

300
00:09:11,200 --> 00:09:12,133
学习的速度

301
00:09:12,133 --> 00:09:12,933
非常的快

302
00:09:13,083 --> 00:09:13,600
相当于

303
00:09:13,600 --> 00:09:14,883
有天赋的专家

304
00:09:14,883 --> 00:09:17,000
就尽可能的提高你自己的水平

305
00:09:17,083 --> 00:09:17,533
这样的话

306
00:09:17,533 --> 00:09:20,566
就强化了 localization 的一个相关的特性

307
00:09:20,566 --> 00:09:21,533
让每个专家

308
00:09:21,533 --> 00:09:23,566
更好的拟合到自己擅长的部分

309
00:09:23,566 --> 00:09:24,850
加速整体的训练

310
00:09:24,850 --> 00:09:25,883
各个专家之间

311
00:09:25,883 --> 00:09:27,800
也更好的进行一个 cooperative

312
00:09:27,800 --> 00:09:29,650
也就是协同的问题了

313
00:09:29,650 --> 00:09:30,333
所以这里面

314
00:09:30,333 --> 00:09:31,966
蛮有意思的就是第二个标题

315
00:09:31,966 --> 00:09:35,133
就 making competitive learning associative

316
00:09:35,400 --> 00:09:37,166
更好地做一个协同了

317
00:09:37,483 --> 00:09:38,050
后面内容

318
00:09:38,050 --> 00:09:39,566
就是相关的实验了

319
00:09:39,566 --> 00:09:42,450
相关的内容还有对应的引用了

320
00:09:42,450 --> 00:09:44,050
那这篇文章的核心思想

321
00:09:44,050 --> 00:09:45,766
已经跟大家介绍完了

322
00:09:46,133 --> 00:09:47,400
今天的内容比较简洁

323
00:09:47,400 --> 00:09:48,966
我们一天一篇文章

324
00:09:48,966 --> 00:09:51,766
去看一下最经典的 M1 的架构的论文

325
00:09:51,766 --> 00:09:52,133
当然了

326
00:09:52,133 --> 00:09:54,933
后面也会对 Dipstick 的相关的论文

327
00:09:54,933 --> 00:09:56,333
全部的详细的展开

328
00:09:56,600 --> 00:09:57,733
那我们所有的开源链接

329
00:09:57,733 --> 00:09:58,600
都在这里面

330
00:09:58,600 --> 00:09:59,283
那现在

331
00:09:59,283 --> 00:10:00,733
很多人会反馈

332
00:10:00,733 --> 00:10:03,050
钟米你的开源链接经常下不了

333
00:10:03,050 --> 00:10:04,050
经常上不了

334
00:10:04,050 --> 00:10:06,366
所以钟米就放出了一个夸克的链接

335
00:10:06,366 --> 00:10:08,600
提供大家去下载相关的 PPT

336
00:10:08,600 --> 00:10:11,000
和相关的中米解读后的论文

