1
00:00:00,000 --> 00:00:02,533
内容/录制:Z0MI 酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,533 --> 00:00:03,433
哈喽大家好

3
00:00:03,433 --> 00:00:05,933
我是那个工作时嫌我老

4
00:00:06,033 --> 00:00:08,100
退休时嫌我少的

5
00:00:08,133 --> 00:00:10,400
ZOMI 现在退休都要 65 岁了

6
00:00:10,400 --> 00:00:13,633
但是 35 岁已经遇到职场焦虑了

7
00:00:13,633 --> 00:00:14,533
好惊险

8
00:00:14,566 --> 00:00:15,533
好危险

9
00:00:18,266 --> 00:00:18,966
我们今天

10
00:00:19,000 --> 00:00:22,066
来到了整个大模型遇上 moe

11
00:00:22,066 --> 00:00:24,333
看一下最近我们这些大模型

12
00:00:24,366 --> 00:00:26,366
跟 moe 结合能有哪些

13
00:00:26,433 --> 00:00:27,533
最新的变化

14
00:00:27,566 --> 00:00:29,200
首先我觉得这个表蛮有意思的

15
00:00:29,200 --> 00:00:31,933
就是从 GP4 的 MR8 进 7B

16
00:00:31,933 --> 00:00:33,866
到 Deepseek 的一个 moe

17
00:00:33,966 --> 00:00:36,366
到 DBRX 还有 Grok

18
00:00:36,366 --> 00:00:38,233
到最近的一个 Deepseek V3

19
00:00:38,233 --> 00:00:38,766
你会发现

20
00:00:38,800 --> 00:00:41,533
整个大模型加 moe 的趋势

21
00:00:41,866 --> 00:00:43,433
是越来越明显了

22
00:00:43,433 --> 00:00:43,866
但是

23
00:00:43,866 --> 00:00:46,300
这里面又发生了一些细微的变化

24
00:00:46,333 --> 00:00:48,933
是我们今天重点去观察的内容

25
00:00:48,933 --> 00:00:49,566
那今天

26
00:00:49,566 --> 00:00:52,200
主要是跟大家去分享四个内容的

27
00:00:52,200 --> 00:00:53,166
或整个视频里面

28
00:00:53,166 --> 00:00:54,433
分开四个小节

29
00:00:54,466 --> 00:00:54,733
第一个

30
00:00:54,766 --> 00:00:58,133
我们看一下 Mistral Al 里面的 8*7B

31
00:00:58,133 --> 00:01:01,133
还有 8*22B 的模型有哪些区别

32
00:01:01,166 --> 00:01:01,600
为什么

33
00:01:01,600 --> 00:01:04,366
Mistral 当时候应该是在 24 年的时候

34
00:01:04,366 --> 00:01:06,600
还引起蛮轰动的一个效果的

35
00:01:06,633 --> 00:01:10,033
到 Grok 现在马斯克的 X AI

36
00:01:10,033 --> 00:01:12,366
用了 20 万卡的 H100

37
00:01:12,400 --> 00:01:14,000
去训练这个 Grok 3

38
00:01:14,133 --> 00:01:15,966
取得了非常惊人的效果

39
00:01:15,966 --> 00:01:17,633
但是这些惊人的效果

40
00:01:17,733 --> 00:01:21,000
值不值得我们投入这么夸张的算力

41
00:01:21,000 --> 00:01:21,833
去实现

42
00:01:22,033 --> 00:01:22,700
那第三个

43
00:01:22,733 --> 00:01:25,066
就是 Deepseek 的 123 了

44
00:01:25,066 --> 00:01:26,933
那大家看到 Deepseek 从 Moe

45
00:01:26,966 --> 00:01:28,933
然后到现在的 Deepseek V3

46
00:01:29,166 --> 00:01:31,666
整体的模型规模在增大

47
00:01:31,666 --> 00:01:34,133
模型的激活数在降低

48
00:01:34,233 --> 00:01:35,166
所以说蛮有意思的

49
00:01:35,200 --> 00:01:37,933
就是我们引入了最后一个概念

50
00:01:38,400 --> 00:01:40,833
最近的 Moe LLM 

51
00:01:40,966 --> 00:01:42,733
从大参数少专家

52
00:01:42,766 --> 00:01:45,666
慢慢转向小参数多专家这里

53
00:01:45,666 --> 00:01:47,233
面为什么这么转变

54
00:01:47,233 --> 00:01:48,733
对整个生态

55
00:01:48,766 --> 00:01:51,666
和整个大模型的算法的演进

56
00:01:51,833 --> 00:01:52,900
和我们的算力消耗

57
00:01:52,933 --> 00:01:55,000
有哪些新的启示

58
00:01:55,000 --> 00:01:57,200
这是我们今天重点关心的内容

59
00:01:58,866 --> 00:02:01,266
我们现在还是看一下整个视频

60
00:02:01,266 --> 00:02:03,666
或者整个 Moe 系列的一个目录大纲

61
00:02:03,666 --> 00:02:06,433
我们之前跟大家重点的去分享了

62
00:02:06,433 --> 00:02:08,300
Moe 的基础的介绍

63
00:02:08,366 --> 00:02:09,833
Moe 的前世今生

64
00:02:09,833 --> 00:02:11,633
它的一个来源的发展

65
00:02:11,633 --> 00:02:14,666
然后重点来看一下 Moe 相关的论文

66
00:02:14,833 --> 00:02:15,666
了解完论文

67
00:02:15,666 --> 00:02:18,533
ZOMI 觉得最核心的一个 Moe 的内容

68
00:02:18,733 --> 00:02:20,666
应该是这一节跟这一节了

69
00:02:20,666 --> 00:02:22,866
第一节是宏观性的介绍 moe

70
00:02:22,866 --> 00:02:23,500
那这一节

71
00:02:23,533 --> 00:02:26,133
是更细节的介绍 moe 所有的模块

72
00:02:26,133 --> 00:02:27,733
它的正式的原理

73
00:02:27,733 --> 00:02:28,266
那今天

74
00:02:28,266 --> 00:02:31,066
我们来到一个大模型遇上 moe

75
00:02:31,066 --> 00:02:34,133
看一下大模型最新的内容有哪些

76
00:02:34,800 --> 00:02:35,466
首先第一个

77
00:02:35,466 --> 00:02:37,500
就是 Mistral AI

78
00:02:37,533 --> 00:02:39,766
是欧洲的一个公司

79
00:02:39,800 --> 00:02:40,833
那 Mistral AI

80
00:02:40,833 --> 00:02:44,033
其实成立于 23 年的法国的 AI 公司

81
00:02:44,233 --> 00:02:45,733
由之前谷歌的 Deepmind

82
00:02:45,766 --> 00:02:48,133
还有 Meta 相关的人力所组成的

83
00:02:48,133 --> 00:02:49,966
位于法国的巴黎

84
00:02:50,033 --> 00:02:51,766
作为一个 AI 的领域

85
00:02:51,800 --> 00:02:54,800
或者 AI 的大模型的代表公司

86
00:02:54,800 --> 00:02:57,066
它的目标是对标 open AI

87
00:02:57,066 --> 00:02:59,733
可惜在 Deepseek 这把风里面

88
00:02:59,866 --> 00:03:01,566
open AI 都挡不住了

89
00:03:01,600 --> 00:03:03,000
何况 Mistral AI

90
00:03:03,366 --> 00:03:04,433
那不管怎么样

91
00:03:04,433 --> 00:03:05,433
整个 Mistral AI

92
00:03:05,433 --> 00:03:08,300
在 23 年年底到 24 年年初的时候

93
00:03:08,333 --> 00:03:10,066
还是非常的风光的

94
00:03:10,066 --> 00:03:11,900
一时间因为他重点

95
00:03:11,933 --> 00:03:15,033
发布了一个模型叫做 Mistral 8 乘以 7B

96
00:03:15,166 --> 00:03:15,566
第一个

97
00:03:15,566 --> 00:03:18,133
把以前的 Llama 7B 这一系列的模型

98
00:03:18,133 --> 00:03:19,733
通过 MOE 的架构

99
00:03:19,766 --> 00:03:20,933
拓展了 8 个

100
00:03:20,933 --> 00:03:22,733
专家说有 8 乘以 7B

101
00:03:23,066 --> 00:03:26,966
然后到了 24 年的年底又发布了另外一个新的模型

102
00:03:27,000 --> 00:03:28,633
8 乘以 22B

103
00:03:28,633 --> 00:03:30,133
同样 8 个专家

104
00:03:30,166 --> 00:03:33,266
每个专家大概有 22B 的一个参数量

105
00:03:33,266 --> 00:03:35,833
因此整个 Mister AI 总共

106
00:03:35,833 --> 00:03:38,033
在 Moe 架构发布了两个模型

107
00:03:38,033 --> 00:03:39,766
我们现在来看一下相关的内容

108
00:03:40,066 --> 00:03:42,166
首先就是总的参数量

109
00:03:42,366 --> 00:03:44,833
整个 Mistral 8 乘以 22B

110
00:03:44,833 --> 00:03:47,666
总体的参数量是 176B

111
00:03:47,833 --> 00:03:51,666
而 8 乘以 7B 的参数量是 46B

112
00:03:51,733 --> 00:03:52,533
那蛮有意思的

113
00:03:52,533 --> 00:03:53,833
大家一定要注意了

114
00:03:53,833 --> 00:03:56,333
这里面 7*8 等于 56

115
00:03:56,366 --> 00:03:58,466
但实际上的参数量是 46

116
00:03:58,466 --> 00:04:00,833
我们等一下会简单开个白板

117
00:04:00,833 --> 00:04:03,100
跟大家一起去讨论一下的

118
00:04:03,566 --> 00:04:04,166
那接着

119
00:04:04,166 --> 00:04:06,766
就是一个激活的参数量

120
00:04:06,766 --> 00:04:09,166
那激活参数量是 390 亿

121
00:04:09,233 --> 00:04:10,700
这里面是 12 亿

122
00:04:10,733 --> 00:04:11,866
那专家数量

123
00:04:11,866 --> 00:04:12,533
都差不多

124
00:04:12,566 --> 00:04:13,800
一个是 8 个专家

125
00:04:13,833 --> 00:04:15,033
都需要 8 个专家

126
00:04:15,033 --> 00:04:17,433
每个专家就相当于他的一个参数量

127
00:04:17,433 --> 00:04:18,733
规模蛮大的

128
00:04:18,766 --> 00:04:19,800
那上下文

129
00:04:19,800 --> 00:04:22,600
一开始的一个 8 乘以 7B 是 32K

130
00:04:22,666 --> 00:04:24,533
到了那个 8 乘以 22B

131
00:04:24,833 --> 00:04:26,766
就把上下文扩充了

132
00:04:26,800 --> 00:04:28,000
扩大了一半

133
00:04:28,066 --> 00:04:28,766
那基本上

134
00:04:28,800 --> 00:04:30,733
都是以这种方式来去实现的

135
00:04:30,733 --> 00:04:31,866
而这次的语言

136
00:04:31,866 --> 00:04:34,233
可能会以欧洲的小语种为主

137
00:04:34,233 --> 00:04:36,033
因为它位于一个欧洲的公司

138
00:04:36,033 --> 00:04:38,133
收集欧洲的语料比较多

139
00:04:38,200 --> 00:04:40,266
所以说语言是比较多种的

140
00:04:40,266 --> 00:04:41,900
而且两个模型

141
00:04:41,933 --> 00:04:43,566
使用的都是 Apache

142
00:04:43,566 --> 00:04:46,366
Apache 的一个相关的协议

143
00:04:47,200 --> 00:04:48,333
那我们现在

144
00:04:48,333 --> 00:04:50,166
来到了第二个内容

145
00:04:50,166 --> 00:04:52,666
看一下 Grok 的 123

146
00:04:57,600 --> 00:04:58,433
所谓的 Grok

147
00:04:58,433 --> 00:04:59,333
这个模型

148
00:04:59,366 --> 00:05:00,766
是由 X AI 呀

149
00:05:00,766 --> 00:05:02,366
SAI 是 a 龙

150
00:05:02,433 --> 00:05:05,666
埃隆·马斯克不知道怎么读了

151
00:05:05,733 --> 00:05:07,633
创立的一个人工智能公司

152
00:05:07,633 --> 00:05:09,733
之前他投的是 open AI 可

153
00:05:09,766 --> 00:05:10,766
惜分家了

154
00:05:10,766 --> 00:05:13,200
分家出来就考了一个 X AI 了

155
00:05:13,200 --> 00:05:14,233
那这里面 X AI

156
00:05:14,233 --> 00:05:16,266
就发布了 Grok 的一个大模型

157
00:05:16,266 --> 00:05:18,133
这个大模型或者这一系列

158
00:05:18,366 --> 00:05:20,133
主要是基于混合专家

159
00:05:20,133 --> 00:05:21,533
也就 Moe 的架构

160
00:05:21,566 --> 00:05:23,033
那我们看一下整个 Grok

161
00:05:23,033 --> 00:05:25,666
它其实发布了三代的模型

162
00:05:25,666 --> 00:05:27,966
分别是一二三

163
00:05:28,000 --> 00:05:30,966
那第一弹就是 24 年的 3 月份发布的

164
00:05:31,000 --> 00:05:31,666
蛮有意思

165
00:05:31,666 --> 00:05:33,100
就是 24 年发布的时候

166
00:05:33,133 --> 00:05:36,633
网络模型的规模是 3,140 亿

167
00:05:36,633 --> 00:05:38,033
一共有 8 个专家

168
00:05:38,033 --> 00:05:40,066
每个专家激活两个

169
00:05:40,066 --> 00:05:41,666
每次激活两个专家

170
00:05:42,000 --> 00:05:42,633
那这个

171
00:05:42,633 --> 00:05:46,033
跟我们刚才讲到的 Mistral  AI 非常的像

172
00:05:46,033 --> 00:05:46,500
不过

173
00:05:46,533 --> 00:05:49,233
后面就没有给出更详细的路径了

174
00:05:49,233 --> 00:05:51,366
主要是第一个是开源的

175
00:05:51,633 --> 00:05:54,100
那上下文也是非常的长的

176
00:05:54,133 --> 00:05:55,833
一共有 8K 的一个 TOKEN

177
00:05:55,833 --> 00:05:57,233
那后面到的 Grok2

178
00:05:57,233 --> 00:05:59,166
就拓展到越来越长了

179
00:05:59,200 --> 00:06:00,833
那资源数蛮有意思

180
00:06:00,833 --> 00:06:01,766
就是可能我们

181
00:06:01,800 --> 00:06:03,533
其他都没有太多的公布

182
00:06:03,533 --> 00:06:05,766
因为它不是完全的开源

183
00:06:05,800 --> 00:06:07,333
所以说我们看一下 Grok3

184
00:06:07,333 --> 00:06:09,933
用的一开始是分多阶段训练的

185
00:06:09,933 --> 00:06:12,333
第一阶段用了 10 万卡的 H100

186
00:06:12,333 --> 00:06:13,233
那第二阶段

187
00:06:13,233 --> 00:06:16,133
用了一个 20 万卡的一个 H100

188
00:06:16,233 --> 00:06:18,766
如果对整个 Grok 怎么去训的话

189
00:06:18,800 --> 00:06:21,933
也可以看一下 ZOMI 的一个系列视频

190
00:06:21,933 --> 00:06:23,133
我们找一找

191
00:06:23,366 --> 00:06:24,833
整体的在

192
00:06:24,866 --> 00:06:26,566
在一个 AI 超节点里面

193
00:06:26,600 --> 00:06:30,033
就讲到了世界最大的 10 万卡的集群

194
00:06:30,033 --> 00:06:33,266
GPU X AI 里面的一个硬件集群

195
00:06:33,266 --> 00:06:34,900
是怎么去组网的

196
00:06:34,933 --> 00:06:36,966
然后这里面有两节内容

197
00:06:36,966 --> 00:06:40,333
专门的去讲 10 万卡集群的一个思考

198
00:06:40,333 --> 00:06:41,766
那后面的 X AI

199
00:06:41,766 --> 00:06:44,266
也就是 ZOMI 发完这两篇视频之后

200
00:06:44,433 --> 00:06:46,466
X AI 过了两个月就发布了

201
00:06:46,466 --> 00:06:47,900
也就 2025 年的 2 月份

202
00:06:47,933 --> 00:06:48,466
就发布了

203
00:06:48,466 --> 00:06:49,733
基于 20 万卡的

204
00:06:49,766 --> 00:06:52,000
明尼斯达州的一个 X AI 的

205
00:06:52,033 --> 00:06:53,966
超大规模的 a i 集群

206
00:06:54,000 --> 00:06:56,533
GPU 集群来训出了这个模型

207
00:06:56,600 --> 00:06:57,966
所以大家对这个硬件

208
00:06:57,966 --> 00:07:00,133
也可以回头看看相关的视频

209
00:07:01,000 --> 00:07:02,466
那这里面我们可以看到

210
00:07:02,466 --> 00:07:04,766
真的是这个 X AI Grok 3

211
00:07:04,800 --> 00:07:07,066
是由马斯克亲自去发布的

212
00:07:07,066 --> 00:07:07,666
蛮有意思

213
00:07:07,666 --> 00:07:10,533
就是里面说到了他基本上

214
00:07:10,566 --> 00:07:12,066
全线的去超越了

215
00:07:12,066 --> 00:07:13,466
在各个榜单 Demini

216
00:07:13,466 --> 00:07:14,133
Deepseek

217
00:07:14,166 --> 00:07:17,400
Claude 还有 GPT-4o 相关的内容

218
00:07:17,400 --> 00:07:18,600
能够实现一个多模态

219
00:07:18,600 --> 00:07:20,633
还有对应的 reasoning 跟 model

220
00:07:20,666 --> 00:07:21,933
在编程和数学

221
00:07:21,966 --> 00:07:23,000
创意写作

222
00:07:23,333 --> 00:07:25,000
指令遵循长序列

223
00:07:25,000 --> 00:07:28,200
还有多轮对话的排名都是 number 1

224
00:07:28,366 --> 00:07:29,366
ZOMI 老师你好

225
00:07:29,366 --> 00:07:34,233
哎马斯克的 X AI 使用了 20 万张 GPU

226
00:07:34,333 --> 00:07:35,766
20 万张哎

227
00:07:35,766 --> 00:07:38,600
训练一个 Grok 略强于 Deepseek

228
00:07:38,966 --> 00:07:39,833
那这个时候

229
00:07:39,833 --> 00:07:41,366
其实我想问的就是

230
00:07:41,400 --> 00:07:44,133
有没有必要继续讲 scaling Law

231
00:07:44,133 --> 00:07:46,933
有没有必要用到 20 万张卡

232
00:07:47,066 --> 00:07:48,500
那么的夸张

233
00:07:48,966 --> 00:07:51,800
哎小新提的这个问题还挺好的

234
00:07:52,400 --> 00:07:53,333
其实我们会发现

235
00:07:53,333 --> 00:07:55,466
根据官网的批漏的数据

236
00:07:55,466 --> 00:07:57,366
XAI 使用了 122 天

237
00:07:57,400 --> 00:08:00,566
让首批 10 万卡的集群同时使用

238
00:08:00,566 --> 00:08:02,566
后面又花费了 92 天

239
00:08:02,566 --> 00:08:06,600
把 10 万卡计数 scaling out 到一百二十万张卡

240
00:08:06,833 --> 00:08:09,266
这个时候对比起 Deepseek

241
00:08:09,266 --> 00:08:10,700
真正的 V3 训出来

242
00:08:10,733 --> 00:08:13,400
端到端的只使用了两 k 的集群

243
00:08:13,400 --> 00:08:15,733
不管真正可能 Deepseek 训练的时候

244
00:08:15,733 --> 00:08:17,733
会发挥 1 万张卡也好

245
00:08:17,733 --> 00:08:19,600
20 万张卡对比 1 万张卡

246
00:08:19,600 --> 00:08:22,000
其实也是非常不小的开销

247
00:08:22,000 --> 00:08:26,066
所以 ZOMI 认为这里面就仁者见仁智者见智了

248
00:08:26,233 --> 00:08:28,300
可能有些小伙伴和友商

249
00:08:28,333 --> 00:08:29,333
如果他有钱

250
00:08:29,333 --> 00:08:30,266
能够堆料

251
00:08:30,400 --> 00:08:32,133
确实没有必要花那么大力气

252
00:08:32,133 --> 00:08:33,933
在 infer 层面的一个优化

253
00:08:33,966 --> 00:08:36,233
但是随着 Deepseek 这几天的开业

254
00:08:36,233 --> 00:08:37,366
我们可以看到

255
00:08:37,433 --> 00:08:39,033
真正的未来的方向

256
00:08:39,033 --> 00:08:39,233
是

257
00:08:39,233 --> 00:08:42,100
不断的去优化我们的一个集群的性能

258
00:08:42,133 --> 00:08:43,233
集群的利用率

259
00:08:43,633 --> 00:08:46,233
才能够充分的把集群的算力

260
00:08:46,233 --> 00:08:47,366
压榨干净

261
00:08:47,400 --> 00:08:49,833
这也是现在大家希望做的一个事情

262
00:08:49,833 --> 00:08:52,133
因为摩尔线程或者摩尔定律

263
00:08:52,166 --> 00:08:53,733
已经越来越慢了

264
00:08:53,733 --> 00:08:55,833
甚至有人说摩尔定律在消失

265
00:08:55,833 --> 00:08:57,266
那这个大背景之下

266
00:08:57,266 --> 00:09:00,633
大家不断的去细抠 infer 场面的内容

267
00:09:01,566 --> 00:09:02,733
所以打开 ZOMI

268
00:09:02,733 --> 00:09:03,233
这里面

269
00:09:03,233 --> 00:09:05,133
ZOMI 还是希望大家多去学学

270
00:09:05,166 --> 00:09:06,533
AI 系统的东西

271
00:09:06,533 --> 00:09:08,833
那 ZOMI 整个 AI 系统相关的视频

272
00:09:08,833 --> 00:09:11,066
都已经在这里面做了个集合

273
00:09:11,066 --> 00:09:11,566
然后

274
00:09:11,600 --> 00:09:14,600
欢迎大家根据这个相关的内容

275
00:09:14,800 --> 00:09:17,600
去多多学习 AI infer 相关的内容

276
00:09:18,033 --> 00:09:18,766
如果有兴趣的

277
00:09:18,800 --> 00:09:19,566
也可以看看

278
00:09:19,566 --> 00:09:22,600
ZOMI AI system 相关的这个系列

279
00:09:22,600 --> 00:09:26,366
还有 AI infer 这个相关的文章系列

280
00:09:26,366 --> 00:09:28,666
里面具有非常多的内容

281
00:09:28,666 --> 00:09:31,333
每一章或者都已经归档好了

282
00:09:32,933 --> 00:09:35,266
了解完刚才 Mistral 跟 Grok 之外

283
00:09:35,266 --> 00:09:38,166
我们现在来看一下今年最靓的仔

284
00:09:38,200 --> 00:09:39,866
Deepseek 的 123

285
00:09:41,533 --> 00:09:42,266
那蛮有意思的

286
00:09:42,266 --> 00:09:44,033
就是 Deepseek 的深度求索

287
00:09:44,033 --> 00:09:46,966
其实才成立于 2023 年的 7 月份

288
00:09:47,033 --> 00:09:49,433
由杭州的一个量化深度巨头了

289
00:09:49,433 --> 00:09:51,233
幻方量化来创立的

290
00:09:51,233 --> 00:09:52,933
那这个公司刚成立的时候

291
00:09:52,966 --> 00:09:56,033
最重要的就是专注于 LLM 

292
00:09:56,033 --> 00:09:57,033
所以说 Deepseek

293
00:09:57,033 --> 00:09:58,700
是一个重要的 LLM 

294
00:09:59,000 --> 00:10:01,166
里面唯一的目标就是推动人工

295
00:10:01,166 --> 00:10:03,200
智能 AGI 的一个发展

296
00:10:03,266 --> 00:10:03,966
那蛮有意思

297
00:10:04,000 --> 00:10:05,133
就是你会发现

298
00:10:05,133 --> 00:10:08,033
最近的 Deepseek 的一个开源周

299
00:10:08,033 --> 00:10:09,133
最核心的技术

300
00:10:09,166 --> 00:10:10,366
就是做 infer

301
00:10:10,366 --> 00:10:13,166
infer 层面的研究和算法层面的研究

302
00:10:13,166 --> 00:10:16,400
使得通过 infer 去减少我们的一个算力

303
00:10:16,400 --> 00:10:18,400
或者提升我们的算力的利用率

304
00:10:18,400 --> 00:10:20,566
那通过算法跟依法程的结合

305
00:10:20,566 --> 00:10:22,833
更好的使得我们的模型的效果

306
00:10:22,833 --> 00:10:24,533
发挥极致的效率

307
00:10:24,866 --> 00:10:25,566
那这里面

308
00:10:25,600 --> 00:10:27,200
整个演进的路线

309
00:10:27,200 --> 00:10:28,666
也就比较有意思

310
00:10:28,666 --> 00:10:30,300
从它成立半年之后

311
00:10:30,333 --> 00:10:31,633
就发布了第一个模型

312
00:10:31,633 --> 00:10:32,766
或者第一代的模型

313
00:10:32,800 --> 00:10:34,600
叫做 Deepseek Moe

314
00:10:34,966 --> 00:10:36,533
接着在半年之后

315
00:10:36,533 --> 00:10:38,033
又发布了第二代的模型

316
00:10:38,033 --> 00:10:39,533
叫做 Deepseek V2

317
00:10:39,733 --> 00:10:41,333
在半年之后

318
00:10:41,333 --> 00:10:43,366
基本上你看到每个大模型

319
00:10:43,366 --> 00:10:45,800
都是半年左右为周期

320
00:10:45,800 --> 00:10:47,466
也就是我训练两个月

321
00:10:47,466 --> 00:10:49,533
一般来说一个大模型端到端

322
00:10:49,566 --> 00:10:51,533
从预训练到后训练到微调了

323
00:10:51,533 --> 00:10:53,833
大概是两个月到 3 个月的时间

324
00:10:53,833 --> 00:10:54,633
那基本上

325
00:10:54,633 --> 00:10:57,166
训练两三次就已经半年了

326
00:10:57,200 --> 00:10:58,766
所以说大模型的演进

327
00:10:58,766 --> 00:11:00,000
也是按这个节奏的

328
00:11:00,000 --> 00:11:01,033
比较明显

329
00:11:01,133 --> 00:11:03,033
那到了 24 年的 1 月份

330
00:11:03,033 --> 00:11:03,966
就 Deepseek

331
00:11:04,000 --> 00:11:05,466
就发布了 V3 模型

332
00:11:05,466 --> 00:11:07,266
那真正让 Deepseek 出圈的

333
00:11:07,266 --> 00:11:09,733
是 Deepseek R1 这个系列的模型

334
00:11:09,800 --> 00:11:10,766
那我们现在看一下

335
00:11:10,766 --> 00:11:12,433
网络模型的参数量

336
00:11:12,433 --> 00:11:13,866
从 Deepseek 开始之后

337
00:11:13,866 --> 00:11:15,833
有一个比较有意思的变化

338
00:11:16,033 --> 00:11:19,500
Deepseek 的一个 Moe 模型最大的是 145B

339
00:11:19,533 --> 00:11:20,766
到了 Deepseek V2

340
00:11:20,766 --> 00:11:24,566
是 236BDeepseek V3 是 671B

341
00:11:24,600 --> 00:11:26,133
网络模型的参数量

342
00:11:26,133 --> 00:11:26,933
在增大

343
00:11:26,933 --> 00:11:30,133
但是参数的激活同样也在增大

344
00:11:30,133 --> 00:11:32,000
21B 371B

345
00:11:32,000 --> 00:11:33,666
那么一次就我们看一下

346
00:11:33,666 --> 00:11:35,266
路由的专家数量

347
00:11:35,533 --> 00:11:36,833
比较关心的这个

348
00:11:36,833 --> 00:11:38,166
那路由的专家数量

349
00:11:38,200 --> 00:11:39,966
从 160 个到

350
00:11:39,966 --> 00:11:42,400
256 个也就是 expert

351
00:11:42,400 --> 00:11:43,400
越来越多

352
00:11:44,000 --> 00:11:46,233
但是里面的共享专家数

353
00:11:46,233 --> 00:11:47,733
却越来越小

354
00:11:47,766 --> 00:11:48,433
那蛮有意思

355
00:11:48,433 --> 00:11:51,033
就是我们之前讲到的 Gork 也好

356
00:11:51,033 --> 00:11:51,933
Mistral 也好

357
00:11:52,033 --> 00:11:54,500
里面的专家数只有 8 个

358
00:11:54,533 --> 00:11:57,066
现在都已经变成 256 个了

359
00:11:57,066 --> 00:11:58,133
非常的多

360
00:11:58,166 --> 00:11:59,566
这里面我们也可以看到

361
00:11:59,566 --> 00:12:01,400
整个 Moe 的架构

362
00:12:01,800 --> 00:12:03,166
开始改变了

363
00:12:03,166 --> 00:12:05,966
我们的专家数量越来越多了

364
00:12:05,966 --> 00:12:06,633
那因此

365
00:12:06,633 --> 00:12:09,666
我们就会进入到下一个内容的环节

366
00:12:10,400 --> 00:12:12,800
看一下我们的思考跟小结了

367
00:12:15,266 --> 00:12:15,900
那 ZOMI

368
00:12:15,933 --> 00:12:17,600
在整个 Deepseek 里面

369
00:12:17,600 --> 00:12:19,200
其实之前的一系列文章

370
00:12:19,200 --> 00:12:20,866
都分享的非常的多了

371
00:12:20,866 --> 00:12:22,100
有兴趣的小伙伴们

372
00:12:22,133 --> 00:12:23,166
也可以去看一下

373
00:12:23,166 --> 00:12:25,133
ZOMI 分享的 Deepseek 相关的内容

374
00:12:25,466 --> 00:12:27,966
那 ZOMI 主要是想跟大家去探讨一下

375
00:12:28,000 --> 00:12:29,833
大参数少专家

376
00:12:29,833 --> 00:12:32,433
像 Mistral 跟 Grok 这种的模型

377
00:12:32,533 --> 00:12:34,733
跟当前幻方的这种趋势

378
00:12:34,733 --> 00:12:37,666
小参数多专家有什么区别

379
00:12:37,666 --> 00:12:40,733
到底哪个才是 AGI 的未来

380
00:12:40,766 --> 00:12:42,933
虽然大家都说 Moe Moe

381
00:12:42,933 --> 00:12:45,600
但是 Moe 里面也有很多讲究的

382
00:12:46,033 --> 00:12:48,166
那我们现在来看一下第一个内容

383
00:12:48,200 --> 00:12:50,233
就是大参数小专家

384
00:12:50,333 --> 00:12:53,033
早期的 Moe 说实话也没多早了

385
00:12:53,033 --> 00:12:56,933
也就是 24 年的年初到 24 年的年底

386
00:12:57,000 --> 00:12:59,633
大部分都是走大参数小专家

387
00:12:59,633 --> 00:13:01,300
那所谓的大参数小专家

388
00:13:01,333 --> 00:13:02,766
就像 Mistral 一样

389
00:13:02,766 --> 00:13:04,666
一共有八个专家

390
00:13:04,666 --> 00:13:05,766
然后参数量

391
00:13:05,800 --> 00:13:06,866
是非常的大的

392
00:13:06,866 --> 00:13:07,833
每个专家

393
00:13:07,833 --> 00:13:10,566
都在大概参数量是 7B 左右

394
00:13:11,166 --> 00:13:12,233
那早期的 Moe

395
00:13:12,233 --> 00:13:14,133
我们看一下它的主要的特性

396
00:13:14,166 --> 00:13:17,033
早期的 Moe 主要采用少量的专家

397
00:13:17,066 --> 00:13:18,433
所以说大概八个

398
00:13:18,433 --> 00:13:20,433
16 个呀每个专家的数量

399
00:13:20,433 --> 00:13:22,233
很大那这个时候

400
00:13:22,233 --> 00:13:23,633
专家的模型设计

401
00:13:23,633 --> 00:13:24,900
相对复杂

402
00:13:25,000 --> 00:13:28,733
主要是把 Transformer 层的 FFN 替换掉

403
00:13:28,733 --> 00:13:30,966
那所谓的复杂是指我们的一个权重

404
00:13:30,966 --> 00:13:31,833
非常的大

405
00:13:32,033 --> 00:13:32,566
那另外的话

406
00:13:32,600 --> 00:13:33,600
它的一个门控

407
00:13:33,600 --> 00:13:34,433
就是路由

408
00:13:34,433 --> 00:13:35,633
设计相对简单

409
00:13:35,666 --> 00:13:38,766
主要是通过 Softmax 函数去分配权重的

410
00:13:38,800 --> 00:13:40,766
因为专家数量太少

411
00:13:40,933 --> 00:13:42,866
均衡负载没必要做太多

412
00:13:42,866 --> 00:13:45,733
简单的一个 Softmax 能解决很多问题

413
00:13:45,933 --> 00:13:47,666
所以我们看一下它的一个 Pro

414
00:13:47,666 --> 00:13:49,333
也就是所谓的好处了

415
00:13:49,400 --> 00:13:50,633
优点那第一个

416
00:13:50,633 --> 00:13:52,333
就是提升了计算的效率

417
00:13:52,566 --> 00:13:53,400
所谓的计算效率

418
00:13:53,400 --> 00:13:55,933
主要是我们通过增加专家的数量

419
00:13:55,966 --> 00:13:58,033
每个专家的数量的参数量

420
00:13:58,033 --> 00:13:59,266
其实非常的大的

421
00:13:59,266 --> 00:14:01,133
但是我们整体是降低了

422
00:14:01,166 --> 00:14:03,000
单个专家的计算复杂度

423
00:14:03,000 --> 00:14:04,566
反正矩阵层就行了

424
00:14:04,566 --> 00:14:06,966
提升了整个计算的一个效率

425
00:14:07,000 --> 00:14:09,533
如果这里面的提升的计算效率

426
00:14:09,533 --> 00:14:11,933
不是跟 xxx 或者那个稠密的比

427
00:14:12,166 --> 00:14:14,200
而是跟多专家去比的

428
00:14:14,200 --> 00:14:17,033
我少专家我肯定计算更加密集嘛

429
00:14:17,033 --> 00:14:17,733
在一个地方

430
00:14:17,766 --> 00:14:18,966
在一个单元内

431
00:14:19,033 --> 00:14:20,166
那我多专家的话

432
00:14:20,200 --> 00:14:22,033
我需要更多的通讯嘛

433
00:14:22,033 --> 00:14:23,833
所以提升我们的计算的效率

434
00:14:23,833 --> 00:14:24,533
那第二点

435
00:14:24,566 --> 00:14:25,866
就是容易训练了

436
00:14:26,266 --> 00:14:28,466
少专家其实更容易训练

437
00:14:28,466 --> 00:14:29,933
因为专家数越多

438
00:14:29,966 --> 00:14:33,433
我们越不利于我们的计算的一个负载

439
00:14:33,466 --> 00:14:35,233
所以说专家越来越多了

440
00:14:35,233 --> 00:14:38,033
就会出现各种各样的均衡负载的问题

441
00:14:38,266 --> 00:14:41,166
所以因此大参数小专家

442
00:14:41,200 --> 00:14:43,800
是比较利于我们整体的训练的

443
00:14:43,800 --> 00:14:45,333
也是为什么一开始

444
00:14:45,333 --> 00:14:48,366
我们的一个 LLM 加 MOE

445
00:14:48,366 --> 00:14:51,166
会选择大参数小专家的原因

446
00:14:51,166 --> 00:14:52,933
就为了更好的训练出

447
00:14:52,933 --> 00:14:54,566
我们的一个模型出来

448
00:14:54,566 --> 00:14:56,233
或者达到我们的模型效果

449
00:14:56,600 --> 00:14:58,366
但是大参数小

450
00:14:58,366 --> 00:15:00,433
专家也有一些坏处

451
00:15:00,433 --> 00:15:01,533
这些坏处比较明显

452
00:15:01,566 --> 00:15:02,266
我们看一下

453
00:15:02,266 --> 00:15:02,633
第一个

454
00:15:02,633 --> 00:15:04,333
就计算成本高

455
00:15:04,366 --> 00:15:07,600
因为我们每个专家数量都非常大的

456
00:15:07,600 --> 00:15:08,333
参数量

457
00:15:08,333 --> 00:15:11,133
导致我们的显存跟内存的需求很高

458
00:15:11,333 --> 00:15:11,600
第六个

459
00:15:11,600 --> 00:15:13,000
就专家负载不均衡

460
00:15:13,000 --> 00:15:14,600
那专家负载不均衡

461
00:15:14,600 --> 00:15:16,366
它是个永恒的话题

462
00:15:16,366 --> 00:15:18,366
不管是大参数小专家

463
00:15:18,366 --> 00:15:20,366
还是小参数多专家

464
00:15:20,433 --> 00:15:22,133
都会存在这个问题

465
00:15:22,366 --> 00:15:22,866
那最后一个

466
00:15:22,866 --> 00:15:24,866
就是专家的利用率低

467
00:15:24,866 --> 00:15:26,133
因为我们会发现

468
00:15:26,166 --> 00:15:26,866
这个时候

469
00:15:26,866 --> 00:15:28,666
我们的专家数量少

470
00:15:28,666 --> 00:15:31,766
所以会导致专家的专业化程度不够高

471
00:15:31,800 --> 00:15:34,466
每个专家都要成为一个八面手

472
00:15:34,466 --> 00:15:36,266
学习很多的知识

473
00:15:36,266 --> 00:15:37,733
很多的内容

474
00:15:37,800 --> 00:15:39,666
所以说我们觉得专家的利用率

475
00:15:39,666 --> 00:15:41,066
可能普遍较低

476
00:15:41,233 --> 00:15:44,066
也就是我们现在所讲到的 LLM 

477
00:15:44,066 --> 00:15:47,166
其实大家只是利用了他 30%的能力

478
00:15:47,200 --> 00:15:50,233
LLM 还有很多潜力待我们挖掘的

479
00:15:51,466 --> 00:15:51,933
那第二个

480
00:15:51,966 --> 00:15:53,800
我们要看一下当前的趋势

481
00:15:53,800 --> 00:15:55,366
跟刚前完全不一样了

482
00:15:55,366 --> 00:15:58,133
现在是小参数多专家

483
00:15:58,133 --> 00:16:00,466
刚才是大参数小专家

484
00:16:00,833 --> 00:16:01,633
典型的代表

485
00:16:01,633 --> 00:16:02,566
就是 Deepseek

486
00:16:02,600 --> 00:16:05,400
利用了一个细腻度的专家的划分

487
00:16:05,466 --> 00:16:08,100
反正把以前的 128 个专家

488
00:16:08,133 --> 00:16:11,533
裂变成为我们的 256 个专家

489
00:16:11,533 --> 00:16:14,333
那每一个 FFN 层的一个网络模型的

490
00:16:14,333 --> 00:16:15,933
参数缩半

491
00:16:15,933 --> 00:16:18,266
但是数量增倍

492
00:16:18,266 --> 00:16:19,933
这种方式来去实现的

493
00:16:19,966 --> 00:16:22,000
所以叫做细腻度的专家划分

494
00:16:22,000 --> 00:16:23,466
和动态的路由

495
00:16:23,466 --> 00:16:26,333
那特点就是每个专家的参数量很少

496
00:16:26,366 --> 00:16:28,266
但是专家数量很多

497
00:16:28,666 --> 00:16:31,333
所以说能够降低显存的一个优势

498
00:16:31,366 --> 00:16:32,066
那我们看一下

499
00:16:32,066 --> 00:16:35,033
整个小参数多专家的一个能力

500
00:16:35,033 --> 00:16:35,733
那现在

501
00:16:35,766 --> 00:16:37,933
你会发现从今年开始

502
00:16:37,966 --> 00:16:39,466
也就是 25 年开始

503
00:16:39,466 --> 00:16:41,366
越来越多的 Moe 的模型

504
00:16:41,400 --> 00:16:44,233
倾向采用于多专家的形式

505
00:16:44,233 --> 00:16:47,033
128 256 512

506
00:16:47,066 --> 00:16:49,466
每个专家的参数量比较小

507
00:16:49,666 --> 00:16:52,100
我们叫做小参数多专家

508
00:16:52,433 --> 00:16:53,666
第二点我们看一下

509
00:16:53,666 --> 00:16:54,833
专家的模型

510
00:16:54,833 --> 00:16:56,100
更加轻量化了

511
00:16:56,133 --> 00:16:58,166
因为我们细腻度的划分完之后

512
00:16:58,166 --> 00:16:59,133
不同的专家

513
00:16:59,133 --> 00:17:02,233
可以专注于不同的一些相关的领域

514
00:17:02,566 --> 00:17:03,533
但是一个问题

515
00:17:03,533 --> 00:17:04,666
就是门控网络

516
00:17:04,666 --> 00:17:05,533
我们的路由

517
00:17:05,566 --> 00:17:06,933
设计更加复杂了

518
00:17:06,933 --> 00:17:09,033
需要引入更多的动态路由

519
00:17:09,033 --> 00:17:10,133
和负载均衡

520
00:17:10,233 --> 00:17:11,866
去优化我们的专家

521
00:17:11,866 --> 00:17:15,033
然后更好的平衡我们整体集群的计算

522
00:17:15,333 --> 00:17:15,566
同样

523
00:17:15,566 --> 00:17:18,866
我们看一下小参数多专家的一些好处

524
00:17:18,866 --> 00:17:20,900
第一个就是计算效率

525
00:17:20,933 --> 00:17:22,600
可能会实现更高

526
00:17:22,600 --> 00:17:24,533
因为小参数多专家

527
00:17:24,533 --> 00:17:26,333
减少每次推理的计算量

528
00:17:26,333 --> 00:17:27,733
这里面主要是指推理

529
00:17:27,766 --> 00:17:28,766
通过动态路由了

530
00:17:28,766 --> 00:17:31,266
优化整个专家的负载均衡

531
00:17:31,466 --> 00:17:33,733
那第二个就是扩展性更好

532
00:17:33,966 --> 00:17:36,133
因为之前的是小专家

533
00:17:36,133 --> 00:17:37,533
其实不利于我们扩展的

534
00:17:37,533 --> 00:17:38,466
因为数据

535
00:17:38,466 --> 00:17:40,633
都集中在那几个专家去处理

536
00:17:40,633 --> 00:17:41,300
那现在

537
00:17:41,333 --> 00:17:42,200
很多专家

538
00:17:42,200 --> 00:17:44,566
可以做很多不同的数据的泛化

539
00:17:44,566 --> 00:17:45,866
和相关的理解

540
00:17:45,866 --> 00:17:47,666
所以说你看到千问也好

541
00:17:47,666 --> 00:17:48,433
Deepseek 也好

542
00:17:48,433 --> 00:17:49,900
还是 open AI 也好

543
00:17:49,933 --> 00:17:50,466
你会发现

544
00:17:50,466 --> 00:17:53,566
他们都慢慢的转向 Moe 的架构了

545
00:17:54,333 --> 00:17:57,000
那最后一个就是部署的成本更低了

546
00:17:57,000 --> 00:17:58,366
小参数的专家

547
00:17:58,366 --> 00:18:01,000
现在很明显是去降低了内存的

548
00:18:01,000 --> 00:18:02,400
显存的需求的

549
00:18:02,400 --> 00:18:04,933
适合在有限的资源里面进行一个部署

550
00:18:04,933 --> 00:18:06,633
但是这个不完全成立

551
00:18:06,633 --> 00:18:08,733
因为在整个 Moe 架构里面

552
00:18:08,766 --> 00:18:11,733
现在部署成本是非常的高的

553
00:18:11,733 --> 00:18:13,333
乃至于最近 Deepseek

554
00:18:13,333 --> 00:18:15,033
也引起了一个很大的争议

555
00:18:15,033 --> 00:18:17,066
它到底部署成本怎么样

556
00:18:17,066 --> 00:18:18,300
我们将会

557
00:18:18,333 --> 00:18:21,133
留在 Deepseek 的那一系列的视频里面

558
00:18:21,133 --> 00:18:24,133
去分享 Deepseek 给这种 MUD 的架构

559
00:18:24,133 --> 00:18:26,633
它的部署成本到底怎么去核算

560
00:18:27,766 --> 00:18:29,666
那最后一个环节了

561
00:18:29,666 --> 00:18:30,766
我们要对比一下

562
00:18:30,800 --> 00:18:32,233
大参数小专家

563
00:18:32,233 --> 00:18:34,766
跟小参数多专家的一个区别

564
00:18:35,066 --> 00:18:36,100
本质上

565
00:18:36,200 --> 00:18:37,733
为什么会有这两个

566
00:18:37,733 --> 00:18:40,566
到底是专家的数量的比较

567
00:18:40,733 --> 00:18:41,966
本质的问题就在于

568
00:18:41,966 --> 00:18:45,566
模型的规模跟细算的一个效率的平衡

569
00:18:46,000 --> 00:18:48,033
如果我们有了新的方案

570
00:18:48,033 --> 00:18:49,466
就细腻度的专家分工

571
00:18:49,466 --> 00:18:51,033
跟稀疏的激活路由

572
00:18:51,033 --> 00:18:53,366
能够实现更高的参数量

573
00:18:53,433 --> 00:18:55,266
因为通过小参数多专家

574
00:18:55,266 --> 00:18:57,866
可以让网络模型的容量上去

575
00:18:58,066 --> 00:18:59,333
重点的是模型容量

576
00:18:59,366 --> 00:19:01,000
而不是指模型的参数量

577
00:19:01,000 --> 00:19:03,466
大家知道模型的容量就像我们的大脑

578
00:19:03,466 --> 00:19:04,266
我们的脑袋

579
00:19:04,266 --> 00:19:06,933
跟小老鼠的脑袋的容量是不一样的

580
00:19:06,966 --> 00:19:08,033
会跟我的小猫咪

581
00:19:08,033 --> 00:19:11,233
小维尼大脑的容量是不一样的

582
00:19:13,366 --> 00:19:15,033
所以说我们的大脑越大

583
00:19:15,033 --> 00:19:16,366
模型的容量越高

584
00:19:16,633 --> 00:19:17,233
那第二个

585
00:19:17,233 --> 00:19:19,533
就是更低的推理成本了

586
00:19:19,566 --> 00:19:22,566
因为我们现在的激活也就是很小了

587
00:19:22,566 --> 00:19:24,166
因为小参数嘛

588
00:19:24,166 --> 00:19:25,233
所以每一次推理了

589
00:19:25,233 --> 00:19:27,433
都激活某个专家或者部分专家

590
00:19:27,466 --> 00:19:30,300
所以说整体的推理真正的成本会更少

591
00:19:30,466 --> 00:19:32,933
但是可能对于推理的一个 HBM

592
00:19:32,966 --> 00:19:34,533
显存的占比的要求

593
00:19:34,533 --> 00:19:35,566
可能会更高

594
00:19:35,933 --> 00:19:36,733
那另外的话

595
00:19:36,733 --> 00:19:38,833
更强的一个任务的适配性

596
00:19:39,000 --> 00:19:40,200
因为我们现在呀

597
00:19:40,200 --> 00:19:42,166
更逼近于专家即服务

598
00:19:42,166 --> 00:19:45,033
也就是 expert-as-a-service 的一个理想架构

599
00:19:45,033 --> 00:19:46,533
我们现有很多专家

600
00:19:46,566 --> 00:19:47,533
不同的专家

601
00:19:47,533 --> 00:19:49,166
学习不同的内容

602
00:19:49,166 --> 00:19:52,333
不同的专家有不同的一个学习的方案

603
00:19:52,333 --> 00:19:53,800
和大脑的容量

604
00:19:54,166 --> 00:19:55,633
那未来我们乐观

605
00:19:55,633 --> 00:19:56,466
的去预计

606
00:19:56,466 --> 00:19:58,333
随着路由算法和硬件的优化

607
00:19:58,566 --> 00:19:59,666
MOE 的架构

608
00:19:59,666 --> 00:20:01,133
特别是小参数的多

609
00:20:01,166 --> 00:20:01,800
专家

610
00:20:01,800 --> 00:20:05,000
有可能进一步向大规模的一个发展

611
00:20:05,033 --> 00:20:07,333
AGI 的很重要的发展路径

612
00:20:07,666 --> 00:20:08,533
那今天的内容

613
00:20:08,566 --> 00:20:09,466
就先到这里

614
00:20:09,466 --> 00:20:10,066
谢谢各位

615
00:20:10,066 --> 00:20:10,900
拜了个拜

