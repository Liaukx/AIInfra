{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bff34f2",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 03: MFU 模型利用率评估\n",
    "\n",
    "模型算力利用率（Model FLOPs Utilization, MFU）是评估 AI 模型训练效率的关键指标，它衡量了模型在实际训练中对硬件计算能力的利用程度。\n",
    "\n",
    "其计算公式为：\n",
    "\n",
    "$$ \\text{MFU} = \\frac{\\text{模型实际 FLOPs/迭代时间}}{\\text{硬件峰值 FLOPs}} = \\frac{\\text{实际 FLOPS}}{\\text{理论 FLOPS}} $$\n",
    "\n",
    "要准确计算 MFU，关键在于精确计算模型的 FLOPs。下面我们将分别推导稠密 Transformer 和 MoE 架构的计算公式。\n",
    "\n",
    "## 2. 稠密 Transformer 的计算量\n",
    "\n",
    "![](./images/CODE03MFU01.jpg)\n",
    "\n",
    "### 2.1 自注意力机制 FLOPs 计算\n",
    "\n",
    "对于单头注意力，计算过程可分为三个部分：\n",
    "\n",
    "1. **Q、K、V 投影**：\n",
    "   $$ \\text{FLOPs}_{\\text{proj}} = 3 \\times B \\times s \\times h \\times h \\times 2 = 6Bs h^2 $$\n",
    "\n",
    "2. **注意力计算**（QK^T 和 softmax）：\n",
    "   $$ \\text{FLOPs}_{\\text{attn}} = B \\times n_{\\text{heads}} \\times (2 \\times s \\times h_{\\text{per\\_head}} \\times s) = 2Bs^2 h $$\n",
    "\n",
    "3. **输出投影**：\n",
    "   $$ \\text{FLOPs}_{\\text{out}} = B \\times s \\times h \\times h \\times 2 = 2Bs h^2 $$\n",
    "\n",
    "### 2.2 MLP FLOPs 计算\n",
    "\n",
    "标准 FFN 包含两个线性变换和激活函数：\n",
    "\n",
    "$$ \\text{FLOPs}_{\\text{mlp}} = B \\times s \\times (2 \\times h \\times 4h + 2 \\times 4h \\times h) = 16Bs h^2 $$\n",
    "\n",
    "### 2.3 嵌入层 FLOPs 计算\n",
    "\n",
    "$$ \\text{FLOPs}_{\\text{embed}} = B \\times s \\times h \\times V \\times 2 = 2Bs h V $$\n",
    "\n",
    "### 2.4 完整稠密模型 FLOPs 公式\n",
    "\n",
    "综合所有组件，单次前向传播的 FLOPs 为：\n",
    "\n",
    "$$ \\text{FLOPs}_{\\text{forward}} = L \\times (6Bs h^2 + 2Bs^2 h + 2Bs h^2 + 16Bs h^2) + 2Bs h V $$\n",
    "\n",
    "简化后：\n",
    "\n",
    "$$ \\text{FLOPs}_{\\text{forward}} = 24L Bs h^2 + 2L Bs^2 h + 2Bs h V $$\n",
    "\n",
    "考虑反向传播（计算量约为前向的 2 倍），总 FLOPs 为：\n",
    "\n",
    "$$ \\text{FLOPs}_{\\text{total}} = 3 \\times (24L Bs h^2 + 2L Bs^2 h + 2Bs h V) $$\n",
    "\n",
    "$$ \\text{FLOPs}_{\\text{total}} = 72L Bs h^2 + 6L Bs^2 h + 6Bs h V $$\n",
    "\n",
    "## 3. MoE 架构的 FLOPs 计算\n",
    "\n",
    "### 3.1 MoE 架构的特殊性\n",
    "\n",
    "MoE（Mixture of Experts）模型的关键特点：\n",
    "\n",
    "- 总专家数：$E_{\\text{total}}$\n",
    "- 每个 token 激活的专家数：$E_{\\text{active}}$（通常为 1-2）\n",
    "- 门控网络计算开销\n",
    "\n",
    "### 3.2 注意力部分 FLOPs\n",
    "\n",
    "$$ \\text{FLOPs}_{\\text{attn}} = 72L Bs h^2 + 6L Bs^2 h $$\n",
    "\n",
    "### 3.3 MLP 部分 FLOPs\n",
    "\n",
    "$$ \\text{FLOPs}_{\\text{mlp-moe}} = 16L Bs h^2 \\times \\frac{E_{\\text{active}}}{E_{\\text{total}}} $$\n",
    "\n",
    "### 3.4 门控网络 FLOPs\n",
    "$$ \\text{FLOPs}_{\\text{gate}} = 2L Bs h E_{\\text{total}} $$\n",
    "\n",
    "### 3.5 嵌入层 FLOPs\n",
    "\n",
    "$$ \\text{FLOPs}_{\\text{embed}} = 6Bs h V $$\n",
    "\n",
    "### 3.6 完整 MoE 模型 FLOPs\n",
    "\n",
    "$$ \\text{FLOPs}_{\\text{moe-total}} = 72L Bs h^2 + 6L Bs^2 h + 16L Bs h^2 \\times \\frac{E_{\\text{active}}}{E_{\\text{total}}} + 2L Bs h E_{\\text{total}} + 6Bs h V $$\n",
    "\n",
    "## 4. FLOPs 计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4277268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dense_flops(L, h, B, s, V, include_backward=True):\n",
    "    \"\"\"\n",
    "    计算稠密 Transformer 模型的 FLOPs\n",
    "    \n",
    "    参数:\n",
    "    L: Transformer 层数\n",
    "    h: 隐藏层维度\n",
    "    B: 批次大小\n",
    "    s: 序列长度\n",
    "    V: 词表大小\n",
    "    include_backward: 是否包含反向传播\n",
    "    \n",
    "    返回:\n",
    "    total_flops: 总 FLOPs 数\n",
    "    \"\"\"\n",
    "    # 前向传播 FLOPs\n",
    "    flops_forward = (\n",
    "        24 * L * B * s * h**2 +  # 注意力机制和 MLP 的主要计算\n",
    "        2 * L * B * s**2 * h +   # 注意力矩阵计算\n",
    "        2 * B * s * h * V        # 嵌入层\n",
    "    )\n",
    "    \n",
    "    # 总 FLOPs（前向+反向）\n",
    "    coeff = 3 if include_backward else 1\n",
    "    total_flops = coeff * flops_forward\n",
    "    \n",
    "    return total_flops\n",
    "\n",
    "def compute_moe_flops(L, h, B, s, V, E_total, E_active=2, include_backward=True):\n",
    "    \"\"\"\n",
    "    计算 MoE Transformer 模型的 FLOPs\n",
    "    \n",
    "    参数:\n",
    "    L: Transformer 层数\n",
    "    h: 隐藏层维度\n",
    "    B: 批次大小\n",
    "    s: 序列长度\n",
    "    V: 词表大小\n",
    "    E_total: 总专家数\n",
    "    E_active: 每个 token 激活的专家数\n",
    "    include_backward: 是否包含反向传播\n",
    "    \n",
    "    返回:\n",
    "    total_flops: 总 FLOPs 数\n",
    "    \"\"\"\n",
    "    # 注意力部分 FLOPs（与稠密模型相同）\n",
    "    flops_attn = 72 * L * B * s * h**2 + 6 * L * B * s**2 * h\n",
    "    \n",
    "    # MoE 特有的 MLP 部分 FLOPs\n",
    "    flops_mlp_moe = 16 * L * B * s * h**2 * (E_active / E_total)\n",
    "    \n",
    "    # 门控网络 FLOPs\n",
    "    flops_gate = 2 * L * B * s * h * E_total\n",
    "    \n",
    "    # 嵌入层 FLOPs\n",
    "    flops_embed = 6 * B * s * h * V\n",
    "    \n",
    "    # 总 FLOPs\n",
    "    total_flops = flops_attn + flops_mlp_moe + flops_gate + flops_embed\n",
    "    \n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4719770",
   "metadata": {},
   "source": [
    "让我们通过具体数值来计算 DeepSeek 和 Qwen3 的 FLOPs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316dbeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek-7B 参数配置\n",
    "L_deepseek = 30    # 层数\n",
    "h_deepseek = 4096  # 隐藏维度\n",
    "V_deepseek = 102400  # 词表大小\n",
    "\n",
    "# Qwen3-7B 参数配置\n",
    "L_qwen = 40        # 层数\n",
    "h_qwen = 5120      # 隐藏维度\n",
    "V_qwen = 151936    # 词表大小\n",
    "\n",
    "# 训练配置\n",
    "B = 4      # 批大小\n",
    "s = 512    # 序列长度\n",
    "\n",
    "# 计算稠密模型 FLOPs\n",
    "flops_deepseek_dense = compute_dense_flops(L_deepseek, h_deepseek, B, s, V_deepseek)\n",
    "flops_qwen_dense = compute_dense_flops(L_qwen, h_qwen, B, s, V_qwen)\n",
    "\n",
    "print(f\"DeepSeek 稠密模型 FLOPs/迭代: {flops_deepseek_dense / 1e12:.2f} TFLOPs\")\n",
    "print(f\"Qwen3 稠密模型 FLOPs/迭代: {flops_qwen_dense / 1e12:.2f} TFLOPs\")\n",
    "\n",
    "# 计算 MoE 模型 FLOPs（假设 8 个专家，激活 2 个）\n",
    "E_total = 8\n",
    "E_active = 2\n",
    "\n",
    "flops_deepseek_moe = compute_moe_flops(L_deepseek, h_deepseek, B, s, V_deepseek, E_total, E_active)\n",
    "flops_qwen_moe = compute_moe_flops(L_qwen, h_qwen, B, s, V_qwen, E_total, E_active)\n",
    "\n",
    "print(f\"DeepSeek-MoE 模型 FLOPs/迭代: {flops_deepseek_moe / 1e12:.2f} TFLOPs\")\n",
    "print(f\"Qwen3-MoE 模型 FLOPs/迭代: {flops_qwen_moe / 1e12:.2f} TFLOPs\")\n",
    "\n",
    "# 计算 MoE 相对于稠密的节省比例\n",
    "saving_deepseek = (flops_deepseek_dense - flops_deepseek_moe) / flops_deepseek_dense\n",
    "saving_qwen = (flops_qwen_dense - flops_qwen_moe) / flops_qwen_dense\n",
    "\n",
    "print(f\"DeepSeek-MoE FLOPs 节省: {saving_deepseek * 100:.2f}%\")\n",
    "print(f\"Qwen3-MoE FLOPs 节省: {saving_qwen * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86044c97",
   "metadata": {},
   "source": [
    "## 5. MFU 计算完整实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5108a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mfu_comprehensive(model_config, batch_size, seq_length, iteration_time, device_flops, is_moe=False):\n",
    "    \"\"\"\n",
    "    综合计算模型的 MFU\n",
    "    \n",
    "    参数:\n",
    "    model_config: 模型配置字典\n",
    "    batch_size: 批次大小\n",
    "    seq_length: 序列长度\n",
    "    iteration_time: 迭代时间(秒)\n",
    "    device_flops: 设备峰值 FLOPS\n",
    "    is_moe: 是否为 MoE 模型\n",
    "    \n",
    "    返回:\n",
    "    mfu: 模型算力利用率\n",
    "    detailed_breakdown: 详细计算分解\n",
    "    \"\"\"\n",
    "    # 提取模型参数\n",
    "    L = model_config['num_hidden_layers']\n",
    "    h = model_config['hidden_size']\n",
    "    V = model_config['vocab_size']\n",
    "    \n",
    "    if is_moe:\n",
    "        E_total = model_config.get('num_experts', 8)\n",
    "        E_active = model_config.get('num_experts_per_tok', 2)\n",
    "        total_flops = compute_moe_flops(L, h, batch_size, seq_length, V, E_total, E_active)\n",
    "    else:\n",
    "        total_flops = compute_dense_flops(L, h, batch_size, seq_length, V)\n",
    "    \n",
    "    # 计算实际 FLOPS\n",
    "    actual_flops_per_sec = total_flops / iteration_time\n",
    "    \n",
    "    # 计算 MFU\n",
    "    mfu = actual_flops_per_sec / device_flops\n",
    "    \n",
    "    # 生成详细分解\n",
    "    detailed_breakdown = {\n",
    "        'total_flops': total_flops,\n",
    "        'iteration_time': iteration_time,\n",
    "        'actual_flops_per_sec': actual_flops_per_sec,\n",
    "        'device_flops': device_flops,\n",
    "        'mfu': mfu\n",
    "    }\n",
    "    \n",
    "    return mfu, detailed_breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec439d",
   "metadata": {},
   "source": [
    "让我们通过具体数值来分析不同架构的 MFU 差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12616ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设备峰值算力（假设 A100 GPU）\n",
    "device_flops = 312 * 1e12  # 312 TFLOPS\n",
    "\n",
    "# 模型配置\n",
    "deepseek_config = {\n",
    "    'num_hidden_layers': 30,\n",
    "    'hidden_size': 4096,\n",
    "    'vocab_size': 102400\n",
    "}\n",
    "\n",
    "qwen_config = {\n",
    "    'num_hidden_layers': 40,\n",
    "    'hidden_size': 5120,\n",
    "    'vocab_size': 151936\n",
    "}\n",
    "\n",
    "# 假设的迭代时间（基于实际测量）\n",
    "iteration_time_dense = 0.5  # 秒\n",
    "iteration_time_moe = 0.3    # 秒\n",
    "\n",
    "# 计算 MFU\n",
    "print(\"DeepSeek 模型 MFU 分析:\")\n",
    "mfu_deepseek_dense, breakdown_dense = calculate_mfu_comprehensive(\n",
    "    deepseek_config, 4, 512, iteration_time_dense, device_flops, False\n",
    ")\n",
    "mfu_deepseek_moe, breakdown_moe = calculate_mfu_comprehensive(\n",
    "    deepseek_config, 4, 512, iteration_time_moe, device_flops, True\n",
    ")\n",
    "\n",
    "print(f\"稠密架构 MFU: {mfu_deepseek_dense * 100:.2f}%\")\n",
    "print(f\"MoE 架构 MFU: {mfu_deepseek_moe * 100:.2f}%\")\n",
    "print(f\"MFU 提升: {(mfu_deepseek_moe - mfu_deepseek_dense) / mfu_deepseek_dense * 100:.2f}%\")\n",
    "\n",
    "# 输出详细计算信息\n",
    "print(\"\\n 详细计算分解（稠密）:\")\n",
    "for key, value in breakdown_dense.items():\n",
    "    if 'flops' in key:\n",
    "        print(f\"{key}: {value / 1e12:.2f} TFLOPs\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n 详细计算分解（MoE）:\")\n",
    "for key, value in breakdown_moe.items():\n",
    "    if 'flops' in key:\n",
    "        print(f\"{key}: {value / 1e12:.2f} TFLOPs\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f31ce2",
   "metadata": {},
   "source": [
    "不同条件下的 MFU 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mfu_variations(model_config, device_flops, is_moe=False):\n",
    "    \"\"\"\n",
    "    分析不同批大小和序列长度对 MFU 的影响\n",
    "    \n",
    "    参数:\n",
    "    model_config: 模型配置\n",
    "    device_flops: 设备峰值 FLOPS\n",
    "    is_moe: 是否为 MoE 模型\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # 测试不同的批大小和序列长度\n",
    "    batch_sizes = [1, 2, 4, 8, 16]\n",
    "    seq_lengths = [256, 512, 1024, 2048]\n",
    "    \n",
    "    # 假设迭代时间与计算量成正比\n",
    "    base_time = 0.3 if is_moe else 0.5\n",
    "    \n",
    "    results = np.zeros((len(batch_sizes), len(seq_lengths)))\n",
    "    \n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        for j, sl in enumerate(seq_lengths):\n",
    "            # 估算迭代时间（与实际计算量成正比）\n",
    "            if is_moe:\n",
    "                flops = compute_moe_flops(\n",
    "                    model_config['num_hidden_layers'],\n",
    "                    model_config['hidden_size'],\n",
    "                    bs, sl,\n",
    "                    model_config['vocab_size'],\n",
    "                    8, 2\n",
    "                )\n",
    "            else:\n",
    "                flops = compute_dense_flops(\n",
    "                    model_config['num_hidden_layers'],\n",
    "                    model_config['hidden_size'],\n",
    "                    bs, sl,\n",
    "                    model_config['vocab_size']\n",
    "                )\n",
    "            \n",
    "            # 假设迭代时间与 FLOPs 成正比\n",
    "            iteration_time = base_time * (flops / (312 * 1e12))  # 归一化\n",
    "            \n",
    "            mfu, _ = calculate_mfu_comprehensive(\n",
    "                model_config, bs, sl, iteration_time, device_flops, is_moe\n",
    "            )\n",
    "            results[i, j] = mfu\n",
    "    \n",
    "    # 可视化结果\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 批大小对 MFU 的影响\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for j, sl in enumerate(seq_lengths):\n",
    "        plt.plot(batch_sizes, results[:, j], 'o-', label=f'SeqLen={sl}')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('MFU')\n",
    "    plt.title('MFU vs Batch Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 序列长度对 MFU 的影响\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        plt.plot(seq_lengths, results[i, :], 'o-', label=f'BS={bs}')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('MFU')\n",
    "    plt.title('MFU vs Sequence Length')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6e884",
   "metadata": {},
   "source": [
    "![](./images/CODE03MFU02.png)\n",
    "\n",
    "## 6. 技术原理深度解析\n",
    "\n",
    "MoE 架构的核心思想是通过稀疏激活来减少计算量：\n",
    "\n",
    "$$\n",
    "\\text{计算节省} = 1 - \\frac{E_{\\text{active}}}{E_{\\text{total}}}\n",
    "$$\n",
    "\n",
    "当 $E_{\\text{total}} = 8$ 且 $E_{\\text{active}} = 2$ 时，MLP 部分的计算量减少到原来的 25%。\n",
    "\n",
    "实际 MFU 通常低于理论值的主要原因：\n",
    "\n",
    "1. **内存带宽限制**：数据搬运时间占比较大\n",
    "2. **通信开销**：分布式训练中的梯度同步\n",
    "3. **计算并行度**：无法完全利用所有计算单元\n",
    "4. **内核启动开销**：GPU 内核启动的延迟\n",
    "\n",
    "$$ \\text{实际 MFU} = \\text{理论 MFU} \\times \\eta_{\\text{memory}} \\times \\eta_{\\text{communication}} \\times \\eta_{\\text{parallelism}} $$\n",
    "\n",
    "## 7. 总结与思考\n",
    "\n",
    "通过公式推导和代码实现，我们深入分析了稠密和 MoE 架构的 FLOPs 计算原理。稀疏激活，MoE 架构可以显著减少计算量，通常能节省 50-75%的 FLOPs，虽然 MoE 减少计算量，但需要权衡通信开销和内存使用。\n",
    "\n",
    "另外，为了提高 MFU 需要综合考虑计算、内存、通信三个方面的综合优化。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
