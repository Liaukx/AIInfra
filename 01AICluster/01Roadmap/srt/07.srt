1
00:00:00,000 --> 00:00:02,400
内容/录制:Z0MI 酱，视频后期/字幕:梁嘉铭 

2
00:00:02,400 --> 00:00:03,033
hello 大家好

3
00:00:03,033 --> 00:00:06,000
我是那个周三周三不想上班的 ZOMI

4
00:00:11,033 --> 00:00:11,266
今天

5
00:00:11,300 --> 00:00:13,933
来到了一个 AI 智算集群里面

6
00:00:13,933 --> 00:00:16,000
看一下它的整体的系统架构

7
00:00:16,000 --> 00:00:17,066
也是 ZOMI 认为

8
00:00:17,100 --> 00:00:19,200
在整个集群智算之路

9
00:00:19,233 --> 00:00:21,366
最核心的一个内容

10
00:00:21,366 --> 00:00:24,200
就是整个集群智算的整体的系统架构

11
00:00:24,200 --> 00:00:25,033
看一下

12
00:00:25,033 --> 00:00:27,366
从之前的上一个视频讲到

13
00:00:27,366 --> 00:00:29,300
智算集群的总览开始

14
00:00:29,333 --> 00:00:32,633
分解 L0 L1 L2 到 L3

15
00:00:32,633 --> 00:00:34,666
每一层到底有哪些内容

16
00:00:34,700 --> 00:00:35,200
那这个

17
00:00:35,200 --> 00:00:39,266
也是整个 AI 集群最核心的内容

18
00:00:39,300 --> 00:00:41,200
那后面的所有的内容

19
00:00:41,200 --> 00:00:43,366
都会围绕着这一块进行展开

20
00:00:43,366 --> 00:00:44,300
那今天的视频

21
00:00:44,333 --> 00:00:46,233
可能分开三个小节

22
00:00:46,233 --> 00:00:48,000
第一个小节就是 L0 到 L1

23
00:00:48,000 --> 00:00:49,700
主要是围绕着机房的建设

24
00:00:49,733 --> 00:00:52,366
包括风火水电相关的内容

25
00:00:52,500 --> 00:00:55,366
第二节就是 L2 算力底座

26
00:00:55,366 --> 00:00:56,266
那所谓的算力底座

27
00:00:56,300 --> 00:00:59,633
就是算存网三个主要的内容

28
00:00:59,633 --> 00:01:02,400
infra 层真正比较偏硬核的东西

29
00:01:02,400 --> 00:01:02,966
那第三个

30
00:01:02,966 --> 00:01:05,900
就是 L3 到 L4 的智算使能

31
00:01:05,933 --> 00:01:06,700
那智算使能

32
00:01:06,700 --> 00:01:09,166
更多是指软件层面的内容

33
00:01:09,166 --> 00:01:11,433
也就是 AI infra 软件相关的内容

34
00:01:11,433 --> 00:01:12,566
包括容器化部署

35
00:01:12,566 --> 00:01:14,166
虚拟化多租户

36
00:01:14,166 --> 00:01:16,100
还有分布式并行相关的智算

37
00:01:16,133 --> 00:01:17,333
最后还有关于云

38
00:01:17,333 --> 00:01:20,033
还有整个 AI 集群的运维和运营

39
00:01:20,033 --> 00:01:21,233
相关的内容

40
00:01:22,333 --> 00:01:23,400
刚才讲到这一期视频

41
00:01:23,400 --> 00:01:25,566
是 ZOMI 觉得在整个集群智算之路

42
00:01:25,566 --> 00:01:26,266
最核心

43
00:01:26,300 --> 00:01:26,500
是因为

44
00:01:26,500 --> 00:01:29,100
它是一个关于整个智算集群总览

45
00:01:29,100 --> 00:01:31,433
相关的一个综述的展开

46
00:01:31,433 --> 00:01:32,266
那回顾一下

47
00:01:32,300 --> 00:01:34,100
在整个群智算

48
00:01:34,100 --> 00:01:35,533
整体的解决方案里面

49
00:01:35,533 --> 00:01:36,433
在硬件

50
00:01:36,433 --> 00:01:39,100
主要是之前上一节视频讲过

51
00:01:39,133 --> 00:01:40,966
主要是 L0 L1 L2

52
00:01:40,966 --> 00:01:41,733
那今天

53
00:01:41,733 --> 00:01:44,300
主要是围绕着 L0 跟 L2 进行展开

54
00:01:44,333 --> 00:01:45,633
那在软件层面

55
00:01:45,633 --> 00:01:47,800
可能更多的是 L3 到 L4

56
00:01:47,800 --> 00:01:49,400
做算力使能

57
00:01:49,400 --> 00:01:52,300
和对外提供一个 SaaS 服务相关的内容

58
00:01:52,333 --> 00:01:53,733
那接下来

59
00:01:53,733 --> 00:01:55,300
正式的打开

60
00:01:55,300 --> 00:01:57,966
刚才讲到的每一层的内容啦

61
00:01:58,033 --> 00:01:58,600
那第一个

62
00:01:58,600 --> 00:02:01,433
就是 L0 到 L1 的整体的机房

63
00:02:01,433 --> 00:02:04,000
或者 AI 集群的建设

64
00:02:04,000 --> 00:02:05,066
那机房有点 low

65
00:02:05,100 --> 00:02:09,333
更多的其实愿意称它为 AI 集群

66
00:02:10,566 --> 00:02:13,366
在整个 L0 到 L1 的机房的配套里面

67
00:02:13,366 --> 00:02:15,400
可以看到 L0 跟 L1 的机房

68
00:02:15,400 --> 00:02:18,500
其实不管是左边的通用云数据中心

69
00:02:18,533 --> 00:02:20,400
还是新型的智算中心

70
00:02:20,400 --> 00:02:24,100
最核心的就是整体的集群的布局

71
00:02:24,133 --> 00:02:24,633
那布局

72
00:02:24,633 --> 00:02:26,233
就涉及到供电的制冷

73
00:02:26,233 --> 00:02:28,033
还有相关的承重

74
00:02:28,133 --> 00:02:28,366
当然

75
00:02:28,366 --> 00:02:30,566
现在比较明确的就是算力

76
00:02:30,566 --> 00:02:31,666
越来越密集

77
00:02:31,700 --> 00:02:33,966
就整体的已经出现了超节点

78
00:02:33,966 --> 00:02:36,566
还有非常高密度高能耗的机柜

79
00:02:36,566 --> 00:02:37,266
那这个时候

80
00:02:37,300 --> 00:02:39,133
就需要整体的机房

81
00:02:39,133 --> 00:02:41,500
从风冷慢慢的走向液冷

82
00:02:41,500 --> 00:02:43,700
就是一个非常明确的趋势

83
00:02:43,700 --> 00:02:45,000
那整体看一下

84
00:02:45,000 --> 00:02:47,300
就是通用的云数据中心

85
00:02:47,333 --> 00:02:50,333
DC 跟现在的 AI 的智算中心

86
00:02:50,333 --> 00:02:51,900
有哪些主要的区别

87
00:02:51,900 --> 00:02:54,300
那首先就是机房的供电

88
00:02:54,433 --> 00:02:54,900
很明显

89
00:02:54,933 --> 00:02:56,300
现在的 AI 的智算中心

90
00:02:56,300 --> 00:02:59,633
整体的供电的功耗是大了非常的多

91
00:02:59,633 --> 00:03:02,066
到了 20 千瓦甚至 30 千瓦每柜

92
00:03:02,100 --> 00:03:03,366
也是非常的正常

93
00:03:03,366 --> 00:03:05,033
因为 AI 里面一台

94
00:03:05,033 --> 00:03:06,700
这里面主要是挂 CPU

95
00:03:06,833 --> 00:03:08,566
在 AI 的智算中心里面

96
00:03:08,566 --> 00:03:10,600
挂了更多的 NPU

97
00:03:10,600 --> 00:03:12,500
NPU 跟 GPU 的功耗绝对

98
00:03:12,533 --> 00:03:13,600
的确比 CPU 多

99
00:03:13,600 --> 00:03:15,300
而且 CPU 也没减

100
00:03:15,366 --> 00:03:17,100
可能原来是两台 CPU 一个节点

101
00:03:17,133 --> 00:03:18,833
现在还是两个 CPU 一个节点

102
00:03:18,833 --> 00:03:21,233
但是后面多了 8 张 NPU

103
00:03:21,233 --> 00:03:22,266
或者 8 张 GPU

104
00:03:22,300 --> 00:03:24,566
这个时候功耗是多了非常多

105
00:03:24,566 --> 00:03:26,166
假设一个 GPU 的芯片

106
00:03:26,166 --> 00:03:27,900
它功耗是 300 瓦

107
00:03:28,000 --> 00:03:29,433
一个节点就多了 8 个

108
00:03:29,433 --> 00:03:30,433
3*8=24

109
00:03:30,433 --> 00:03:31,233
所以你可以看到

110
00:03:31,233 --> 00:03:33,000
多了非常高的一个功耗

111
00:03:33,000 --> 00:03:33,666
那另外的话

112
00:03:33,700 --> 00:03:36,100
可能整体的机房的制冷

113
00:03:36,100 --> 00:03:38,366
也慢慢的从传统的 DC 的风冷

114
00:03:38,366 --> 00:03:40,800
转向现在的 AI 的智算的液冷

115
00:03:40,800 --> 00:03:43,000
当然 AI 的智算不是纯液冷

116
00:03:43,000 --> 00:03:45,166
它可能还是跟风冷进行混合

117
00:03:45,166 --> 00:03:47,566
当然你会发现有一部分降频了之后

118
00:03:47,566 --> 00:03:49,066
还能够做到风冷

119
00:03:49,133 --> 00:03:51,700
而且承重了也是高了很多

120
00:03:51,700 --> 00:03:52,200
那另外的话

121
00:03:52,200 --> 00:03:55,400
看一下整体的一个网络的部署

122
00:03:55,400 --> 00:03:56,066
那网络部署

123
00:03:56,100 --> 00:03:58,433
ZOMI 觉得还是非常的有意义

124
00:03:58,466 --> 00:03:59,433
去讲讲

125
00:03:59,433 --> 00:04:00,366
首先管理面

126
00:04:00,366 --> 00:04:01,466
业务面跟数据面

127
00:04:01,500 --> 00:04:03,200
实际上是比较通用

128
00:04:03,200 --> 00:04:04,666
后面讲网络

129
00:04:04,700 --> 00:04:05,566
在第二节的时候

130
00:04:05,566 --> 00:04:07,900
会重点的去展开每个业务面

131
00:04:07,933 --> 00:04:09,933
或者每个网络平面有什么区别

132
00:04:09,933 --> 00:04:11,700
但是关于整个智算中心

133
00:04:11,700 --> 00:04:13,166
多了一个参数面

134
00:04:13,200 --> 00:04:14,200
所谓的参数面

135
00:04:14,200 --> 00:04:16,966
就是指训练大模型的时候

136
00:04:16,966 --> 00:04:18,300
权重的参数

137
00:04:18,333 --> 00:04:19,700
模型的参数

138
00:04:19,700 --> 00:04:22,533
那这些参数是单独走一个网络

139
00:04:22,533 --> 00:04:24,366
所以现在看到其实比较简单

140
00:04:24,366 --> 00:04:25,633
就这么按下回车

141
00:04:25,633 --> 00:04:26,566
它就跑起来

142
00:04:26,566 --> 00:04:28,833
但是在 AI infra 相关的建设里面

143
00:04:28,833 --> 00:04:30,200
就多了一层

144
00:04:30,233 --> 00:04:31,966
核心的参数面的内容

145
00:04:31,966 --> 00:04:33,266
而且整体的运维

146
00:04:33,300 --> 00:04:34,000
你会发现

147
00:04:34,000 --> 00:04:36,166
以前传统的这种云数据中心是

148
00:04:36,166 --> 00:04:37,400
分散式管理

149
00:04:37,566 --> 00:04:38,100
也就是对应

150
00:04:38,133 --> 00:04:40,433
集群里面的某一部分机器

151
00:04:40,433 --> 00:04:41,566
我是独立的管理

152
00:04:41,566 --> 00:04:43,466
然后有很多个管理面

153
00:04:43,500 --> 00:04:44,933
有很多个管理的业务

154
00:04:44,933 --> 00:04:47,100
但是因为整个 AI 的智算中心

155
00:04:47,100 --> 00:04:50,033
现在更变相于之前讲到

156
00:04:50,033 --> 00:04:53,266
HPC 里面的 k 线图里面的新的内容

157
00:04:53,300 --> 00:04:54,200
超节点

158
00:04:54,200 --> 00:04:56,600
更多的希望统一的进行运维管理

159
00:04:56,600 --> 00:04:58,000
统一的智能管理

160
00:04:58,233 --> 00:04:59,066
一个大模型

161
00:04:59,100 --> 00:04:59,700
一训起来

162
00:04:59,700 --> 00:05:00,333
一拉起来

163
00:05:00,333 --> 00:05:02,000
直接拉起千卡到万卡

164
00:05:02,033 --> 00:05:04,033
那多模态也是的一拉拉起来

165
00:05:04,033 --> 00:05:05,400
拉起百卡到千卡

166
00:05:05,400 --> 00:05:08,633
所以说统一的管理非常的重要

167
00:05:08,766 --> 00:05:10,666
那接着看一下整体

168
00:05:10,700 --> 00:05:12,200
刚才讲到的一个综述

169
00:05:12,200 --> 00:05:13,100
看一下整体

170
00:05:13,133 --> 00:05:13,600
液冷

171
00:05:13,600 --> 00:05:16,766
现在已经是一个非常重要的内容

172
00:05:16,766 --> 00:05:18,433
现在有一个比较明确的论调

173
00:05:18,433 --> 00:05:20,666
就是现在的芯片

174
00:05:20,700 --> 00:05:22,133
你想充分发挥它的能力

175
00:05:22,133 --> 00:05:24,600
你需要更多的功耗才能算的更多

176
00:05:24,600 --> 00:05:25,433
那这个时候

177
00:05:25,433 --> 00:05:26,166
功耗大

178
00:05:26,166 --> 00:05:28,000
散热就变成一个卡点

179
00:05:28,000 --> 00:05:28,800
所以说液冷

180
00:05:28,800 --> 00:05:31,033
现在是使算力的芯片

181
00:05:31,033 --> 00:05:32,833
充分的能力发挥出来

182
00:05:32,833 --> 00:05:35,000
现在芯片的密集度越高

183
00:05:35,000 --> 00:05:37,400
半导体之间的密集度也会越来越高

184
00:05:37,400 --> 00:05:39,200
所以说液冷

185
00:05:39,200 --> 00:05:41,100
上液冷还是非常的有必要

186
00:05:41,133 --> 00:05:41,900
而且液冷

187
00:05:41,900 --> 00:05:42,800
能够整体

188
00:05:42,800 --> 00:05:46,166
提升整个系统的可靠性

189
00:05:46,166 --> 00:05:47,766
那可以看到风冷的服务器

190
00:05:47,766 --> 00:05:49,366
是左边的一个辐射图

191
00:05:49,366 --> 00:05:51,066
那右边是液冷的一个辐射图

192
00:05:51,100 --> 00:05:52,433
可以看到整体组液冷

193
00:05:52,433 --> 00:05:54,033
整个板载

194
00:05:54,033 --> 00:05:55,466
它的一个散热

195
00:05:55,500 --> 00:05:57,133
整体是做的非常的好

196
00:05:57,133 --> 00:05:58,633
左边走传统的风冷

197
00:05:58,633 --> 00:06:00,433
可能导致部分的元器件

198
00:06:00,433 --> 00:06:01,866
可能更容易坏

199
00:06:01,900 --> 00:06:02,433
所以说这

200
00:06:02,433 --> 00:06:02,800
里面

201
00:06:02,800 --> 00:06:04,633
就有了所谓的失效率的问题

202
00:06:04,633 --> 00:06:06,033
不管是服务的可靠性

203
00:06:06,033 --> 00:06:07,700
还是零部件的可靠性

204
00:06:07,733 --> 00:06:09,966
还都会满足基本的要求

205
00:06:09,966 --> 00:06:12,200
反正现在更多的是三年一淘汰

206
00:06:12,200 --> 00:06:13,700
但是真正到了三年之后

207
00:06:13,733 --> 00:06:16,333
会发现 AI 或者 NPU 的算力

208
00:06:16,333 --> 00:06:17,700
还没有淘汰

209
00:06:18,100 --> 00:06:19,333
在后面的环节里面

210
00:06:19,333 --> 00:06:20,000
重点

211
00:06:20,000 --> 00:06:23,200
会介绍液冷跟风冷相关的一些技术

212
00:06:23,200 --> 00:06:24,233
那整体液冷

213
00:06:24,233 --> 00:06:26,100
其实最核心的一个指标

214
00:06:26,133 --> 00:06:27,400
就是降低整体机房

215
00:06:27,400 --> 00:06:29,066
或者 AI 集群的 PUE

216
00:06:29,100 --> 00:06:32,333
功耗 power usage efficient

217
00:06:32,333 --> 00:06:33,433
电能的利用率

218
00:06:33,433 --> 00:06:33,833
提升

219
00:06:33,833 --> 00:06:37,666
整个机房的一个电力的使用情况

220
00:06:37,833 --> 00:06:39,600
现在可以从右边的这个图

221
00:06:39,600 --> 00:06:40,266
可以看到

222
00:06:40,300 --> 00:06:42,033
现在的单机柜的功耗

223
00:06:42,033 --> 00:06:43,100
肯定是越来越高

224
00:06:43,133 --> 00:06:45,133
算力的密集也越来越高

225
00:06:45,133 --> 00:06:46,166
那整体的 PUE

226
00:06:46,166 --> 00:06:49,900
是往着更低的方向去发展

227
00:06:51,100 --> 00:06:55,000
那了解完 L0 到 L1 的一个基础建设

228
00:06:55,000 --> 00:06:56,300
就是集群建设之后

229
00:06:56,333 --> 00:06:58,700
现在来到了一个算力的底座

230
00:06:58,700 --> 00:07:00,433
刚才只是一些外设

231
00:07:00,433 --> 00:07:03,233
现在真正的才是最核心的内容

232
00:07:03,233 --> 00:07:04,000
那后面

233
00:07:04,000 --> 00:07:06,766
同样的会在第二节跟第三节里面

234
00:07:06,766 --> 00:07:08,866
重点的去展开所谓的计算

235
00:07:08,900 --> 00:07:10,500
还有网络跟存储

236
00:07:10,500 --> 00:07:12,300
那算存网三个

237
00:07:12,300 --> 00:07:13,933
就是最核心的内容

238
00:07:13,933 --> 00:07:14,500
那首先

239
00:07:14,500 --> 00:07:16,400
看一下所谓的计算

240
00:07:16,400 --> 00:07:16,966
那计算

241
00:07:16,966 --> 00:07:19,100
其实最重要的就是做 scale up

242
00:07:19,133 --> 00:07:21,166
提升一个单节点

243
00:07:21,166 --> 00:07:22,900
单集群的一个算力的密集度

244
00:07:22,933 --> 00:07:24,600
因为现在的模型越来越大

245
00:07:24,600 --> 00:07:27,266
现在模型对算力的消耗也越来越强

246
00:07:27,300 --> 00:07:28,100
所以希望

247
00:07:28,100 --> 00:07:29,833
可能现在出现的越来越多

248
00:07:29,833 --> 00:07:32,266
像之前两节里面说到

249
00:07:32,300 --> 00:07:33,833
现在利用 scale up 的方式

250
00:07:33,833 --> 00:07:34,700
做超节点

251
00:07:34,766 --> 00:07:36,800
我算力的一个密集度的提升

252
00:07:36,800 --> 00:07:37,500
那另外的话

253
00:07:37,533 --> 00:07:40,000
可能还会使用更低精度的格式

254
00:07:40,000 --> 00:07:41,000
例如 BF 16

255
00:07:41,000 --> 00:07:42,233
还有那个 FP 8

256
00:07:42,233 --> 00:07:44,200
来去提升一个算力

257
00:07:44,300 --> 00:07:45,166
那第二个

258
00:07:45,166 --> 00:07:46,833
就是网络

259
00:07:46,833 --> 00:07:47,766
那所谓的网络

260
00:07:47,766 --> 00:07:50,600
就尽可能的把算力组合起来

261
00:07:50,600 --> 00:07:52,633
组成一个超大规模的一个集群

262
00:07:52,633 --> 00:07:54,600
可能把单卡变成多卡

263
00:07:54,600 --> 00:07:55,866
多卡变成多节点

264
00:07:55,900 --> 00:07:57,566
多节点变成一个大集群

265
00:07:57,566 --> 00:07:58,366
那这种方式

266
00:07:58,366 --> 00:08:00,700
更多的是做 scale out 的方式

267
00:08:00,733 --> 00:08:02,133
把整体的集群

268
00:08:02,133 --> 00:08:04,166
通过网络行互联起来

269
00:08:04,166 --> 00:08:05,900
而且网络传输的过程当中

270
00:08:05,933 --> 00:08:08,000
尽可能的提升网络的带宽

271
00:08:08,000 --> 00:08:09,400
提升并发的效率

272
00:08:09,400 --> 00:08:12,000
然后降低 IO 的耗时

273
00:08:12,000 --> 00:08:12,800
那第三个

274
00:08:12,800 --> 00:08:14,300
可能大家会经常忽略

275
00:08:14,333 --> 00:08:16,933
就是更大的数据的存储和读写

276
00:08:16,933 --> 00:08:18,833
那这里面的数据的存储和读写

277
00:08:18,833 --> 00:08:19,266
这里面

278
00:08:19,300 --> 00:08:20,333
不仅仅只是指

279
00:08:20,333 --> 00:08:22,766
存储的时候的一个 SuperPoD

280
00:08:22,833 --> 00:08:24,700
更多的是指多级的缓存

281
00:08:24,733 --> 00:08:25,800
多级的存储

282
00:08:25,800 --> 00:08:28,633
从一个 SuperPoD SSD 的一个阵列

283
00:08:28,633 --> 00:08:30,166
到缓存存储器

284
00:08:30,166 --> 00:08:31,833
到一个 HDD 的大容量池

285
00:08:31,966 --> 00:08:33,366
到整个集群

286
00:08:33,366 --> 00:08:36,400
智算中心的时候的一个 DDR LPDDR

287
00:08:36,400 --> 00:08:37,400
CPU 里面

288
00:08:37,400 --> 00:08:39,666
当 CPU 跟 GPU 或者 NPU 里面

289
00:08:39,700 --> 00:08:41,366
统一共享内存地址

290
00:08:41,366 --> 00:08:44,066
就是 LPDDR 跟 HBM 拉齐

291
00:08:44,100 --> 00:08:45,700
到可能单芯片

292
00:08:45,700 --> 00:08:48,800
单芯粒里面的一个 HBM L2 Cache L1 Cache

293
00:08:48,800 --> 00:08:49,866
到 Register file

294
00:08:49,900 --> 00:08:51,000
寄存器

295
00:08:51,000 --> 00:08:53,700
那这里面就提供更详细的内容

296
00:08:53,733 --> 00:08:56,366
在后面也会重点去展开存储

297
00:08:56,366 --> 00:08:58,366
特别是 CKPT 存储的时候

298
00:08:58,366 --> 00:08:59,700
应该在整个集群里面

299
00:08:59,733 --> 00:09:00,633
数据是怎么留

300
00:09:00,633 --> 00:09:02,466
然后它怎么做快速恢复

301
00:09:02,866 --> 00:09:05,000
那了解完整个横向之后

302
00:09:05,000 --> 00:09:06,666
现在逐个的去打开

303
00:09:06,700 --> 00:09:07,333
那首先

304
00:09:07,333 --> 00:09:09,933
L2 底座算力的最核心的第一个内容

305
00:09:09,933 --> 00:09:12,033
就是所谓的计算

306
00:09:12,033 --> 00:09:12,566
那计算

307
00:09:12,566 --> 00:09:13,433
其实你会发现

308
00:09:13,433 --> 00:09:15,900
有几个重要的内容

309
00:09:18,033 --> 00:09:19,266
在整个算力底座

310
00:09:19,300 --> 00:09:21,933
现在打开计算的这个内容

311
00:09:21,933 --> 00:09:23,366
那看到左边的这个

312
00:09:23,366 --> 00:09:24,866
就是具体昇腾

313
00:09:24,900 --> 00:09:27,200
一个物理的一个硬件视图

314
00:09:27,200 --> 00:09:29,566
那上面是四块整个鲲鹏

315
00:09:29,566 --> 00:09:31,500
下面是 8 块 NPU

316
00:09:31,533 --> 00:09:32,166
那可以看到

317
00:09:32,166 --> 00:09:33,433
整体的那个逻辑架构图

318
00:09:33,433 --> 00:09:34,633
就像右边所示

319
00:09:34,633 --> 00:09:36,300
四块鲲鹏进行互联

320
00:09:36,333 --> 00:09:37,800
然后 8 个 NPU

321
00:09:37,800 --> 00:09:39,033
也进行互联

322
00:09:39,033 --> 00:09:42,566
在整个 8 个 NPU 跟 4 个 CPU

323
00:09:42,566 --> 00:09:45,500
就组成了一个 AI 的计算节点

324
00:09:45,533 --> 00:09:47,400
然后整个计算节点的后面

325
00:09:47,400 --> 00:09:50,300
后面的也就是下面的这一个背板

326
00:09:50,333 --> 00:09:51,133
下面这个背板

327
00:09:51,133 --> 00:09:54,133
8 个 NPU 其实是进行一个互联

328
00:09:55,433 --> 00:09:56,500
那接着看一下

329
00:09:56,533 --> 00:09:57,900
英伟达的一个方式

330
00:09:57,900 --> 00:09:58,433
那英伟达

331
00:09:58,433 --> 00:10:00,800
它更多的是提供一个 Blackwell

332
00:10:00,800 --> 00:10:02,833
跟 Grace CPU 的一个互联

333
00:10:02,833 --> 00:10:04,800
这里面就叫做 C2C

334
00:10:04,800 --> 00:10:06,300
然后两个 Blackwell 之间

335
00:10:06,333 --> 00:10:07,733
也进行一个互联

336
00:10:07,733 --> 00:10:09,833
然后整体的那个硬件架构图

337
00:10:09,833 --> 00:10:11,400
就像这个所示

338
00:10:11,600 --> 00:10:12,433
两个 Grace

339
00:10:12,433 --> 00:10:13,766
然后四个 Blackwell

340
00:10:13,766 --> 00:10:14,900
然后四个 Blackwell 之间

341
00:10:14,933 --> 00:10:16,833
其实它也是两两进行贴合

342
00:10:16,833 --> 00:10:18,700
然后后面还有一个这个

343
00:10:18,733 --> 00:10:21,133
应该是 NV switch 的 Tray

344
00:10:21,200 --> 00:10:21,633
那整体

345
00:10:21,633 --> 00:10:22,300
可以看到

346
00:10:22,333 --> 00:10:23,933
不管是哪个厂商

347
00:10:23,933 --> 00:10:26,366
现在都往着大集群大节点这么去做

348
00:10:26,366 --> 00:10:28,066
因为现在的大模型越来越大

349
00:10:28,100 --> 00:10:30,533
AI 消耗的算力也越来越强

350
00:10:30,833 --> 00:10:32,200
那讲完计算以外

351
00:10:32,200 --> 00:10:33,366
看一下网络

352
00:10:33,366 --> 00:10:34,266
网络比较特别

353
00:10:34,300 --> 00:10:35,133
刚才讲到

354
00:10:35,133 --> 00:10:37,933
它有四个平面进行独立组网的参数面

355
00:10:37,933 --> 00:10:40,566
数据面主要是用大带宽的 RoCE

356
00:10:40,566 --> 00:10:42,766
满足大的一个通讯流

357
00:10:42,766 --> 00:10:43,766
那四个网络面

358
00:10:43,766 --> 00:10:44,300
分别是

359
00:10:44,333 --> 00:10:47,433
将会在后面的第二章的内容

360
00:10:47,433 --> 00:10:48,666
就是很后面的视频

361
00:10:48,700 --> 00:10:49,633
跟大家介绍

362
00:10:49,700 --> 00:10:50,000
第一个

363
00:10:50,000 --> 00:10:51,833
就是数据的 AI 的参数面

364
00:10:51,833 --> 00:10:52,900
还有数据面

365
00:10:52,933 --> 00:10:53,400
业务面

366
00:10:53,400 --> 00:10:53,966
运维面

367
00:10:53,966 --> 00:10:55,200
每个平面都不一样

368
00:10:55,200 --> 00:10:56,533
传的数据也不一样

369
00:10:56,566 --> 00:10:59,333
网络面跟网络面的之间是进行隔离

370
00:10:59,333 --> 00:11:00,833
而且不同的网络面

371
00:11:00,833 --> 00:11:03,033
用的一个网络的拓扑结构

372
00:11:03,033 --> 00:11:04,900
是不完全相同

373
00:11:04,933 --> 00:11:06,033
因为它对 IO

374
00:11:06,033 --> 00:11:06,633
对带宽

375
00:11:06,633 --> 00:11:08,966
还有对整个网络的要求不一样

376
00:11:09,833 --> 00:11:11,066
那最重要的关键特性

377
00:11:11,100 --> 00:11:11,900
就是参数面

378
00:11:11,900 --> 00:11:12,366
数据面

379
00:11:12,366 --> 00:11:14,000
需要支持一个安全的隔离

380
00:11:14,000 --> 00:11:16,200
还有参数面的负载的均衡

381
00:11:16,300 --> 00:11:17,633
那了解到网络

382
00:11:17,633 --> 00:11:17,966
其实

383
00:11:17,966 --> 00:11:20,300
更多的可能会看网络的拓扑图

384
00:11:20,333 --> 00:11:23,033
那真正的所谓的 NPU 的集群

385
00:11:23,333 --> 00:11:25,166
其实是在这里面

386
00:11:25,166 --> 00:11:25,900
那另外的话

387
00:11:25,933 --> 00:11:27,166
NPU 跟 NPU 之间

388
00:11:27,166 --> 00:11:28,666
要进行互相的传输

389
00:11:28,700 --> 00:11:30,500
所以就有了一个参数面的交换机

390
00:11:30,500 --> 00:11:32,766
还有因为要做两层组网

391
00:11:32,766 --> 00:11:35,700
所以会有 Spine Leaf 相关的交换机

392
00:11:35,733 --> 00:11:37,300
那同样的这里面

393
00:11:37,300 --> 00:11:38,766
后面会详细的介绍

394
00:11:38,766 --> 00:11:41,233
因为要控制这一个

395
00:11:41,233 --> 00:11:42,633
整个 NPU 的集群

396
00:11:42,633 --> 00:11:43,466
所以说上面

397
00:11:43,500 --> 00:11:45,933
就要有一个业务面进行一个控制

398
00:11:45,933 --> 00:11:47,000
那因为这里面

399
00:11:47,000 --> 00:11:49,033
还有很多相关的存储区

400
00:11:49,033 --> 00:11:50,366
因为现在训练大模型

401
00:11:50,366 --> 00:11:51,866
数据量那么大怎么办

402
00:11:51,900 --> 00:11:53,633
所以会有一个所谓的数据面

403
00:11:53,633 --> 00:11:56,600
单独的给到一个训练集群

404
00:11:56,600 --> 00:11:57,800
进行一个训练

405
00:11:57,800 --> 00:11:59,466
当然了要管理起来

406
00:11:59,500 --> 00:12:01,900
管理起来整个计算的业务

407
00:12:01,900 --> 00:12:03,300
还有存储的业务

408
00:12:03,300 --> 00:12:06,233
那计算跟存储进行相关的交互

409
00:12:06,400 --> 00:12:09,266
又有了所谓的管理面的一个平台

410
00:12:09,300 --> 00:12:11,133
通过管理面的一个交换机

411
00:12:11,133 --> 00:12:12,033
还有路由器

412
00:12:12,033 --> 00:12:14,866
去管理整个大的网络

413
00:12:14,900 --> 00:12:16,533
所以说基本上网络

414
00:12:16,533 --> 00:12:18,033
就分开非常多层

415
00:12:18,033 --> 00:12:18,900
有 100G 的网络

416
00:12:18,933 --> 00:12:19,800
有 10G 的网络

417
00:12:19,800 --> 00:12:20,900
有 25G 的网络

418
00:12:20,933 --> 00:12:22,700
还有一些 100G 的 RoCE

419
00:12:22,700 --> 00:12:23,766
还有 200G 的 RoCE

420
00:12:23,933 --> 00:12:25,600
还有对应 400G 的 RoCE

421
00:12:25,600 --> 00:12:26,500
后面的内容

422
00:12:26,533 --> 00:12:28,100
将会深入的展开

423
00:12:28,100 --> 00:12:30,166
这里面这么简单的一个介绍

424
00:12:30,166 --> 00:12:31,100
那不管怎么样

425
00:12:31,133 --> 00:12:34,000
整体还是希望在座

426
00:12:34,000 --> 00:12:34,833
就最核心

427
00:12:34,833 --> 00:12:37,166
就是一个智算节点

428
00:12:37,166 --> 00:12:39,033
所谓的 GPU 也好

429
00:12:39,033 --> 00:12:41,200
还是 NPU 也好

430
00:12:41,200 --> 00:12:44,300
最核心的就是参数面的网络的设计

431
00:12:44,433 --> 00:12:45,766
因为现在希望

432
00:12:45,766 --> 00:12:48,666
就是尽可能的单个节点或者单个柜

433
00:12:48,700 --> 00:12:50,700
或者节点跟节点之间的一个超节点

434
00:12:50,700 --> 00:12:52,033
或者一个万卡集群

435
00:12:52,100 --> 00:12:54,200
尽可能的做到网络是无损

436
00:12:54,200 --> 00:12:54,666
而且

437
00:12:54,700 --> 00:12:57,233
尽可能的提升网络的一个带宽

438
00:12:57,233 --> 00:12:58,300
这个就是现在

439
00:12:58,333 --> 00:12:59,600
一个很强大的需求

440
00:12:59,600 --> 00:13:01,833
因此叫做大带宽的网络

441
00:13:01,833 --> 00:13:05,033
还要做到无收敛的一个胖树的拓扑

442
00:13:05,166 --> 00:13:07,766
那到底是用胖树还是 Tours 拓扑

443
00:13:07,766 --> 00:13:09,000
反正目标

444
00:13:09,000 --> 00:13:11,166
是做到大带宽跟无收敛

445
00:13:11,166 --> 00:13:12,800
两个主要的特性

446
00:13:13,366 --> 00:13:13,833
那具体

447
00:13:13,833 --> 00:13:16,233
后面也会详细的跟大家去展开

448
00:13:16,233 --> 00:13:18,600
所谓的各种各样的拓扑的组网

449
00:13:18,600 --> 00:13:21,700
的方式不管是胖树的还是 Tours

450
00:13:22,033 --> 00:13:23,100
简单的了解完

451
00:13:23,133 --> 00:13:24,933
后面会介绍的内容之外

452
00:13:24,933 --> 00:13:27,033
其实也会看一下数据面

453
00:13:27,033 --> 00:13:28,300
一个组网的方式

454
00:13:28,333 --> 00:13:29,600
因为数据面

455
00:13:29,600 --> 00:13:30,200
你会发现

456
00:13:30,200 --> 00:13:32,300
数据面可能跟外面的不太一样

457
00:13:32,333 --> 00:13:33,566
就是计算系统

458
00:13:33,566 --> 00:13:35,833
可能跟外面是完全隔绝

459
00:13:35,833 --> 00:13:37,100
但是存储面

460
00:13:37,133 --> 00:13:38,233
或者存储系统

461
00:13:38,233 --> 00:13:39,166
存储的 PoD

462
00:13:39,200 --> 00:13:40,700
更多的会跟外部的存储

463
00:13:40,733 --> 00:13:42,533
或者跟外部的网络进行结合

464
00:13:42,533 --> 00:13:44,100
因为需要经常爬虫

465
00:13:44,100 --> 00:13:46,333
爬很多很多的数据

466
00:13:46,333 --> 00:13:47,933
给到存储系统

467
00:13:47,933 --> 00:13:49,233
然后计算的时候

468
00:13:49,233 --> 00:13:50,566
或者训练的时候

469
00:13:50,566 --> 00:13:53,866
就会从存储系统里面获取很多的数据

470
00:13:53,900 --> 00:13:55,100
给到训练系统

471
00:13:55,100 --> 00:13:56,766
所以说存储的网络面

472
00:13:56,766 --> 00:13:57,633
基本上

473
00:13:57,633 --> 00:13:59,600
也会做一个两层的无收敛

474
00:13:59,600 --> 00:14:00,000
而且

475
00:14:00,000 --> 00:14:01,833
这是大模型的一个训练的存储

476
00:14:01,833 --> 00:14:04,600
因为这里面除了存一些

477
00:14:04,600 --> 00:14:06,166
网外的数据

478
00:14:06,166 --> 00:14:08,166
就是真正训练时候用到的数据

479
00:14:08,166 --> 00:14:11,666
网络训练的时候的 CKPT CheckPoint

480
00:14:11,700 --> 00:14:12,966
模型的数据

481
00:14:12,966 --> 00:14:15,000
也会往存储系统里面存

482
00:14:15,000 --> 00:14:17,566
所以说基本上需要支持两个内容

483
00:14:17,566 --> 00:14:17,866
第一个

484
00:14:17,900 --> 00:14:18,766
刚才讲到

485
00:14:18,766 --> 00:14:21,000
一个无收敛的一个网络

486
00:14:21,000 --> 00:14:21,400
第二个

487
00:14:21,400 --> 00:14:23,633
就是模型的训练的存储

488
00:14:23,633 --> 00:14:25,566
训练的一个模型参数量

489
00:14:26,533 --> 00:14:27,400
那基本上

490
00:14:27,400 --> 00:14:28,166
右边的内容

491
00:14:28,166 --> 00:14:29,433
就不跟大家详细介绍

492
00:14:29,433 --> 00:14:31,300
因为会在后面

493
00:14:31,333 --> 00:14:33,100
就是很后面的章节

494
00:14:33,233 --> 00:14:36,400
重点的详细的单独的去介绍这个内容

495
00:14:36,400 --> 00:14:37,066
今天

496
00:14:37,100 --> 00:14:39,000
还是围绕着整个集群的解决方案

497
00:14:39,000 --> 00:14:40,233
做一个整体的概览

498
00:14:40,233 --> 00:14:42,166
看一下每一层都有哪些东西

499
00:14:42,166 --> 00:14:45,366
将会在后面详细的展开哪些内容

500
00:14:45,533 --> 00:14:46,400
那谈到存储

501
00:14:46,400 --> 00:14:48,866
刚才只是讲到硬件相关的存储

502
00:14:48,900 --> 00:14:49,600
实际上

503
00:14:49,600 --> 00:14:50,033
存储

504
00:14:50,033 --> 00:14:52,600
还会分开很多软件相关的内容

505
00:14:52,600 --> 00:14:53,900
例如多协议的融合

506
00:14:53,933 --> 00:14:55,900
还有支持一个大模型的 IO

507
00:14:55,900 --> 00:14:57,033
高效的访问

508
00:14:57,033 --> 00:14:59,166
那包括可能用了 direct IO

509
00:14:59,166 --> 00:15:01,300
还有那个 RDMA 等相关的技术

510
00:15:01,333 --> 00:15:03,700
对于一个存储进行优化

511
00:15:03,766 --> 00:15:04,266
那存储

512
00:15:04,300 --> 00:15:06,600
会还会分开一些存储的系统

513
00:15:06,600 --> 00:15:07,700
存储的文件

514
00:15:07,733 --> 00:15:10,633
还有高性能的缓存等相关的内容

515
00:15:10,633 --> 00:15:12,066
相关存储的些东西

516
00:15:12,100 --> 00:15:13,800
其实也在大模型来了之后

517
00:15:13,800 --> 00:15:15,566
进一步的提升

518
00:15:16,533 --> 00:15:18,100
刚才讲完 L2 的算存网之后

519
00:15:18,100 --> 00:15:22,166
现在来到了 L3 到 L4 的算力的使能

520
00:15:27,600 --> 00:15:28,500
所谓的算力使能

521
00:15:28,533 --> 00:15:29,333
就是刚才

522
00:15:29,333 --> 00:15:30,800
其实已经有了一系列的硬件

523
00:15:30,800 --> 00:15:32,166
包括建完机房之后

524
00:15:32,166 --> 00:15:33,800
把算存网都搬进去

525
00:15:33,800 --> 00:15:35,700
现在硬件设备都 ok

526
00:15:35,733 --> 00:15:38,000
那接下来就是软件入场

527
00:15:38,000 --> 00:15:39,300
就像装修的时候

528
00:15:39,333 --> 00:15:40,100
装完硬装

529
00:15:40,100 --> 00:15:42,100
现在来装软装

530
00:15:42,100 --> 00:15:43,366
整个硬装的过程当中

531
00:15:43,366 --> 00:15:44,466
其实只是解决

532
00:15:44,500 --> 00:15:46,133
通用的算力的问题

533
00:15:46,133 --> 00:15:47,500
通用的带宽问题

534
00:15:47,500 --> 00:15:47,900
但是

535
00:15:47,900 --> 00:15:50,033
怎么更好的利用起算力

536
00:15:50,033 --> 00:15:50,966
和带宽

537
00:15:50,966 --> 00:15:53,700
其实更重要的就是在算力的使能层

538
00:15:53,733 --> 00:15:54,533
也是 ZOMI

539
00:15:54,533 --> 00:15:56,800
做 AI Infra 最重要的也是 ZOMI

540
00:15:56,800 --> 00:15:59,300
最近这五六年所聚焦的内容

541
00:15:59,333 --> 00:16:01,433
基本上我这 6 年都在干这个活

542
00:16:01,433 --> 00:16:02,266
那不管怎么样

543
00:16:02,300 --> 00:16:04,400
看一下整算力使能层

544
00:16:04,400 --> 00:16:05,666
其实现在

545
00:16:05,900 --> 00:16:08,500
最大的目标就是提升训练的效率

546
00:16:08,500 --> 00:16:09,300
那训练效率

547
00:16:09,300 --> 00:16:11,700
主要分开了几个来去组成

548
00:16:11,900 --> 00:16:15,000
这个就是单机的计算的效率

549
00:16:15,000 --> 00:16:18,600
就尽可能把单机的性能发挥出来

550
00:16:18,600 --> 00:16:19,800
单机性能发挥不出来

551
00:16:19,800 --> 00:16:21,900
说实话你组集群是没有意义

552
00:16:22,033 --> 00:16:24,600
尽可能把单机的效果提升出来

553
00:16:24,600 --> 00:16:25,100
那第二个

554
00:16:25,133 --> 00:16:28,000
就是万卡集群的一个能力线性度

555
00:16:28,000 --> 00:16:30,366
还有模型的利用力提升起来

556
00:16:30,366 --> 00:16:31,400
就充分的利用好

557
00:16:31,400 --> 00:16:33,700
模型的一个带宽能力

558
00:16:33,733 --> 00:16:36,100
使得整个集群的并行和网络

559
00:16:36,100 --> 00:16:37,233
能够达到最优

560
00:16:37,333 --> 00:16:38,033
那第三个

561
00:16:38,033 --> 00:16:40,766
就是所谓的中断的时间最短

562
00:16:40,766 --> 00:16:42,433
就是尽可能的去提升

563
00:16:42,433 --> 00:16:43,466
故障的恢复时间

564
00:16:43,500 --> 00:16:46,300
也就 MTTR 那对应的能力

565
00:16:46,300 --> 00:16:47,566
三个能力提升起来

566
00:16:47,566 --> 00:16:49,866
才能够证明整个训练集群

567
00:16:49,900 --> 00:16:52,766
或者整个建设的 AI 集群

568
00:16:52,900 --> 00:16:54,500
效率能够做到最优

569
00:16:54,500 --> 00:16:55,166
那这里面

570
00:16:55,166 --> 00:16:57,000
讲到单机的计算效率

571
00:16:57,000 --> 00:16:59,200
实际上会用很多的算法

572
00:16:59,200 --> 00:17:00,466
包括从框架里面做

573
00:17:00,500 --> 00:17:01,600
一些选择性重计算

574
00:17:01,600 --> 00:17:03,066
还有全局内存的复用

575
00:17:03,233 --> 00:17:04,266
还有自适应动态索引

576
00:17:04,300 --> 00:17:07,300
还有相关的一些梯度的优化的算法

577
00:17:07,300 --> 00:17:08,800
来通过一系列的算法

578
00:17:08,800 --> 00:17:11,200
希望尽可能的把算子的效率

579
00:17:11,233 --> 00:17:14,300
把整个单卡的效率提升上来

580
00:17:14,366 --> 00:17:15,366
有了单卡效率之后

581
00:17:15,366 --> 00:17:18,000
现在就做到集群的并行最优

582
00:17:18,000 --> 00:17:19,233
当然了谈到集群

583
00:17:19,233 --> 00:17:21,833
其实更多的是做各种各样的并行

584
00:17:21,900 --> 00:17:22,433
模型并行

585
00:17:22,433 --> 00:17:23,366
数据并行

586
00:17:23,733 --> 00:17:24,333
长序列的并行

587
00:17:24,333 --> 00:17:25,133
优化器并行

588
00:17:25,133 --> 00:17:26,533
各种各样的并行

589
00:17:26,533 --> 00:17:28,300
那怎么样的并行

590
00:17:28,300 --> 00:17:31,633
会决定一个网络的传输的数据

591
00:17:31,633 --> 00:17:33,500
还有网络的传输的包的大小

592
00:17:33,533 --> 00:17:36,366
和网络传输数据流流量的大

593
00:17:36,366 --> 00:17:37,366
小后面

594
00:17:37,366 --> 00:17:38,366
将会重点的展开

595
00:17:38,366 --> 00:17:40,500
集群并行相关的内容

596
00:17:40,533 --> 00:17:43,700
可能在第四节或者在第三章

597
00:17:43,700 --> 00:17:44,766
就大章节里面

598
00:17:44,766 --> 00:17:46,200
会重点的展开

599
00:17:46,233 --> 00:17:48,833
那另外的话就是故障的恢复时间

600
00:17:48,833 --> 00:17:50,800
那故障恢复时间其实跟运维

601
00:17:50,800 --> 00:17:51,900
是息息相关

602
00:17:51,933 --> 00:17:53,533
将会在后面

603
00:17:53,533 --> 00:17:55,766
重点的展开相关的内容呀

604
00:17:56,333 --> 00:17:59,100
那今天基本上已经讲完

605
00:17:59,100 --> 00:17:59,800
那这节 ZOMI

606
00:17:59,800 --> 00:18:01,200
觉得是非常的核心

607
00:18:01,200 --> 00:18:02,666
就是带着大家

608
00:18:02,700 --> 00:18:05,766
能够端到端去看 L0 L1 L2 L3 L4

609
00:18:05,766 --> 00:18:09,433
整个 AI 集群涉及到的相关的技术点

610
00:18:09,433 --> 00:18:11,166
也是后面重点去展开

611
00:18:11,166 --> 00:18:13,833
现在简单的做一个小结

612
00:18:14,166 --> 00:18:15,433
那就是大模型训练

613
00:18:15,433 --> 00:18:17,566
你会发现最核心的特征

614
00:18:17,566 --> 00:18:20,366
就是高并行跟网络化

615
00:18:20,366 --> 00:18:23,566
所以集群就成为了 AI 训练

616
00:18:23,566 --> 00:18:25,400
非常重要的一个平台

617
00:18:25,400 --> 00:18:27,966
所以现在都直接叫做 AI 集群

618
00:18:27,966 --> 00:18:31,066
那这一个系列的就是集群超算之路

619
00:18:31,100 --> 00:18:32,733
从 HPC

620
00:18:32,733 --> 00:18:35,533
到了最后一节的 AI 的集群

621
00:18:35,566 --> 00:18:36,633
这么过来

622
00:18:36,633 --> 00:18:37,566
那接着看一下

623
00:18:37,566 --> 00:18:39,500
集群的建设的一个关键的内容

624
00:18:39,533 --> 00:18:41,300
第一个就是基础设施

625
00:18:41,300 --> 00:18:42,333
一定要有先进性

626
00:18:42,333 --> 00:18:43,800
特别是风火水电

627
00:18:43,800 --> 00:18:45,766
大家不要小看这些这么 low 的东西

628
00:18:45,766 --> 00:18:47,100
实际上液冷

629
00:18:47,133 --> 00:18:49,200
或者一个温度功耗

630
00:18:49,200 --> 00:18:51,066
会决定了整个集群

631
00:18:51,100 --> 00:18:52,766
或者单节点的一个算力

632
00:18:52,866 --> 00:18:54,666
那有了单节点的算力提升之后

633
00:18:54,700 --> 00:18:57,400
需要进行大规模的互联

634
00:18:57,400 --> 00:18:59,633
提升一个算力的效率

635
00:18:59,633 --> 00:19:03,366
整体使得集群能够更好用

636
00:19:03,566 --> 00:19:06,233
更易用那围绕着整个集群

637
00:19:06,233 --> 00:19:07,166
会发现

638
00:19:07,166 --> 00:19:09,800
现在的一个计算的效率还有长稳

639
00:19:09,900 --> 00:19:11,933
是很重要的一个特点

640
00:19:11,933 --> 00:19:14,333
想要打造一个亲和的集群架构

641
00:19:14,333 --> 00:19:15,966
最大化使能有效的算力

642
00:19:15,966 --> 00:19:18,566
说实话除了花很多时间

643
00:19:18,566 --> 00:19:19,100
去建设

644
00:19:19,133 --> 00:19:21,433
整个集群的一个硬件底座

645
00:19:21,433 --> 00:19:24,100
也就是 L0 L1 到 L2

646
00:19:24,133 --> 00:19:25,566
后面的越来越多

647
00:19:25,566 --> 00:19:28,800
能不能充分的发挥集群的能力

648
00:19:28,800 --> 00:19:29,633
就你建完

649
00:19:29,633 --> 00:19:30,900
建了 10 个亿的集群

650
00:19:30,933 --> 00:19:33,633
能不能把 11 个亿真正的用起来

651
00:19:33,633 --> 00:19:35,233
就决定了算力的平台

652
00:19:35,233 --> 00:19:37,266
和应用的服务平台

653
00:19:37,300 --> 00:19:39,600
这两个平台来决定集群的能力

654
00:19:39,600 --> 00:19:40,200
那今天

655
00:19:40,200 --> 00:19:42,233
已经做完整个概览

656
00:19:42,233 --> 00:19:43,266
有兴趣的小伙伴

657
00:19:43,300 --> 00:19:45,633
也可以持续留意 ZOMI 后面的内容

658
00:19:45,633 --> 00:19:49,433
将会详细的展开这些每一个技术点

659
00:19:49,500 --> 00:19:50,400
那今天的内容

660
00:19:50,400 --> 00:19:51,066
就到这里为止

661
00:19:51,100 --> 00:19:51,566
谢谢各位

662
00:19:51,566 --> 00:19:52,600
拜了个拜

663
00:19:53,166 --> 00:19:53,833
卷的不行

664
00:19:53,833 --> 00:19:54,833
卷的不行

665
00:19:54,833 --> 00:19:56,500
AI 系统的全套知识都在这里

666
00:19:56,533 --> 00:19:58,166
欢迎跟着目录进行学习

